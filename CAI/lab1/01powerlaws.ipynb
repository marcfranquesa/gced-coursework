{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a1defe9-0d69-429b-9dcf-79cf72b20e31",
   "metadata": {
    "id": "1a1defe9-0d69-429b-9dcf-79cf72b20e31"
   },
   "source": [
    "# CAI Lab Session 1: Powerlaws"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efc4b56-ecee-4b48-bea1-8a3a28983a3a",
   "metadata": {
    "id": "8efc4b56-ecee-4b48-bea1-8a3a28983a3a"
   },
   "source": [
    "In this session you will:\n",
    "\n",
    "- Learn about so-called power laws, a special case being Zipf's law\n",
    "- Investigate whether words in text seem to fit power laws\n",
    "- Preprocess text with `nltk`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zx9JIvAz7j9U",
   "metadata": {
    "id": "zx9JIvAz7j9U"
   },
   "source": [
    "## 1. On preprocessing (with `nltk`)\n",
    "\n",
    "For this first project, you may use the very popular [natural language tooklit library](https://www.nltk.org) which provides useful language processing functionality.\n",
    "\n",
    "In what follows you can see some examples of the type of preprocessing that we will use during the course.\n",
    "\n",
    "- _Tokenization:_ is the process that splits a sequence of characters into a sequence of word types (or _tokens_)\n",
    "\n",
    "- _Lower case folding_: turning all characters to lowercase\n",
    "\n",
    "- _Stopword removal:_ removes tokens that correspond to functional words (or _stopwords_)\n",
    "\n",
    "- _Stemming:_ mapping word types to their stem (normalization)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ukKyUlRn_BHt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ukKyUlRn_BHt",
    "outputId": "cd7eb57f-8468-42ed-f7ee-09a1d6b3054e"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wLMn9Luk-1bN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wLMn9Luk-1bN",
    "outputId": "610d8bcf-3793-46b9-a5ce-f63910e6ba8a"
   },
   "outputs": [],
   "source": [
    "## tokenizer\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"\"\"Primera parte del ingenioso hidalgo don Quijote de la Mancha.\n",
    "  Capitulo primero. Que trata de la condicion y ejercicio del famoso hidalgo\n",
    "  don Quijote de la Mancha\"\"\"\n",
    "\n",
    "tokenized_text = word_tokenize(text)\n",
    "\n",
    "pprint(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o1TV4DnX_31F",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o1TV4DnX_31F",
    "outputId": "7ff2f388-d188-45bc-bcf0-73a26eddd747"
   },
   "outputs": [],
   "source": [
    "## stopword removal, punctuation removal, lower-case folding\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "spanish_sw = set(stopwords.words('spanish') + list(string.punctuation))\n",
    "\n",
    "print(f'there are {len(spanish_sw)} stopwords in Spanish (including punctuation), which are: {spanish_sw}')\n",
    "\n",
    "filtered_tokenized_text = [w.lower() for w in tokenized_text if w.lower() not in spanish_sw]\n",
    "pprint(filtered_tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MYrK2m-oCw-0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MYrK2m-oCw-0",
    "outputId": "595ba9d6-90c6-4e61-fba5-b6d7a74cda20"
   },
   "outputs": [],
   "source": [
    "## stemmer\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "stemmed_text = [stemmer.stem(w) for w in filtered_tokenized_text]\n",
    "pprint(stemmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zHWeXKgYDlFz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zHWeXKgYDlFz",
    "outputId": "6ec1ecda-b343-451b-e1a3-4bb2488dbda6"
   },
   "outputs": [],
   "source": [
    "## useful for counting words\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "word_counts = Counter(stemmed_text)\n",
    "pprint(word_counts)\n",
    "\n",
    "word_counts_no_stem = Counter(filtered_tokenized_text)\n",
    "pprint(word_counts_no_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b5cdc6-8e9c-47e7-8f8d-bbb5ab5d9b57",
   "metadata": {
    "id": "82b5cdc6-8e9c-47e7-8f8d-bbb5ab5d9b57"
   },
   "source": [
    "## 2. On powerlaws\n",
    "\n",
    "Consider a function $y=f(x)$, that we will call a \"law\" relating $y$ with $x$.\n",
    "\n",
    "__Example 1.__ You are given a bucket of some radioactive isotope. $x$ is the number of seconds since you were given\n",
    "the bucket. $y$ is the number of atoms that disintegrate at the $x$th second.\n",
    "Each atom decides independently from all other atoms whether to disintegrate in the next second (or\n",
    "nanosecond, or whatever). From here you can see that $f$ will have the form $y=c \\cdot a^{-x}$,\n",
    "where $c$ depends on the number of atoms you were given, and $a$ depends on the isotope. In particular,\n",
    "$a>1$ determines the half-life of the isotope.\n",
    "\n",
    "__Example 2.__ $x$ is number of seconds and $y$ is the number of people entering a metro station during the next\n",
    "10 minutes. You may have been told in statistics that $f$ is given by the Poisson distribution\n",
    "(at least for ideal people and metro stations).\n",
    "\n",
    "Many natural and artificial phenomena come in distributions that are neither exponential nor Poisson,\n",
    "but so-called power laws. Intuitively, we have $y$ evolves like $x^a$, where $a$ is a constant called\n",
    "the {\\em exponent} of the power law. If $a$ is positive, $y$ is increasing, and if $a$ is negative,\n",
    "$y$ decreases.\n",
    "\n",
    "More precisely, a power law is\n",
    "$$\n",
    "y = c \\cdot (x+b)^a\n",
    "$$\n",
    "for three constants $a$, $b$, and $c$.\n",
    "\n",
    "There is a lot of theory about why powerlaws are ubiqutous. They are often related to self-organization,\n",
    "fractality, complex dynamical systems, etc. The Web and social networks are full of powerlaws.\n",
    "So is human language, social sciences (population of cities), and natural sciences (intensity of earthquakes,\n",
    "relation of size and metabolism in animals).\n",
    "\n",
    "A powerlaw with exponent $-1$ is called Zipf's law, after Mr. Zipf. By extension, sometimes powerlaws with\n",
    "negative exponents are also called Zipfian laws.\n",
    "\n",
    "In what follows we will fit some datasets to the best powerlaw that we can find - and see that they pretty close.\n",
    "Note that this does NOT mean that the phenomenon generating the dataset is exactly a powerlaw.\n",
    "Proving seriously that a law is (or is not) a powerlaw is another matter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c75cda0-0d03-4fdc-9aa0-785d5232354b",
   "metadata": {
    "id": "5c75cda0-0d03-4fdc-9aa0-785d5232354b"
   },
   "source": [
    "## 3. Looking at \"Don Quijote\"\n",
    "\n",
    "In this exercise, we will be looking at Zipf's and Heap's text laws of the classical text _Don Quijote_. The full plain text is publicly available in many places.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c244af-17d7-4ad9-8515-feca1b3cea20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "10c244af-17d7-4ad9-8515-feca1b3cea20",
    "outputId": "142c2813-0456-409f-893c-12180468ef5e"
   },
   "outputs": [],
   "source": [
    "# load don quijote, e.g. as in:\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "target_url = \"https://fegalaz.usc.es/~gamallo/aulas/lingcomputacional/corpus/quijote-es.txt\"\n",
    "\n",
    "data = urllib.request.urlopen(target_url)\n",
    "\n",
    "# print first N lines only as a demo\n",
    "N = 10\n",
    "for _ in range(N):\n",
    "    line = next(data)\n",
    "    print(line.decode('latin-1').strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qVXH6ddZpbl2",
   "metadata": {
    "id": "qVXH6ddZpbl2"
   },
   "source": [
    "---\n",
    "\n",
    "__Exercise 1.__ Use `python` + `matplotlib` or similar library to plot the frequence of words in `don quijote` in decreasing order.\n",
    "\n",
    "---\n",
    "\n",
    "Is it a powerlaw? Or, can it be approximated by a powerlaw?\n",
    "\n",
    "A trick about powerlaws is the following. Let's forget about the $b$ parameter for a second (or equivalently assume that $b = 0$),\n",
    "so our powerlaw looks like\n",
    "$$\n",
    "y = c \\cdot x^a.\n",
    "$$\n",
    "\n",
    "Taking logs on both sides, it becomes\n",
    "$$\n",
    "\\log y = a \\cdot \\log x + \\log c\n",
    "$$\n",
    "\n",
    "I.e., $\\log y$ is a linear function of $\\log x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bil8ImpbEa",
   "metadata": {
    "id": "41bil8ImpbEa"
   },
   "outputs": [],
   "source": [
    "# getting the data & preprocessing\n",
    "\n",
    "target_url = \"https://fegalaz.usc.es/~gamallo/aulas/lingcomputacional/corpus/quijote-es.txt\"\n",
    "data = urllib.request.urlopen(target_url)\n",
    "\n",
    "quijote = data.read().decode(\"latin-1\").replace(\"\\n\", \" \")\n",
    "tokenized = [word for word in word_tokenize(quijote) if word not in list(string.punctuation)] #tokenize & remove punctuation\n",
    "filtered_tokenized = [word.lower() for word in tokenized if word not in stopwords.words('spanish')] # remove stop words\n",
    "stemmed_tokenized = [stemmer.stem(word) for word in tokenized] # stemm\n",
    "stemmed_filtered_tokenized = [stemmer.stem(word) for word in filtered_tokenized]\n",
    "\n",
    "dataset_tokenized = {\n",
    "    \"Tokenized\": tokenized,\n",
    "    \"Filtered & tokenized\": filtered_tokenized,\n",
    "    \"Stemmed & tokenized\": stemmed_tokenized,\n",
    "    \"Stemmed, filtered & tokenized\": stemmed_filtered_tokenized\n",
    "}\n",
    "\n",
    "\n",
    "word_frequency = {}\n",
    "for name, data in dataset_tokenized.items():\n",
    "    word_frequency[name] = sorted(Counter(data).items(),\n",
    "                                  key=lambda item : item[1],\n",
    "                                  reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Oe97Ass3K4hM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "id": "Oe97Ass3K4hM",
    "outputId": "b765cce5-b57f-412a-9a32-32698488730b"
   },
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = make_subplots(rows=2, cols=2, subplot_titles=([name for name in word_frequency.keys()]))\n",
    "\n",
    "for i, (name, data) in enumerate(word_frequency.items()):\n",
    "    r = 1 + int(bool(max(0, i - 1)))\n",
    "    c = 1 + int(bool(i % 2))\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=list(range(1, len(data) + 1)),\n",
    "            y=[item[1] for item in data],\n",
    "        ),\n",
    "        row=r, col=c,\n",
    "\n",
    "    )\n",
    "    fig.update_xaxes(title_text=\"Decreasing order\", row=r, col=c)\n",
    "    fig.update_yaxes(title_text=\"Word frequency\", row=r, col=c)\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show(renderer=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "T53LV_4kgxJM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "id": "T53LV_4kgxJM",
    "outputId": "9060dce7-7171-427a-80c4-9b6201a0500d"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "name = \"Filtered & tokenized\"\n",
    "data = word_frequency[name]\n",
    "\n",
    "fig = px.scatter(\n",
    "    x=list(range(1, len(data) + 1)),\n",
    "    y=[item[1] for item in data],\n",
    "    labels={\n",
    "        \"x\": \"Decreasing order\",\n",
    "        \"y\": \"Word frequency\",\n",
    "    },\n",
    "    title=name\n",
    ")\n",
    "fig.show(renderer=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Sqc1iFy_e-FT",
   "metadata": {
    "id": "Sqc1iFy_e-FT"
   },
   "source": [
    "---\n",
    "\n",
    "__Exercise 2.__ Now, plot the same but use _logarithmic_ $x$ and $y$ axes.\n",
    "\n",
    "---\n",
    "\n",
    "If our distribution is a powerlaw, this plot should be a straight line, whose slope is $a$\n",
    "and intercept is $\\log c$. If we put back the $b$ parameter, it distorts a bit the\n",
    "low values, so in order to estimate $a$ and $c$ we have to pay attention to the large values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yJg3ZTovfB7L",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "id": "yJg3ZTovfB7L",
    "outputId": "6a7e52a4-93ef-4667-d38d-32dc71304413"
   },
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=2, cols=2, subplot_titles=([name for name in word_frequency.keys()]))\n",
    "\n",
    "for i, (name, data) in enumerate(word_frequency.items()):\n",
    "    r = 1 + int(bool(max(0, i - 1)))\n",
    "    c = 1 + int(bool(i % 2))\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=list(range(1, len(data) + 1)),\n",
    "            y=[item[1] for item in data],\n",
    "            mode=\"markers\"\n",
    "        ),\n",
    "        row=r, col=c\n",
    "    )\n",
    "    fig.update_xaxes(title_text=\"Decreasing order\", type=\"log\", row=r, col=c)\n",
    "    fig.update_yaxes(title_text=\"Word frequency\", type=\"log\", row=r, col=c)\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show(renderer=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JuzJO4Bwh6_U",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "id": "JuzJO4Bwh6_U",
    "outputId": "37f4ad11-ef03-480b-ea8a-3ec03231afc1"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "name = \"Tokenized\"\n",
    "data = word_frequency[name]\n",
    "\n",
    "fig = px.scatter(\n",
    "    x=list(range(1, len(data) + 1)),\n",
    "    y=[item[1] for item in data],\n",
    "    labels={\n",
    "        \"x\": \"Decreasing order\",\n",
    "        \"y\": \"Word frequency\",\n",
    "    },\n",
    "    title=name,\n",
    "    log_x=True,\n",
    "    log_y=True\n",
    ")\n",
    "fig.show(renderer=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fba2952-4558-4c64-a228-7b8d2a07d513",
   "metadata": {
    "id": "9fba2952-4558-4c64-a228-7b8d2a07d513"
   },
   "source": [
    "---\n",
    "\n",
    "__Exercise 4.__ Let's find $a$ and $c$ analytically.\n",
    "Assume we have $\\log y = a \\cdot \\log x + \\log c$.\n",
    "\n",
    "Take two distinct _large_ values of $x$, find their corresponding values of $y$,\n",
    "set up a system of two linear equations, and solve for $a$ and $c$.\n",
    "The solution will probably not fit very well the low values of $x$. You can try to\n",
    "make it better by adding the $b$ parameter, but don't agonize over it.\n",
    "\n",
    "Alternatively, use linear regression to estimate the _slope_  ($a$ parameter) and the intercept ($\\log c$ parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SAVC7PeuKIp1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 551
    },
    "id": "SAVC7PeuKIp1",
    "outputId": "85268fd8-39f9-4b6b-b1a5-b2848a612ab5"
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from numpy import log\n",
    "\n",
    "x = log(list(range(1, len(data) + 1)))\n",
    "y = log([item[1] for item in data])\n",
    "\n",
    "a, logc, _, _, _ = stats.linregress(x, y)\n",
    "\n",
    "def func(x):\n",
    "  return a * x + logc\n",
    "\n",
    "lr = list(map(func, x))\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x, y=y,\n",
    "                    mode=\"markers\",\n",
    "                    name=\"Data\"))\n",
    "fig.add_trace(go.Scatter(x=x, y=lr,\n",
    "                    mode=\"lines\",\n",
    "                    name=\"Linear regression\"))\n",
    "\n",
    "fig.update_layout(showlegend=False)\n",
    "print(f\"{a=}\\n{logc=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc3388f-4717-4175-b640-6b6a08fa310c",
   "metadata": {
    "id": "cfc3388f-4717-4175-b640-6b6a08fa310c"
   },
   "source": [
    "---\n",
    "\n",
    "__Exercise 5.__  Now, it is time to check whether Heap's law applies in the _Don Quijote_. First, plot number of different words (word _types_) as a function of text length. Next, plot it on a log-log scale. You should see a straight line. Finally, give an estimate of $\\beta$ parameter of Heap's law:\n",
    "\n",
    "$$  d = k \\cdot N^{\\beta} $$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v2okpFryOdw5",
   "metadata": {
    "id": "v2okpFryOdw5"
   },
   "outputs": [],
   "source": [
    "new_words = set()\n",
    "new_words_counter = []\n",
    "\n",
    "for word in tokenized:\n",
    "    new_words.add(word)\n",
    "    new_words_counter.append(len(new_words))\n",
    "\n",
    "length = list(range(1, len(new_words_counter) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jcV7mzM-O_R0",
   "metadata": {
    "id": "jcV7mzM-O_R0"
   },
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Different words as a function of length\", \"Log-log scale\"))\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=length,\n",
    "        y=new_words_counter,\n",
    "        mode=\"markers\"\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=length,\n",
    "        y=new_words_counter,\n",
    "        mode=\"markers\"\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "fig.update_xaxes(type=\"log\", row=1, col=2)\n",
    "fig.update_yaxes(type=\"log\", row=1, col=2)\n",
    "\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show(renderer=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65yRdNITD7a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 534
    },
    "id": "c65yRdNITD7a",
    "outputId": "d65cb7a6-1cb2-48ba-bc3a-6de63abd0865"
   },
   "outputs": [],
   "source": [
    "# logd = logk + beta * logN\n",
    "\n",
    "loglength = log(length)\n",
    "logcounter = log(new_words_counter)\n",
    "\n",
    "fitloglength = list(loglength)\n",
    "fitlogcounter = list(logcounter)\n",
    "\n",
    "print(len(fitlogcounter))\n",
    "\n",
    "i = 1000\n",
    "for _ in range(376400):\n",
    "    fitloglength.pop(i)\n",
    "    fitlogcounter.pop(i)\n",
    "\n",
    "print(\"ok\")\n",
    "\n",
    "beta, logk, _, _, _ = stats.linregress(fitloglength, fitlogcounter)\n",
    "\n",
    "def func(x):\n",
    "  return beta * x + logk\n",
    "\n",
    "lr = list(map(func, log(length)))\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=loglength, y=logcounter,\n",
    "                    mode=\"markers\",\n",
    "                    name=\"Data\"))\n",
    "fig.add_trace(go.Scatter(x=loglength, y=lr,\n",
    "                    mode=\"lines\",\n",
    "                    name=\"Linear regression\"))\n",
    "\n",
    "fig.show(renderer=\"png\")\n",
    "\n",
    "print(f\"{beta=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "icodIUNAIoLP",
   "metadata": {
    "id": "icodIUNAIoLP"
   },
   "source": [
    "## 4. Rules of delivery\n",
    "\n",
    "- To be solved in _pairs_.\n",
    "\n",
    "- No plagiarism; don't discuss your work with other teams. You can ask for help to others for simple things, such as recalling a python instruction or module, but nothing too specific to the session.\n",
    "\n",
    "- If you feel you are spending much more time than the rest of the classmates, ask us for help. Questions can be asked either in person or by email, and you'll never be penalized by asking questions, no matter how stupid they look in retrospect.\n",
    "\n",
    "- Write a short report listing the solutions to the exercises proposed. Include things like: how did you find the best $a$, $b$, $c$ parameter values for the text laws? Please include plots that compare the fitted powerlaws with the real data (preferably in log-log scale). You are welcome to add conclusions and findings that depart from what we asked you to do. We encourage you to discuss the difficulties you find; this lets us give you help and also improve the lab session for future editions.\n",
    "\n",
    "- Turn the report to PDF. Make sure it has your names, date, and title.\n",
    "\n",
    "- Submit your work through the [raco](http://www.fib.upc.edu/en/serveis/raco.html). There will be a `Practica` open for each report.\n",
    "\n",
    "- Deadline: Work must be delivered __within 2 weeks__ from the end of the lab session. Late submissions risk being penalized or not accepted at all. If you anticipate problems with the deadline, tell us as soon as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pjYVroT2IwdU",
   "metadata": {
    "id": "pjYVroT2IwdU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
