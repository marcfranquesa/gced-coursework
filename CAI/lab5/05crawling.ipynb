{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAI Lab Session 5: Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session you will:\n",
    "\n",
    "- learn about crawling the web\n",
    "- learn to implement a simple web crawler to extract information from web pages, using the `Scrapy` framework\n",
    "- use `ElasticSearch` to store the information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Crawling the web\n",
    "\n",
    "\n",
    "Crawler, robot, spider, wanderer, ...\n",
    "\n",
    "Systematically explores the web and collects documents:\n",
    "\n",
    "<table style=\"border-collapse: collapse; border: none;\">\n",
    "<tr style=\"border: none;\">\n",
    "<td style=\"border: none;\">\n",
    "\n",
    "```\n",
    "1. add _seed_ URLs to queue\n",
    "2. loop forever:\n",
    "      choose a URL from queue\n",
    "      fetch page, parse it\n",
    "      discard it or add it to DB\n",
    "      add (new) URL's it contains to queue\n",
    "```\n",
    "\n",
    "</td>\n",
    "<td style=\"border: none;\">\n",
    "<img src=\"crawlgraph.jpg\" width=\"450\"/>\n",
    "</td>\n",
    "</tr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. A framework for web crawling: `scrapy`\n",
    "\n",
    "[Scrapy](https://docs.scrapy.org/en/latest/) is a python library for developing web crawlers and extracting information from web pages. This library is designed to facilitate the deployment of crawlers that target specific web pages and the extraction and analysis of their content. \n",
    "\n",
    "In this session we are going to explain the basics of implementing a web crawler, you can find more detailed examples in Scrapy's documentation [tutorial](https://doc.scrapy.org/en/latest/intro/tutorial.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic code scaffolding for a web crawler is created automatically by the library, type in a terminal:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ scrapy startproject caiscrapy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create a crawler project able to host different crawlers. As an example we are going to extract from the [UPC Commons](https://upcommons.upc.edu/) website the information in the pages that store all the TFGs presented by FIB students during the past years.\n",
    "\n",
    "The url of this page is http://upcommons.upc.edu/handle/2099.1/18595/recent-submissions\n",
    "\n",
    "In a terminal type the following:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ cd caiscrapy/caiscrapy\n",
    "$ scrapy genspider UPCCommonsTFG upcommons.upc.edu\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will generate the file `UPCCommonsTFG.py` inside the directory `spiders` with a python class for the\n",
    "spider and its basic configuration. You will have to modify the variable `start_urls` so its starting point (i.e. seed page) is :\n",
    "\n",
    "http://upcommons.upc.edu/handle/2099.1/18595/recent-submissions\n",
    "\n",
    "The class has only one method called `parse` that is the one that receives the pages generated by the \n",
    "crawler. This is the method that extracts information from the page and decides \n",
    "what links to follow. This method has only one parameter `response` that contains the response of the web server to the request of the spider. In its current state the spider does nothing, just downloads the first url and does not generate any output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extracting TFG information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to decide what and how to extract information from a website, first we \n",
    "have to analyze how the information is stored in its pages. Because `HTML`\n",
    "represents a webpage as a tree with different tags and information in their\n",
    "nodes we can use that structure to access what we want to extract.\n",
    "\n",
    "Scrapy has two methods for extracting information from a webpage, one based on \n",
    "`CSS` and other in `XPATH`. We are going to use the first one because is simpler \n",
    "(however less powerful). \n",
    "\n",
    "Fortunately for us the pages that we want to crawl are well structured and the \n",
    "HTML tags that have the information that we want are more or less \n",
    "marked. In order to _dissect_ a webpage we can download it and open it with a text editor or we can use a browser like Chrome or Mozilla to inspect the\n",
    "page using `ctrl+shift+I`.\n",
    "\n",
    "Looking closely at the structure of the page (and I mean closely) we can find \n",
    "that all the TFGs are inside a `<li>` tag of class \n",
    "`ds-artifact-item`. The method `css` of the object stored in the \n",
    "`response` parameter allows us to extract all the occurrences of this \n",
    "tag. We can iterate through all these elements to extract the information \n",
    "inside. The `css` method extracts the part of the HTML tree that begins with the `tag` passed as a parameter. We can use the attributes of the tag to be more selective. Each element that we extract can also be parsed using the same method.\n",
    "\n",
    "Each element has a title, an url that links to the detailed information of the TFG, an \n",
    "author, a publisher, a date, the publication rights and an abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(self, response):\n",
    "    \"\"\"\n",
    "    Process the information of each page of TFGs\n",
    "\n",
    "    :param response:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    for tfg in response.css('li.ds-artifact-item'):\n",
    "        doc = {}\n",
    "        data = tfg.css('div.artifact-info')\n",
    "        doc['title'] = tfg.css('h4 a::text').extract_first()\n",
    "        doc['url'] = response.urljoin(tfg.css('h4 a::attr(href)').extract_first())\n",
    "        doc['author'] = data.css('span.author span::text').extract_first()\n",
    "        doc['publisher'] = data.css('span.publisher::text').extract_first()\n",
    "        doc['date'] = data.css('span.date::text').extract_first()\n",
    "        doc['rights'] = data.css('span.rights::text').extract_first()\n",
    "        doc['abstract'] = data.css('div.artifact-abstract::text').extract_first()\n",
    "\n",
    "        yield doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically the code extracts all the adequate `<li>` tags using the `css`\n",
    "method and for each one we use again `css` to parse the adequate tags \n",
    "for each field of information. The `extract_first` method returns the first occurrence or `None` if there is none. For the url we add the domain of the web to complete the url if it is a relative one. \n",
    "\n",
    "Everything is stored in a dictionary and returned to Scrapy using `yield`, this will make this function a generator, so the elements of the page are retrieved one by one when the crawler needs them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Exercise 1:** Substitute the empty `parse` function from the script `UPCCommonsTFG.py` with the code above, and run the crawler by invoking\n",
    "```\n",
    "$ scrapy crawl UPCCommonsTFG -o tfg.json\n",
    "```\n",
    "You should see in the standard output (mixed with the log of scrappy actions) the information from the first page of TFGs and it will also be stored in `tfg.json` in JSON format.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Going deeper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information in the page with the list of TFGs is not complete. Each TFG has\n",
    "an individual page with more information like the full summary and a list of \n",
    "keywords. We have obtained the link to this page from each TFG and stored it in \n",
    "the `url` field. \n",
    "\n",
    "Scrapy allows following links and adding the \n",
    "information from these links to the one that we already have. This can be done\n",
    "using the `Request` method. This method needs a `url`, the \n",
    "function that will process the webpage obtained from the url and also accepts \n",
    "the information that we have already collected. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(self, response):\n",
    "    \"\"\"\n",
    "    Process the information of each page of TFGs\n",
    "\n",
    "    :param response:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    for tfg in response.css('li.ds-artifact-item'):\n",
    "        doc = {}\n",
    "        data = tfg.css('div.artifact-info')\n",
    "        doc['title'] = tfg.css('h4 a::text').extract_first()\n",
    "        doc['url'] = response.urljoin(tfg.css('h4 a::attr(href)').extract_first())\n",
    "        doc['author'] = data.css('span.author span::text').extract_first()\n",
    "        doc['publisher'] = data.css('span.publisher::text').extract_first()\n",
    "        doc['date'] = data.css('span.date::text').extract_first()\n",
    "        doc['rights'] = data.css('span.rights::text').extract_first()\n",
    "        doc['abstract'] = data.css('div.artifact-abstract::text').extract_first()\n",
    "\n",
    "        yield scrapy.Request(doc['url'], callback=self.parse_detail, meta=doc)\n",
    "\n",
    "\n",
    "def parse_detail(self, response):\n",
    "    \"\"\"\n",
    "    Parses the information of the TFG detailed page\n",
    "\n",
    "    :param response:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    detail = response.meta\n",
    "    detail['description'] = ' '.join(response.css('div.expandable::text').extract())\n",
    "    detail['keywords'] = ' '.join(response.css('div.descripcio a::text').extract())\n",
    "\n",
    "    yield detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above you can see the updated code, where we have substituted the \n",
    "`yield` of the `doc` dictionary by a `yield` of the value returned by the call to `Request`. This receives a new function for \n",
    "parsing the detailed page  (`parse_detail`) and the already collected fields as the `meta` \n",
    "parameter. Notice that if we also return to Scrapy the information already in  the `doc` dictionary we will have two different items for each TFG (the one from the list of TFGs and the one from the detailed page and that is not what we want).\n",
    "\n",
    "The function `parse_detail` extracts the full summary and the keywords of a TFG. \n",
    "The designer of this page was kind enough to mark the tags as \n",
    "`expandable` (the full summary) and `descripcio` (the keywords). \n",
    "The summary in `expandable` is in different languages, but they are not \n",
    "indentified in the tags. In the solution that you have all is joined as one \n",
    "string. We could use a language detection algorithm to be able separate them, \n",
    "but this it is left as an exercise for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Exercise 2:** Run again the crawler with this new _parsing_ version to see the results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Storing the items in ElasticSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing the data in a text file is ok, but it is more useful to store it in a \n",
    "database. Scrapy allows putting a pipeline in the middle of the scraping \n",
    "process and storing the data in a database (or anywhere you need to).\n",
    "\n",
    "Substitute the automatically generated `pipelines.py` file with the one that you have with the \n",
    "session files. This file includes a class with methods that are called at the \n",
    "beginning and the end of the crawling process and each time an item is \n",
    "extracted. If you open the file you will see that a new index named `scrapy` is created in \n",
    "ElasticSearch  and each item is stored as is. \n",
    "One additional possibility is to check if all the fields extracted are valid, dropping the item if not or changing the invalid values by a default, but that is beyond the scope of this session.\n",
    "\n",
    "To activate the pipeline you will have to modify  the `settings.py` file. Uncomment the line that has the `ITEM_PIPELINES` configuration and change it to:\n",
    "\n",
    "```\n",
    "ITEM_PIPELINES = {\n",
    "    'caiscrapy.pipelines.CaiscrapyElasticPipeline': 300,\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Exercise 3:** Run again the crawler with this new _parsing_ version to see the results, which should be found now in a database (index) called `scrapy` in ElasticSearch. The following code should show the number of TFGs indexed.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from pprint import pprint\n",
    "\n",
    "client = Elasticsearch(\"http://localhost:9200\", request_timeout=1000)\n",
    "\n",
    "index = 'scrapy'\n",
    "info = client.cat.count(index=index, format = \"json\")[0]\n",
    "print(f\"Index: {index} with {info['count']} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Going even deeper (paginate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A final step that is missing is to collect the information from more than just \n",
    "the first page of TFGs. For doing this we only have to extract the link in the \n",
    "page that points to the next page. The page designer was also kind enough to mark this link as a `<a>` tag of class `next-page-link`.\n",
    "\n",
    "For this, we need to add the following code to the end of the `parse` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        next = response.css('a.next-page-link::attr(href)').extract_first()\n",
    "        if next is not None:\n",
    "            next_page = response.urljoin(next)\n",
    "            yield scrapy.Request(next_page, callback=self.parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically we search for the next page link and if it exists we follow it, yielding more results.\n",
    "Now the crawler will follow the next link of each page until there is no more\n",
    "pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Scraping all the way\n",
    "\n",
    "Now you can run the crawler and get all the TFGs data. First start ElasticSearch and after that initiate the crawling process:\n",
    "\n",
    "```\n",
    "$ scrapy crawl UPCCommonsTFG \n",
    "```\n",
    "\n",
    "The process will take a few minutes (or more) and in the end you will have all the TFGs info stored in the database. Now we can query the index and search for information. You have a `SearchIndexCrawler.py` script among the session files. This script allows searching an index using LUCENE query syntax.\n",
    "\n",
    "This syntax allows putting as a prefix of a word the field you want to use for \n",
    "the search, for example `author:jordi` will search for the word \n",
    "`jordi` only in the `author` field.\n",
    "\n",
    "For example, you can try:\n",
    "\n",
    "```\n",
    "$ python SearchIndexCrawler.py --index scrapy --query keywords:bases AND keywords:dades\n",
    "$ python SearchIndexCrawler.py --index scrapy --query keywords:machine AND keywords:learning \n",
    "$ python SearchIndexCrawler.py --index scrapy --query description:game\n",
    "$ python SearchIndexCrawler.py --index scrapy --query description:dades\n",
    "$ python SearchIndexCrawler.py --index scrapy --query title:dades~2\n",
    "$ python SearchIndexCrawler.py --index scrapy --query author:miquel~1\n",
    "```\n",
    "---\n",
    "\n",
    "**Exercise 4:** Invent your own queries and see the results.\n",
    "\n",
    "---\n",
    "\n",
    "**Exercise 5:** See if you can extract the director of the project from the detailed TFG page or any other information.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Rules of delivery\n",
    "\n",
    "There are __no deliverables__ for this session."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
