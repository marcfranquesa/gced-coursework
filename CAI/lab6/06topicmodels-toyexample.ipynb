{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "412eb65f-11a4-4eaa-98cf-1790c38145e2",
   "metadata": {},
   "source": [
    "# CAI Lab 6: pLSI with toy corpus\n",
    "\n",
    "\n",
    "_In this lab session, rather than assigning some task for you to do, I present code that implements the EM algorithm applied to the\n",
    "toyexample that we saw in class. The main objective of today's lab session is to clarify this model and provide you with code you\n",
    "can apply to a larger, more realistic corpus in the future._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9b64f1-0807-44f6-ba4c-83c5d371ac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.precision', 5)\n",
    "np.set_printoptions(precision = 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e277acb",
   "metadata": {},
   "source": [
    "## 1. The pLSI probabilistic model (using asymmetric formulation)\n",
    "\n",
    "The central parameters of the model are ($d$ stands for a document, $w$ stands for a word, and $z$ stands for the latent variable indicating topic assignment). \n",
    "\n",
    "- $P(z|d)$: the topic proportions for each document (\"topic distribution for document $d$\"); we have $K N$ such parameters; we use matrix $\\theta$ to store these parameters\n",
    "- $P(w|z)$: the distribution of words for a given topic (\"word distribution for topic $z$\"); we have $K M$ such parameters; we use matrix $\\beta$ to store these parameters\n",
    "\n",
    "The model is given by:\n",
    "\n",
    "- $P(d,w) = P(d) P(w|d) = P(d) \\sum_{k} P(z=k|d) P(w|z=k)$\n",
    "\n",
    "Throughout this document we assume that $P(d)$ is uniform and so we can simplify our formulation:\n",
    "\n",
    "- $P(d,w) \\propto \\sum_{k} P(z=k|d) P(w|z=k)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e591a2-14fe-4ff7-ae60-344f656383e5",
   "metadata": {},
   "source": [
    "## 2. Data generation and toy example\n",
    "\n",
    "I will be using a toy example to illustrate the definitions and computations. The toy example assumes *three* topics (\"uppercase\", \"lowercase\", \"number\"), and the vocabulary is going to be composed of *six* words: $\\{A,B,a,b,1,2\\}$. The toy example is chosen so that the parameter values have a clear meaning. Of course, in a real corpus we will not expect such \"clean\" topics.\n",
    "\n",
    "- word distributions for topics are ($\\beta$):\n",
    "\n",
    "|       |A    |B|a|b|1|2|\n",
    "|-------|-----|--|---|-----|----|-----|\n",
    "uppercase |0.45|0.45|0.025|0.025|0.025|0.025|\n",
    "lowercase |0..025|0.025|0.6|0.3|0.025|0.025|\n",
    "number    |0.025|0.025|0.025|0.025|0.2|0.7|\n",
    "\n",
    "- topic proportions for documents are ($\\theta$):\n",
    "\n",
    "||uppercase|lowercase|number|\n",
    "|-|---------|---------|------|\n",
    "1| 0.8 |0.1|0.1|\n",
    "2|0.1|0.8|0.1|\n",
    "3|0.1|0.1|0.8|\n",
    "4|1/3|1/3|1/3|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d178124b-229f-467a-b9ae-4062292fe72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word distributions for topics: P(w|z = k)\n",
    "\n",
    "words = \"ABab12\"   # vocabulary to get actual \"word\" from index\n",
    "\n",
    "\n",
    "beta = np.array([[0.45, 0.45, 0.025, 0.025, 0.025, 0.025],\n",
    "                 [0.025, 0.025, 0.6, 0.3, 0.025, 0.025],\n",
    "                 [0.025, 0.025, 0.025, 0.025, 0.2, 0.7]])\n",
    "\n",
    "print(pd.DataFrame(data=beta, columns = [w for w in words], index=['uppercase', 'lowercase', 'number']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6b59e4-b85f-4549-87a4-647326de4350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic for documents: P(z = k | d)\n",
    "\n",
    "theta = np.array([[0.8,0.1,0.1],\n",
    "                  [0.1,0.8,0.1],\n",
    "                  [0.1,0.1,0.8],\n",
    "                  [1/3,1/3,1/3]])\n",
    "\n",
    "print(pd.DataFrame(data=theta, index = range(1, 5), columns = ['uppercase', 'lowercase', 'number']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a36ef4-3983-4cd0-a7fe-458d057f6ff0",
   "metadata": {},
   "source": [
    "### Generative process: sample \"fake\" corpus from given model\n",
    "\n",
    "Using the generative mechanism of the model we could sample a possible corpus from this particular model (we assume that each document is 20 \"words\" long):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996c8217-a1db-433e-a687-413006f7431b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "\n",
    "doc_length = 100\n",
    "\n",
    "K = theta.shape[0]   # number of topics, in this example K=3\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "for d in range(theta.shape[0]):         # 4 documents\n",
    "    doc = []\n",
    "    for i in range(doc_length):    # for each word position in doc 'd'\n",
    "        \n",
    "        # sample topic according to a multinomial with parameters given by theta[d, :]\n",
    "        z = np.argmax(np.random.multinomial(1, theta[d,:]))  # z = 0,1,2 with probabilites given by theta[d,:]\n",
    "        #print(f'chosen topic for document {d} is: {z}')\n",
    "\n",
    "        # sample word according to topic z sampled in previous step\n",
    "        w = np.argmax(np.random.multinomial(1, beta[z,:]))\n",
    "        #print(f'chosen word from topic {z} for document {d} is: {w} meaning {words[w]}')\n",
    "        \n",
    "        # append generated word to current document\n",
    "        doc.append(words[w])\n",
    "        \n",
    "    # append generated document to corpus\n",
    "    corpus.append(doc)\n",
    "\n",
    "# dump corpus..\n",
    "for doc in corpus:\n",
    "    print(' '.join(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd22e94c-c939-4a5d-9251-c4f6d1f5b468",
   "metadata": {},
   "source": [
    "#### Compute frequency table for sampled corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d70c121-9d53-4b1f-be9a-2d9325493330",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = [[doc.count(w) for w in words] for doc in corpus]  # frequencies\n",
    "\n",
    "# to pretty print\n",
    "print(pd.DataFrame(data = freq, columns = [w for w in words], index = range(1, theta.shape[0] + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cdf3ae-6a53-4828-8f95-09eb756cab88",
   "metadata": {},
   "source": [
    "## 3. EM algorithm for learning the parameters.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4d05b5-80a0-42d0-ae71-48422125f53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = theta.shape[0]    # 4 documents\n",
    "M = beta.shape[1]     # 6 words\n",
    "K = theta.shape[1]    # 3 topics\n",
    "\n",
    "print(f'{N} documents, {M} words, {K} topics')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ed992e-d642-47db-9d3b-adc3333a7920",
   "metadata": {},
   "source": [
    "### set up counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a5a56f-6368-4f73-b823-93c1c01fb256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency counts are stored in n[d,w] array\n",
    "n = np.array(freq) # make numpy array from list of lists..\n",
    "print(\"frequency couts:\", n)\n",
    "\n",
    "# useful to compute total counts for document..\n",
    "nd = n.sum(axis = 1)\n",
    "print(\"length of docs:\", nd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca15b6ad-4fb8-44aa-bd8a-2094b6f939c7",
   "metadata": {},
   "source": [
    "### initialize parameters randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9847db-e2b2-4c21-a6fa-3b1ef752a0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_rand():\n",
    "    # theta[d, k] : p(z=k|d)\n",
    "    _theta = np.random.rand(N, K)\n",
    "    d_sums = _theta.sum(axis = 1)\n",
    "    _theta /= d_sums[:, np.newaxis]     # normalize appropriately\n",
    "\n",
    "    # beta[k, w] : p(w|z=k)\n",
    "    _beta = np.random.rand(K, M)\n",
    "    w_sums = _beta.sum(axis = 1)\n",
    "    _beta /= w_sums[:, np.newaxis]      # normalize appropriately\n",
    "\n",
    "    return _theta, _beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400dcc28-b840-494f-849d-7e00bcfa6811",
   "metadata": {},
   "source": [
    "### E-step\n",
    "\n",
    "The E-step computes the posterior distribution $P(z=k|d, w)$ for all $d,w$ in the corpus, given current values of $\\theta$ and $\\beta$\n",
    "matrices. It uses the formula\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(z=k|d,w) &= \\frac{P(z=k,d,w)}{P(d,w)} \\\\\n",
    "  &\\propto P(z=k,d,w) \\\\\n",
    "  &\\propto  \\theta_{d,k}\\ \\beta_{k,w}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In the code below, we can observe how the `_post` array is filled out using this formula, and afterwords the resulting result is normalized to obtain a proper probability distribution (the normalization is accross the first dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4c4f9e-d966-4656-bc9b-955e2e9a6ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EStep(theta, beta):\n",
    "    \n",
    "    # _post[k,d,w] : p(z=k|d,w) posterior of latent z!    \n",
    "    _post = np.zeros((K,N,M))\n",
    "    \n",
    "    for d in range(N):\n",
    "        for w in range(M):\n",
    "            for k in range(K):\n",
    "                _post[k, d, w] =  theta[d, k] * beta[k, w]\n",
    "\n",
    "    # now, we normalize using first axis..\n",
    "    k_sums = _post.sum(axis = 0)\n",
    "    _post /= k_sums[np.newaxis,:,:]\n",
    "\n",
    "    return _post"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3a7daf-3748-4a50-8f18-f1dcbe8e7c2a",
   "metadata": {},
   "source": [
    "### M-step\n",
    "\n",
    "The M-step updates the parameter values in the matrices $\\theta$ and $\\beta$ based on the updated latent variable posteriors. It does so\n",
    "by invoking maximum likelihood:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\beta_{k,w} = P(w|z=k)  &=  \\frac{\\sum_d n(d,w) P(z=k|d,w)}{\\sum_{w'} \\sum_d n(d,w') P(z=k|d,w')}\\\\\n",
    "\\theta_{d,k} = P(z=k|d) &= \\frac{\\sum_w n(d,w) P(z=k|d,w)}{\\sum_{k'} \\sum_w n(d,w) P(z=k'|d,w)}\n",
    "    = \\frac{\\sum_w n(d,w) P(z=k|d,w)}{\\sum_w n(d,w)} = \\frac{\\sum_w n(d,w) P(z=k|d,w)}{n(d)}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e1023c-d451-4556-9aa1-5d44e6d2285d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MStep(post):\n",
    "    \n",
    "    # re-estimate theta\n",
    "    _theta = np.zeros([N, K])\n",
    "    for k in range(K):\n",
    "        for d in range(N):\n",
    "            for w in range(M):\n",
    "                _theta[d,k] += n[d,w] * post[k,d,w]\n",
    "            _theta[d,k] /= nd[d]\n",
    "\n",
    "    # re-estimate beta\n",
    "    _beta = np.zeros([K, M])\n",
    "    for k in range(K):\n",
    "        for d in range(N):\n",
    "            for w in range(M):\n",
    "                _beta[k,w] += n[d,w] * post[k,d,w]\n",
    "    # now normalize by axis = 1\n",
    "    w_sums = _beta.sum(axis = 1)\n",
    "    _beta /= w_sums[:, np.newaxis]\n",
    "    \n",
    "    return _theta, _beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e31965c-2ec1-4f26-8413-1d5f9cbd7b36",
   "metadata": {},
   "source": [
    "### Log-likelihood\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "   {\\cal L}     &=  \\sum_{d\\in{\\cal D}} \\sum_{w\\in {\\cal W}} n(d,w) \\log \\sum_k{ P(z=k|d) P(w|z=k)}\\\\\n",
    "        &= \\sum_{d\\in{\\cal D}} \\sum_{w\\in {\\cal W}} n(d,w) \\log \\sum_k{ \\theta_{d,k}\\ \\beta_{k,w}}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e0ad37-2ce9-4fab-8826-7212506f7843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogLikelihood(theta, beta):\n",
    "    tmp = 0\n",
    "    for d in range(N):\n",
    "        for w in range(M):\n",
    "            tmp += n[d,w] * np.log(theta[d,:].dot(beta[:,w]))\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc63bb5-4efa-4eba-8fee-3cc47c98ccd8",
   "metadata": {},
   "source": [
    "### EM algorithm\n",
    "\n",
    "Finally, we have all elements to implement EM for pLSI. As stated in the lecture notes:\n",
    "\n",
    "1. Randomly initialize $P(w|z)$ (matrix $\\beta$) and $P(z|d)$ (matrix $\\theta$)\n",
    "2. Iterate until convergence:\n",
    "    - E-step: _// re-compute posterior for latent variable_ $z$\n",
    "        - for all $k$, compute: $P(z=k|d,w) = \\frac{\\theta_{d,k}\\ \\beta_{k,w}}{\\sum_{k'}\\theta_{d,k'}\\ \\beta_{k',w}}$\n",
    "    - M-step: _// re-estimate parameters based on new posterior _\n",
    "        - for all $k$, compute $\\beta_{k,w} =  \\frac{\\sum_d n(d,w) P(z=k|d,w)}{\\sum_{w'}\\sum_d n(d,w') P(z=k|d,w')}$\n",
    "        - for all $k$, compute $\\theta_{d,k} = \\frac{\\sum_w n(d,w) P(z=k|d,w)}{n(d)}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b664ff-c70f-400b-8d8a-ff051fa46a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "em_theta, em_beta = init_rand()\n",
    "logl = LogLikelihood(em_theta, em_beta)\n",
    "print(f'log-likelihood: {logl:.2f}')\n",
    "start = time.time()\n",
    "it = 0\n",
    "for _ in range(100):\n",
    "    t = time.time()\n",
    "    z_post = EStep(em_theta, em_beta)\n",
    "    em_theta, em_beta = MStep(z_post)\n",
    "    elapsed = time.time() - t\n",
    "    \n",
    "#    print(f\"iteration {i} took {elapsed:.2f} s.\")\n",
    "    old_logl = logl\n",
    "    logl = LogLikelihood(em_theta, em_beta)\n",
    "    print(f'log-likelihood: {logl:.2f}')\n",
    "    \n",
    "    # break of converged..\n",
    "    it += 1\n",
    "    if np.isclose(logl, old_logl):\n",
    "        break\n",
    "    \n",
    "print(f\"EM took {time.time()-start:.2f} s. and converged in {it} iterations.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f5c63a-9820-48c7-ae20-583f29b5fba9",
   "metadata": {},
   "source": [
    "## 4. Inspect learned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db9cf5f-0811-4770-a0f0-50b4567fb031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print word distributions for each topic, compare to true dist.\n",
    "print(f'Learned word distributions:')\n",
    "print(pd.DataFrame(data=em_beta, columns = [w for w in words]))\n",
    "print()\n",
    "print(f'True word distributions:')\n",
    "print(pd.DataFrame(data=beta, columns = [w for w in words], index=['uppercase', 'lowercase', 'number']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbd03aa-1c0e-4805-a365-4c6eef2e2679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print topic proportions for each document, compare to true dist.\n",
    "print(f'Learned topic proportions:')\n",
    "print(pd.DataFrame(data = em_theta, index=range(1,5)))\n",
    "print()\n",
    "print(f'True topic proportions:')\n",
    "print(pd.DataFrame(data = theta, index=range(1,5), columns=['uppercase', 'lowercase', 'number']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175b9dc0-1b66-4e32-bbd0-e289c66bfd0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
