{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AA1 lab 10\n",
    "\n",
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:01:00.852570Z",
     "start_time": "2020-07-15T10:01:00.843613Z"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment to upgrade packages\n",
    "# !pip install pandas --upgrade --user --quiet\n",
    "# !pip install numpy --upgrade --user --quiet\n",
    "# !pip install scipy --upgrade --user --quiet\n",
    "# !pip install statsmodels --upgrade --user --quiet\n",
    "# !pip install scikit-learn --upgrade --user --quiet\n",
    "# !pip install tensorflow --user\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:01:01.681335Z",
     "start_time": "2020-07-15T10:01:00.853925Z"
    }
   },
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from collections import Counter\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "pd.set_option('display.precision', 3)\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:01:02.521836Z",
     "start_time": "2020-07-15T10:01:01.683466Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extra imports\n",
    "from numpy.random import  uniform,normal\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import calinski_harabasz_score, silhouette_score, davies_bouldin_score\n",
    "from numpy.random import multivariate_normal\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from time import time\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:01:02.525890Z",
     "start_time": "2020-07-15T10:01:02.523330Z"
    }
   },
   "outputs": [],
   "source": [
    "# turn off annoying warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# for reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 1. Clustering easy artificial 2D data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a simple data set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:01:02.647635Z",
     "start_time": "2020-07-15T10:01:02.527783Z"
    }
   },
   "outputs": [],
   "source": [
    "K = 3 \n",
    "N = 100\n",
    "\n",
    "data, labels = make_blobs(n_samples=N, n_features=2, centers=K, random_state=42)\n",
    "data = pd.DataFrame(data, columns=['X', 'Y'])\n",
    "data['labels'] = labels\n",
    "sns.scatterplot(x='X', y='Y', hue='labels', data=data);\n",
    "t = plt.title('Ground Truth');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " so we have 3 very clean clusters ...\n",
    "\n",
    "Let's execute k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:01:03.703918Z",
     "start_time": "2020-07-15T10:01:03.701871Z"
    }
   },
   "outputs": [],
   "source": [
    "K = 3 # yeah, we are cheating .. why 3?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " execute k-means with a maximum of 100 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:01:03.843596Z",
     "start_time": "2020-07-15T10:01:03.705124Z"
    }
   },
   "outputs": [],
   "source": [
    "kmeans_3 = KMeans(n_clusters=3, max_iter=100)\n",
    "kmeans_3.fit(data.loc[:,['X','Y']]);\n",
    "data['kmeans3_labels'] = kmeans_3.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot and paint the clusters (according to the computed assignments) and plot the cluster centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:01:04.041455Z",
     "start_time": "2020-07-15T10:01:03.845453Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x='X', y='Y', hue='kmeans3_labels', data=data);\n",
    "plt.plot(kmeans_3.cluster_centers_[:,0], kmeans_3.cluster_centers_[:,1], 'bo', markersize=12,alpha=0.7);\n",
    "plt.title('K Means predictions with K=3');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that the clusters correspond exactly to the ground truth, so we can say that we have a very good clustering model there. \n",
    "Usually we will not have the ground truth to compare with, and most times the data will have more than two dimensions, which difficults compare the clusters on a visualization. Also, on a real problem we will not know the best number of clusters.\n",
    "\n",
    "To handle theses issues we will use metrics to decide if we have a good clustering or a bad one, and to chose between different models. \n",
    "\n",
    "### Metrics\n",
    "\n",
    "We will use three different metrics to evaluate our perfect clustering. \n",
    "\n",
    "**Calinski-Harabasz index**: \n",
    "\n",
    "This index measures the dispersion of the data points within the clusters (SSW) and between the clusters (SSB)\n",
    "\n",
    "A good clustering has small SSW (compact clusters) and large SSB (separated cluster centers). So we want a CH as big as possible.\n",
    "\n",
    "There is also a correction for the number of clusters\n",
    " \n",
    "The CH index is then:\n",
    "\n",
    " $$\n",
    " CH = \\frac{SSB/(K-1)}{SSW/(N-K)}\n",
    " $$\n",
    "\n",
    " where $N$ is the number of data points and $K$ is the number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index= pd.MultiIndex.from_arrays([['kmeans'], [3]], names=('model', 'K'))\n",
    "\n",
    "results_df = pd.DataFrame(index=index, columns= ['CH score', 'Silhouette score', 'DB score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:01:04.118546Z",
     "start_time": "2020-07-15T10:01:04.043806Z"
    }
   },
   "outputs": [],
   "source": [
    "CH_3 = calinski_harabasz_score(data.loc[:,['X','Y']], kmeans_3.labels_ )\n",
    "\n",
    "results_df.loc[('kmeans',3), 'CH score'] = CH_3\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Silhouette Coefficient**\n",
    "\n",
    "This metric is based on distances between the clusters. It uses the Mean Intra cluster distance (I) and Mean Nearest Cluster distance (N) for each sample as $S_i = \\frac{N_i-I_i}{max(N_i,I_i)}$. Then silhouette index is computed as the mean of all these values . \n",
    "\n",
    "$$\n",
    "S = mean_i(S_i) = mean_i(\\frac{N_i-I_i}{max(N_i,I_i)})\n",
    "$$\n",
    "\n",
    "The best value for this metric is 1 ($I=0, N>0$) and the worse value is -1 ($I>0, N=0$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_3 = silhouette_score(data.loc[:,['X','Y']], kmeans_3.labels_ )\n",
    "\n",
    "results_df.loc[('kmeans', 3), 'Silhouette score'] = S_3\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Davies-Bouldin score**\n",
    "\n",
    "Is defined as the average similarity measure for ach cluster with its most similar cluster. \n",
    "\n",
    "Similarity is computed as the ratio of within cluster distances to between cluster distances. \n",
    "\n",
    "The best value for this meassure is 0, which means condensed clusters far awai from each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_3 = davies_bouldin_score(data.loc[:,['X','Y']], kmeans_3.labels_ )\n",
    "\n",
    "results_df.loc[('kmeans', 3), 'DB score'] = DB_3\n",
    "results_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that, even though we have a perfect clustering acording to the ground truth, we have not obtained a perfect score on any of the metrics. \n",
    "\n",
    "This is because measuring the performance of an unsupervised is _difficult_. \n",
    "These metrics have been defined to try to measure some characteristics that we asociate to a good clustering, but they will not alwais be the best for each context.   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's not _\"cheat\"_ choosing the best k. We are going to try some k values and see their clustering results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:01:04.222773Z",
     "start_time": "2020-07-15T10:01:04.119813Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def compute_clustering_and_plot(K, data, results):\n",
    "\n",
    "    # Train the model with k-means with given K\n",
    "    kmeans = KMeans(n_clusters=K, max_iter=100)\n",
    "    kmeans.fit(data.loc[:,['X','Y']]);\n",
    "    data['kmeans_labels'] = kmeans.labels_\n",
    "\n",
    "    # Plot results\n",
    "    sns.scatterplot(x='X', y='Y', hue='kmeans_labels', data=data);\n",
    "    plt.plot(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], 'bo', markersize=12,alpha=0.7);\n",
    "    plt.xlim(plt.ylim());\n",
    "    plt.title(f'K Means predictions with K={K}');\n",
    "    plt.show();\n",
    "\n",
    "    # Compute metrics\n",
    "    CH = calinski_harabasz_score(data.loc[:,['X','Y']], kmeans.labels_ )\n",
    "    S = silhouette_score(data.loc[:,['X','Y']], kmeans.labels_ )\n",
    "    DB = davies_bouldin_score(data.loc[:,['X','Y']], kmeans.labels_ )\n",
    "    \n",
    "    # store metrics\n",
    "    results.loc[('kmeans', k),:] = [CH,S,DB]\n",
    "    return results\n",
    "\n",
    "K_values = [2,3,5,15,20]\n",
    "\n",
    "for k in K_values:\n",
    "    results = compute_clustering_and_plot(k, data,results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.sort_values(by='CH score', ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see on our results that according to all three metrics the best value for K is 3. Well done K-means! But this was very easy data to cluster.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 2. Clustering not-so-easy artificial 2D data with k-means and E-M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "K = 5\n",
    "N = 1000\n",
    "center = np.array((0,0))\n",
    "dispersion = 25\n",
    "\n",
    "# get random centers\n",
    "mu_k= multivariate_normal(center, np.eye(2)*dispersion,K)\n",
    "# get random standard devs for each cluster\n",
    "sigma_k = uniform(0.2, 1.5, size=K)\n",
    "# create clusters (with isotropic Gaussians); same density each cluster (N/5 points in each)\n",
    "data, labels = make_blobs(n_samples=N, n_features=2, centers=mu_k, cluster_std=sigma_k)\n",
    "\n",
    "# Rotate and scale (all clusters equally..)\n",
    "linear_transform =  np.array([[1,0.4],[0.2,0.6]])\n",
    "data = np.dot(data, linear_transform) \n",
    "mu_k= np.dot(mu_k, linear_transform)\n",
    "\n",
    "data = pd.DataFrame(data, columns=['X', 'Y'])\n",
    "data['labels']=labels\n",
    "\n",
    "# plot generated data\n",
    "sns.scatterplot(x='X', y='Y',hue='labels', data=data);\n",
    "plt.ylim(plt.xlim());\n",
    "plt.title('Ground Truth');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate 2D data as a mixture of 5 Gaussians, The rotation makes the two variable non independent with different variances"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at the unconditional density p(x) by computing the 2D kernel density. This is the raw data (what the clustering method sees) and a contour plot of the unconditional density. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:01:06.050168Z",
     "start_time": "2020-07-15T10:01:04.978945Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x='X', y='Y', data=data);\n",
    "plt.title('Kernel Density');\n",
    "p = sns.kdeplot(x='X', y='Y', data=data, n_levels=15, cmap='afmhot', alpha=0.5);\n",
    "plt.ylim(plt.xlim());\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were working with real data we would not have labels for our samples. Let's try to find the best clustering for  our data without using this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:01:06.250331Z",
     "start_time": "2020-07-15T10:01:06.053972Z"
    }
   },
   "outputs": [],
   "source": [
    "p = sns.scatterplot(x='X', y='Y', data=data);\n",
    "plt.ylim(plt.xlim());\n",
    "plt.title('Our data');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try first with k-means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index= pd.MultiIndex.from_arrays([['kmeans'], [3]], names=('model', 'K'))\n",
    "\n",
    "results_df2 = pd.DataFrame(index=index, columns= ['CH score', 'Silhouette score', 'DB score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "K_values = [2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "\n",
    "for k in K_values:\n",
    "    results_df2 = compute_clustering_and_plot(k, data, results_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df2.sort_values(by='CH score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our metrics are not so sure.Silhouette and DB metrics agree that the best k is 5 but according to CH score the best K would be 10.\n",
    "\n",
    "How can this be happening? \n",
    "\n",
    "CH score on the other hand is being tricked more smaller clusters as points are closer to their centroids this way. \n",
    "\n",
    "If we check the evolution of our metrics when we increase the k we can see that according to CH score k=5 would be a good candidate, as there is not a lot of difference on the performance betwen k=5 and k > 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:01:11.520608Z",
     "start_time": "2020-07-15T10:01:07.469494Z"
    }
   },
   "outputs": [],
   "source": [
    "results_df2 = results_df2.sort_index()\n",
    "results_df2 = results_df2.astype(float)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(16,6))\n",
    "\n",
    "sns.lineplot(x='K', y='CH score',data=results_df2.reset_index(),label='CH score', color='r', ax=ax[0]);\n",
    "sns.lineplot(x='K', y='Silhouette score',data=results_df2.reset_index(),label='Silhouette',ax=ax[1]);\n",
    "sns.lineplot(x='K', y='DB score',data=results_df2.reset_index(),label='DB');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the real *shape* of the clusters cannot be captured, because k-means only \"sees\" spherical clusters and these are ellipsoidal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=5\n",
    "kmeans = KMeans(n_clusters=K, max_iter=100)\n",
    "kmeans.fit(data.loc[:,['X','Y']]);\n",
    "data['kmeans_labels'] = kmeans.labels_\n",
    "\n",
    "fix, axes = plt.subplots(1,2,figsize=(12,5))\n",
    "\n",
    "# Plot results\n",
    "sns.scatterplot(x='X', y='Y', hue='kmeans_labels', data=data,ax=axes[0]);\n",
    "\n",
    "axes[0].set_xlim(axes[0].get_ylim())\n",
    "axes[0].plot(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], 'bo', markersize=12,alpha=0.7);\n",
    "axes[0].set_title('K Means predictions with K={}'.format(K));\n",
    "\n",
    "sns.scatterplot(x='X', y='Y',hue='labels', data=data,ax=axes[1]);\n",
    "axes[1].set_xlim(axes[1].get_ylim())\n",
    "axes[1].set_title('Ground Truth');\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try EM mixture of gaussians models to try to get a better clustering."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EM Mixture of Gaussians**\n",
    "\n",
    "This method performs E-M for mixture densities, including mixtures of Gaussians\n",
    "we can specify which family of gaussians we intend to fit:\n",
    "\n",
    "* \"full\" each component has full covariance matrix, \n",
    "* \"diagonal\" covariances are diagonal (just variance), \n",
    "*  \"spherical\"  each component has his own diagonal univariance and \"tied\" all components share variance\n",
    "\n",
    "    WARNING: default is \"full\".\n",
    "\n",
    " suppose first that we specify axis-aligned densities (i.e., independent variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:01:11.563004Z",
     "start_time": "2020-07-15T10:01:11.522356Z"
    }
   },
   "outputs": [],
   "source": [
    "gm = GaussianMixture(n_components=5, covariance_type='diag').fit(data.loc[:,['X','Y']])\n",
    "print('\\nLOG Likelihood=', gm.lower_bound_)\n",
    "print('\\nWEIGHTS=')\n",
    "pd.DataFrame(gm.weights_)\n",
    "print('\\nMEANS=')\n",
    "pd.DataFrame(gm.means_)\n",
    "print('\\nCOV=')\n",
    "pd.DataFrame(gm.covariances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This is a graphical summary of the clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:01:11.839845Z",
     "start_time": "2020-07-15T10:01:11.564423Z"
    }
   },
   "outputs": [],
   "source": [
    "data['gm_labels'] = gm.predict(data.loc[:,['X','Y']])\n",
    "\n",
    "fix, axes = plt.subplots(1,2,figsize=(12,5))\n",
    "\n",
    "# Plot results\n",
    "sns.scatterplot(x='X', y='Y', hue='gm_labels', data=data,ax=axes[0]);\n",
    "\n",
    "axes[0].set_xlim(axes[0].get_ylim())\n",
    "axes[0].plot(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], 'bo', markersize=12,alpha=0.7);\n",
    "axes[0].set_title('GM predictions with K={}'.format(5));\n",
    "\n",
    "sns.scatterplot(x='X', y='Y',hue='labels', data=data,ax=axes[1]);\n",
    "axes[1].set_xlim(axes[1].get_ylim())\n",
    "axes[1].set_title('Ground Truth');\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " it was very likely that E-M performed extremely well\n",
    " why? because we knew the truth (cluster form and number)\n",
    "\n",
    " suppose now we fit general gaussians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:01:11.880494Z",
     "start_time": "2020-07-15T10:01:11.841836Z"
    }
   },
   "outputs": [],
   "source": [
    "gm =GaussianMixture(n_components=5, covariance_type='full').fit(data.loc[:,['X','Y']])\n",
    "print('\\nLOG Likelihood=', gm.lower_bound_)\n",
    "print('\\nWEIGHTS=')\n",
    "pd.DataFrame(gm.weights_)\n",
    "print('\\nMEANS=')\n",
    "pd.DataFrame(gm.means_)\n",
    "print('\\nCOV=')\n",
    "gm.covariances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:01:12.132493Z",
     "start_time": "2020-07-15T10:01:11.882720Z"
    }
   },
   "outputs": [],
   "source": [
    "data['gm_labels'] = gm.predict(data.loc[:,['X','Y']])\n",
    "\n",
    "fix, axes = plt.subplots(1,2,figsize=(12,5))\n",
    "\n",
    "# Plot results\n",
    "sns.scatterplot(x='X', y='Y', hue='gm_labels', data=data,ax=axes[0]);\n",
    "axes[0].set_xlim(axes[0].get_ylim())\n",
    "axes[0].plot(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], 'bo', markersize=12,alpha=0.7);\n",
    "axes[0].set_title('GM predictions with K=5');\n",
    "\n",
    "sns.scatterplot(x='X', y='Y',hue='labels', data=data,ax=axes[1]);\n",
    "axes[1].set_xlim(axes[1].get_ylim())\n",
    "axes[1].set_title('Ground Truth');\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " the method works very smoothly\n",
    " why? because the data *is* gaussian\n",
    "\n",
    "compare the estimated centers with the truth (note the clusters may appear in a different order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat({'Estimated Means':pd.DataFrame(gm.means_).sort_values(by=0,ignore_index=True), 'Truth means':pd.DataFrame(mu_k).sort_values(by=0,ignore_index=True)},axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or the estimated coefficients  with the truth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:01:12.510010Z",
     "start_time": "2020-07-15T10:01:12.356855Z"
    }
   },
   "outputs": [],
   "source": [
    "c= Counter(labels)\n",
    "pd.DataFrame({'estimated coefficients':gm.weights_, 'truth': np.array([c[v] for v in c])/N})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our model has found the real clusters of our data almost perfectly. \n",
    "\n",
    "Mixture of gaussians can return the clusters as an array of probability of belonging to each of the possible clusters, so we can see now \"how sure\" is our model about its predictions for each class. These are what we called _soft assignments_ in class.\n",
    "\n",
    "As you can see on these plots, the model is more sure about the predictions of \"easy\" clusters and it is less sure about the predictions of difficult ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "prob_prediction = gm.predict_proba(data.loc[:,['X','Y']])\n",
    "data['gm_labels_pred_0'] = prob_prediction[:,0]\n",
    "data['gm_labels_pred_1'] = prob_prediction[:,1]\n",
    "data['gm_labels_pred_2'] = prob_prediction[:,2]\n",
    "data['gm_labels_pred_3'] = prob_prediction[:,3]\n",
    "data['gm_labels_pred_4'] = prob_prediction[:,4]\n",
    "\n",
    "fix, axes = plt.subplots(3,2,figsize=(12,16))\n",
    "axes = axes.reshape(-1)\n",
    "# Plot results\n",
    "for i in range(0,5):\n",
    "    sns.scatterplot(x='X', y='Y', hue=f'gm_labels_pred_{i}', data=data,ax=axes[i]);\n",
    "    axes[i].set_xlim(axes[0].get_ylim())\n",
    "    axes[i].plot(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], 'bo', markersize=12,alpha=0.7);\n",
    "    axes[i].set_title(f'GM predictions of class {i} with K=5');\n",
    "sns.scatterplot(x='X', y='Y',hue='labels', data=data, ax=axes[-1]);\n",
    "axes[-1].set_xlim(axes[1].get_ylim())\n",
    "axes[-1].set_title('Ground Truth');\n",
    "plt.show();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 3. Real life example\n",
    "\n",
    "Now we are going to try these models on a real dataset. We will be using the fashion mnist dataset, a small image dataset for clasification. The idea for using this\n",
    "labelled dataset (meant for classification) is that we can view the classes as forming clusters, so we can in a way evaluate the results of the clustering algorithms. In essence, we will see whether clusters found contain images belonging to the same type of pictures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels_names={0:\"T-shirt/top\",\n",
    "1:\"Trouser\",\n",
    "2:\"Pullover\",\n",
    "3:\"Dress\",\n",
    "4:\"Coat\",\n",
    "5:\"Sandal\",\n",
    "6:\"Shirt\",\n",
    "7:\"Sneaker\",\n",
    "8:\"Bag\",\n",
    "9:\"Ankle boot\"}\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "h= 28\n",
    "w=28\n",
    "n = x_test.shape[0]\n",
    "real_k = 10\n",
    "\n",
    "data = x_test.reshape(n, h*w)   # rows are images, columns are (flattened) pixel values\n",
    "labels = np.array(list(map(labels_names.get, y_test)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see a subset of our data. There are classes looking quite similar and classes that look completely different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random_subset_of_data(data,labels):\n",
    "    fig, axes = plt.subplots(10,10,figsize=(12, 12));\n",
    "    axes = axes.reshape(-1);\n",
    "    for i in range(10*10):\n",
    "        idx = np.random.randint(data.shape[0]);  \n",
    "        axes[i].imshow(data[idx,:].reshape(h,w), cmap=plt.cm.gray);\n",
    "        axes[i].set_title(labels[idx]);\n",
    "        axes[i].set_axis_off();\n",
    "    fig.tight_layout();\n",
    "    plt.show();\n",
    "    \n",
    "plot_random_subset_of_data(data,labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try first to find clusters with K means. A priori we don't know how to set K (well we do know that there are 10 classes however some of these may be clustered together if they are \"similar\"). So we are going to try out different K values. \n",
    "\n",
    "Our data is composed by small images, the shape of these images could not be ideal for a kmeans model (based on distances). \n",
    "\n",
    "We can see in the heatmap of our data that images have strong patterns, this is probably consequence of the black background of our samples (pixel value codified as 0). \n",
    "\n",
    "Also, pixel ranges are quite wide, it could be a good idea to scale them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,7));\n",
    "sns.heatmap(data);\n",
    "plt.title('Heatmap of our data');\n",
    "plt.xlabel('Pixel value');\n",
    "plt.ylabel('Sample (i.e. image)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix these problems we are going to use PCA. By using PCA on our preprocessing we obtain a denser dataset and reduce the number of variables. It also is controling the range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA();\n",
    "# We scale the data to improve PCA performance. \n",
    "pca.fit(minmax_scale(data));\n",
    "components = np.sum(np.cumsum(pca.explained_variance_ratio_) <= 0.9)\n",
    "print(f'If we maintain 90% of the variance we obtain {components} principal components, not bad at all!')\n",
    "pca = PCA(n_components=components);\n",
    "data_pca = pca.fit_transform(minmax_scale(data));\n",
    "\n",
    "print('Original shape:', data.shape,'New shape:',data_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,7));\n",
    "sns.heatmap(data_pca);\n",
    "plt.title('Heatmap of our data');\n",
    "plt.xlabel('Components');\n",
    "plt.ylabel('Sample');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have processed the data we can try our first model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index= pd.MultiIndex.from_arrays([[], []], names=('model', 'K'))\n",
    "\n",
    "results_df3 = pd.DataFrame(index=index, columns= ['CH score', 'Silhouette score', 'DB score']);\n",
    "\n",
    "for k in [2, 5, 10, 15]:\n",
    "    #init_train_time = time();\n",
    "    kmeans = KMeans(n_clusters=k,max_iter=1000);\n",
    "    kmeans.fit(data_pca);\n",
    "    #print('Training time for k={}: {}'.format(k, timedelta(seconds=time()-init_train_time)));\n",
    "\n",
    "    CH = calinski_harabasz_score(data, kmeans.labels_ );\n",
    "    S = silhouette_score(data, kmeans.labels_ );\n",
    "    DB = davies_bouldin_score(data, kmeans.labels_ );\n",
    "    \n",
    "    results_df3.loc[('kmeans', k),:] = [CH,S,DB];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df3.sort_values(by='CH score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df3 = results_df3.sort_index()\n",
    "results_df3 = results_df3.astype(float)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(16,6))\n",
    "\n",
    "sns.lineplot(x='K', y='CH score',data=results_df3.reset_index(),label='CH score', color='r', ax=ax[0]);\n",
    "sns.lineplot(x='K', y='Silhouette score',data=results_df3.reset_index(),label='Silhouette',ax=ax[1]);\n",
    "sns.lineplot(x='K', y='DB score',data=results_df3.reset_index(),label='DB');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our metrics are not showing good results. It could be that our clusters are not making sense or that our metrics do not work well with our data. \n",
    "\n",
    "Accoriding to our metrics best k is k=2. We can now look at our clusters to see if they make any sense. Cluster analysis depends strongly of the kind of data you are working with, as we are clustering images the only way we have to check if the clusters make sense is to check sample images of each of the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2,max_iter=5000);\n",
    "kmeans.fit(data_pca);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case we have two clusters: 0 and 1\n",
    "cluster_data1 = data[kmeans.labels_ == 1,:]\n",
    "cluster_labels1 = labels[kmeans.labels_ == 1]\n",
    "cluster_data0 = data[kmeans.labels_ == 0,:]\n",
    "cluster_labels0 = labels[kmeans.labels_ == 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_random_subset_of_data(cluster_data0, cluster_labels0)\n",
    "plot_random_subset_of_data(cluster_data1, cluster_labels1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case it looks like the model has made the clustering based on \"dark\" vs \"light\" images.\n",
    "\n",
    "We can use an histogram of the mean pixel value of our images to validate this theory. Darker images will be closer to zero and lighter images will be closer to 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pixel_value_by_image_cluster_1 = np.mean(data[kmeans.labels_ == 1,:],axis=1)\n",
    "mean_pixel_value_by_image_cluster_0 = np.mean(data[kmeans.labels_ == 0,:],axis=1)\n",
    "\n",
    "plt.title('Mean pixel value of each sample');\n",
    "sns.histplot(mean_pixel_value_by_image_cluster_0,label='cluster 0',color='r');\n",
    "sns.histplot(mean_pixel_value_by_image_cluster_1,label='cluster 1');\n",
    "plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want a better/more complex clustering criteria. Let's try a Mixture of Gaussians model.\n",
    "We will assume full covariance for a lack of further information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [2, 3, 5, 10, 15]:\n",
    "    #init_train_time = time();\n",
    "    gm = GaussianMixture(n_components=k, covariance_type='full')\n",
    "    gm.fit(data_pca);\n",
    "    #print('Trainin time for k={}: {}'.format(k, timedelta(seconds=time()-init_train_time)));\n",
    "    prediction = gm.predict(data_pca) \n",
    "    CH = calinski_harabasz_score(data,prediction )\n",
    "    S = silhouette_score(data, prediction)\n",
    "    DB = davies_bouldin_score(data,prediction)\n",
    "    \n",
    "    results_df3.loc[('GM', k),:] = [CH,S,DB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df3.sort_values(by='CH score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the best k is 2 again. Also, our metrics were slightly better with kmeans. Let's see our clusters now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm =GaussianMixture(n_components=2, covariance_type='full')\n",
    "gm.fit(data_pca);\n",
    "prediction = gm.predict(data_pca) \n",
    "prob_prediction =  gm.predict_proba(data_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_data0 = data[prediction == 0,:]\n",
    "cluster_labels0 = labels[prediction == 0]\n",
    "cluster_data1 = data[prediction == 1,:]\n",
    "cluster_labels1 = labels[prediction == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_random_subset_of_data(cluster_data0, cluster_labels0)\n",
    "plot_random_subset_of_data(cluster_data1, cluster_labels1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time it looks like the model is separating shoes vs no shoes. Which is not bad taking into account that is using pixel data only. \n",
    "This time is more difficult to visualize our probabilities, so we are going to use two principal components to visualize them.\n",
    "\n",
    "In our representation we can see a quite clear cluster separation, also our model looks to be quite sure of its predictions this time for almost all the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "data_plot = pca.fit_transform(data)\n",
    "data_plot = pd.DataFrame(data_plot,columns = ['x','y'])\n",
    "data_plot['labels'] = labels\n",
    "data_plot['prediction'] = prediction\n",
    "data_plot['prediction_prob_0'] = prob_prediction[:,0]\n",
    "data_plot['prediction_prob_1'] = prob_prediction[:,1]\n",
    "\n",
    "prob_prediction =  gm.predict_proba(data_pca)\n",
    "\n",
    "fig, axes = plt.subplots(1,2,figsize=(14,6));\n",
    "sns.scatterplot(x='x',y='y', hue ='labels' , data=data_plot,ax=axes[0]);\n",
    "sns.scatterplot(x='x',y='y', hue ='prediction' , data=data_plot,ax=axes[1]);\n",
    "\n",
    "fig, axes = plt.subplots(1,2,figsize=(14,6));\n",
    "sns.scatterplot(x='x',y='y', hue ='prediction_prob_0' , data=data_plot,ax=axes[0],palette='icefire');\n",
    "sns.scatterplot(x='x',y='y', hue ='prediction_prob_1' , data=data_plot,ax=axes[1],palette='icefire');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, as a curiosity we can see how the real labels of our samples would distribute on the clusters. It is nice that such a simple model has been able to separate the classes into clusters that make sense and has put samples of the same class on the same cluster most of the times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.crosstab(labels, prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "382px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
