{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intimate-facial",
   "metadata": {
    "id": "intimate-facial"
   },
   "source": [
    "# AA1 lab 04\n",
    "\n",
    "# Linear Regression: Life Expectancy Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-festival",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 197,
     "status": "ok",
     "timestamp": 1678894301340,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -60
    },
    "id": "catholic-festival",
    "outputId": "e24a2dc7-579b-4efd-bd58-78efa3e27131"
   },
   "outputs": [],
   "source": [
    "# Uncomment to upgrade packages\n",
    "# !pip3 install pandas --user --upgrade --quiet\n",
    "# !pip3 install numpy --user --upgrade --quiet\n",
    "# !pip3 install scipy --user --upgrade --quiet\n",
    "# !pip3 install statsmodels --user --upgrade --quiet\n",
    "# !pip3 install seaborn --user --upgrade --quiet\n",
    "# !pip3 install pillow --user --upgrade -- quiet\n",
    "# !pip3 install matplotlib --user --upgrade --quiet\n",
    "\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-composition",
   "metadata": {
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1678894302009,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -60
    },
    "id": "effective-composition"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from statsmodels.genmod.generalized_linear_model import GLM\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sns.set()\n",
    "pd.set_option(\"display.precision\", 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-imagination",
   "metadata": {
    "id": "backed-imagination"
   },
   "source": [
    "## SECTION 1: Our data\n",
    "\n",
    "In this lab session we are going to predict the life expentancy of different countries during different years given different socio-economic markers.\n",
    "\n",
    "As you can notice this dataset has a temporal component (life expectancy in a country one year can have a correlation with previous and posterior years). Normally this temporal component should be taken into account (breaks the iid assumption for example), however for simplicity we will ignore it. Treating the temporal component is outside the scope of the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-establishment",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1678894302010,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -60
    },
    "id": "selected-establishment",
    "outputId": "0af648e3-de26-4b74-d6c7-26654a2fd08a"
   },
   "outputs": [],
   "source": [
    "# load data from file\n",
    "life_expectancy_data = pd.read_csv(\"Life_Expectancy_Data.csv\")\n",
    "# remove spaces and symbols to avoid problems with statsmodel GLM\n",
    "life_expectancy_data.columns = [\n",
    "    c.lower().strip().replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"-\", \"_\")\n",
    "    for c in life_expectancy_data.columns\n",
    "]\n",
    "\n",
    "# change the type of categorical variables into category\n",
    "categorical_columns = list(\n",
    "    life_expectancy_data.dtypes[life_expectancy_data.dtypes == \"O\"].index.values\n",
    ")\n",
    "for column in categorical_columns:\n",
    "    life_expectancy_data[column] = life_expectancy_data[column].astype(\"category\")\n",
    "\n",
    "# peak into the data\n",
    "life_expectancy_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-binary",
   "metadata": {
    "id": "ranking-binary"
   },
   "source": [
    "It is always a good idea to do a small exploration of the data. Real data needs preprocessing and it is important to understand your dataset to be able to take good design decisions. \n",
    "\n",
    "Our dataset has 2938 samples and 21 predictive variables. Our target is `life_expectancy`. \n",
    "\n",
    "We are going to do a fast visualizacion of the data. In this visualization we use histograms for showing the distribution of the numerical variables and barplots for the categorical ones. \n",
    "\n",
    "With these kinds of easy visualizations we can see a lot of relevant information about our data, like if we have ourliers or if any variable has been wrongly encoded, or whether it \"looks\" Gaussian enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-particle",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 906
    },
    "executionInfo": {
     "elapsed": 26967,
     "status": "ok",
     "timestamp": 1678894328973,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -60
    },
    "id": "modern-particle",
    "outputId": "87d7dff2-bcba-4561-c30a-9a75d50ac6c0"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(6, 4, figsize=(26, 20))\n",
    "\n",
    "# We will not plot country because it has too many categories.\n",
    "for i, c in enumerate(life_expectancy_data.columns[1:]):\n",
    "    ax = axes.reshape(-1)[i]\n",
    "    if life_expectancy_data[c].dtype.kind == \"O\":\n",
    "        a = sns.countplot(x=c, data=life_expectancy_data, ax=ax)\n",
    "    else:\n",
    "        b = sns.histplot(x=c, data=life_expectancy_data, ax=ax)\n",
    "    t = ax.set_title(c)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivated-breast",
   "metadata": {
    "id": "motivated-breast"
   },
   "source": [
    "If you can't plot a effectively a categorical variable because it has too many categories, you can check it with a value_counts function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-interview",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1678894328974,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -60
    },
    "id": "artificial-interview",
    "outputId": "570fcfa5-7921-4982-fc44-aafe2a8d806a"
   },
   "outputs": [],
   "source": [
    "life_expectancy_data[\"country\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-console",
   "metadata": {
    "id": "written-console"
   },
   "source": [
    "Now that we know what our data looks like, it is a good idea to check how many missing values do we have on each variable.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-found",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1678894328974,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -60
    },
    "id": "paperback-found",
    "outputId": "0a4e9d4d-de37-49f7-d1c7-7bec2da3eba9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "life_expectancy_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-auckland",
   "metadata": {
    "id": "norman-auckland"
   },
   "source": [
    "We can use all this information to design an effective pre-processing methodology and to choose an appropiate model adapted to our data. To show how important this is we will do two different pre-processings:\n",
    "* **Generic Preprocessing**: Just transforms the data to something a model can process without throwing errors. Ignoring completely the dataset peculiarities. \n",
    "* **Specific Preprocessing**: Preprocessing adapted to the models we will be using and the data particularities.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-giving",
   "metadata": {
    "id": "graphic-giving"
   },
   "source": [
    "### Resampling protocol\n",
    "\n",
    "We will use two data partitions (train and test) with cross-validation over the train partition for deciding hyperparameters.  We will explain in detail cross-validation on section 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-cyprus",
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1678894328974,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -60
    },
    "id": "acute-cyprus"
   },
   "outputs": [],
   "source": [
    "X = life_expectancy_data.loc[:, life_expectancy_data.columns != \"life_expectancy\"]\n",
    "y = life_expectancy_data[\"life_expectancy\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-baseline",
   "metadata": {
    "id": "inner-baseline"
   },
   "source": [
    "## SECTION 2: Linear Regression (with minimum preprocessing of data)\n",
    "\n",
    "This is the Linear Regression lab sesion. For this reason we will be using the Linear Regression, Lasso Regression and Rigdge Regression models. These models are not compatible with missing values nor categorical variables. So, the absolute minimum we need to do to our data is to remove samples with missings and categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-century",
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1678894328975,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -60
    },
    "id": "external-century"
   },
   "outputs": [],
   "source": [
    "def minimum_preprocessing(X, y):\n",
    "    print(\"Original shape:{}\".format(X.shape))\n",
    "    categorical_columns = X.dtypes[X.dtypes == \"category\"].index.values\n",
    "    # We kill categorical columns\n",
    "    X = X.drop(columns=categorical_columns)\n",
    "    print(\"Droped: {}\".format(categorical_columns))\n",
    "    # We remove missing values\n",
    "    X = X.dropna()\n",
    "    y = y[X.index]\n",
    "    print(\"New shape:{}\".format(X.shape))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-colon",
   "metadata": {
    "id": "recognized-colon"
   },
   "source": [
    "We will aply separately our preprocessing to our train and test partitions to avoid generating biases on our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-rolling",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1678894328975,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -60
    },
    "id": "inclusive-rolling",
    "outputId": "d9560394-e023-4c6c-d0d9-db882539bdbf"
   },
   "outputs": [],
   "source": [
    "X_train, y_train = minimum_preprocessing(X_train, y_train)\n",
    "X_test, y_test = minimum_preprocessing(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-adams",
   "metadata": {
    "id": "enormous-adams"
   },
   "source": [
    "### Linear Regression\n",
    "\n",
    "Small recap of how linear regression works: \n",
    "\n",
    "We model our regression function as \n",
    "\n",
    " $y = f(x) + \\epsilon = w^\\top x  + \\epsilon$\n",
    " \n",
    " where: \n",
    " * $y$ is our target.\n",
    " * $w$ are the weights we will calculate.\n",
    " * $x$ are our samples.\n",
    " * $\\epsilon$ is the independent term. \n",
    "\n",
    "If we assume the error of all variables is gaussian solving this problem is equivalent to minimize the mean squared error of this function. \n",
    "\n",
    "$\\min_w || y - Xw ||^2$\n",
    "\n",
    "We will see two different implementations of linear regression. The one from statsmodels and the one from sklearn. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "global-devon",
   "metadata": {
    "id": "global-devon"
   },
   "source": [
    "#### Sklearn linear regression\n",
    "\n",
    "Sklearn models are really easy to use. They are all implemented on classes with the same structure, so knowing one you know all. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-clearance",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 333,
     "status": "ok",
     "timestamp": 1678894329294,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -60
    },
    "id": "brown-clearance",
    "outputId": "4f0d2729-fdaa-455a-cb58-c272b77afec1"
   },
   "outputs": [],
   "source": [
    "# We instantiate a linear regression.\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Now we train it on train data with fit method\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# We can predict using the predict method\n",
    "y_pred = lr.predict(X_train)\n",
    "\n",
    "weights = lr.coef_\n",
    "intercept = lr.intercept_\n",
    "# You can access to some info about the model, like the weights.\n",
    "print(\"Coefficients: \\n\", weights[:10])\n",
    "print(\"Intercept: \\n\", intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-handbook",
   "metadata": {
    "id": "proud-handbook"
   },
   "source": [
    "\n",
    "#### Statsmodels linear regression\n",
    "*Statsmodels linear regression* is a bit more difficult to use but it generates a lot of statistical data that can be useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-stockholm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 871
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1678894329294,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -60
    },
    "id": "rubber-stockholm",
    "outputId": "c1f9fcc2-2315-436b-dc32-e6d125349961"
   },
   "outputs": [],
   "source": [
    "# data_train = X_train.copy()\n",
    "# data_train['life_expectancy'] = y_train\n",
    "# data_train = sm.add_constant(data_train)\n",
    "# Linear regression is called ordinary least squares (OLS) in statsmodels\n",
    "model = sm.OLS(y_train, sm.add_constant(X_train))\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-professor",
   "metadata": {
    "id": "seasonal-professor"
   },
   "source": [
    "Interpreting this output:\n",
    "\n",
    "Each individual sample will be an array with the next structure:\n",
    "\n",
    "$x=(1, adult\\_mortality, infant\\_deaths,\\dots, scholing)^T$\n",
    "\n",
    "The weights of the model after training are: \n",
    "\n",
    "$w=(53.8131\t, -0.017, 0.1059, \\dots,0.9182)^T$\n",
    "\n",
    "And the model: \n",
    "\n",
    "$y(x; w) = w^T x = 53.8131 - 0.017*adult\\_mortality + 0.1059*infant\\_deaths + \\dots + 0.9182*scholing$\n",
    "\n",
    "Apart from the weights, statsmodels returns the 95% confidence interval of these weights ([0.025\t0.975]), the standard error of the weights (std err) and the p value (P>|z|). If p value is smaller to a threshold (usually 0.05) we can say that the variable is relevant for predicting the target.\n",
    "\n",
    "The residuals (the difference between the actual target value and the predicted target value) are:\n",
    "\n",
    " $(t_n - y(x_n; w)), n = 1,\\dots N$\n",
    "\n",
    "If we plot the residuals of the training data we obtain the next distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-julian",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "executionInfo": {
     "elapsed": 362,
     "status": "ok",
     "timestamp": 1678894329653,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -60
    },
    "id": "velvet-julian",
    "outputId": "1e28ab3b-55af-4400-d2c6-b2a7cf06250b"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.set_xlim([-5, 5])\n",
    "sns.distplot(result.resid, bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-working",
   "metadata": {
    "id": "ranging-working"
   },
   "source": [
    "We want this plot to look Gaussian, as it is our departing assumption (Gaussian error). For this reason this plot is a direct indicator of model validity.\n",
    "\n",
    "Another plot we can use to validate our model is a QQ-plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-judge",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "executionInfo": {
     "elapsed": 344,
     "status": "ok",
     "timestamp": 1678894329994,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -60
    },
    "id": "shaped-judge",
    "outputId": "6433e830-0e56-47bf-f193-55fc80092ac0"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "stats.probplot(result.resid, plot=plt);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-dietary",
   "metadata": {
    "id": "systematic-dietary"
   },
   "source": [
    "## SECTION 3: Metrics\n",
    "\n",
    "There are alternative metrics we can use to measure the performance of a regression model. \n",
    "We will review the most common ones over the training data predictions. \n",
    "\n",
    "**Mean Squared Error (MSE)** \n",
    "\n",
    "The best possible result would be a MSE of 0 sa it would mean a perfect prediction. \n",
    "\n",
    "$MSE(t,y) = \\frac{1}{N} \\sum_{i=1}^N (t - y(x;w))^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-wrestling",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1678894329994,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -60
    },
    "id": "limiting-wrestling",
    "outputId": "5554be06-4bcc-484f-a6d5-381d4a304e8c"
   },
   "outputs": [],
   "source": [
    "N = X_train.shape[0]\n",
    "\n",
    "# using statsmodel\n",
    "prediction = result.predict(sm.add_constant(X_train))\n",
    "mean_square_error = np.sum((y_train - prediction) ** 2) / N\n",
    "\n",
    "# You can also use sklearn implementation\n",
    "mean_square_error_sk = mean_squared_error(y_train, prediction)\n",
    "\n",
    "mean_square_error, mean_square_error_sk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-packet",
   "metadata": {
    "id": "integrated-packet"
   },
   "source": [
    "This number depends on the magnitude of the targets, so we cannot know if it is good or bad as it is. \n",
    "It is a very good practise to normalise it by dividing by the unbiased sample variance of the target. This way we obtain the \n",
    "\n",
    "**Normalized Mean Squared Error**\n",
    "\n",
    "Again, the best possible result would be 0, for the same reason.\n",
    "\n",
    "$norm\\_MSE(t,y) = \\frac {MSE(t,y)}{\\sigma^2(t)} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-clinic",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1678894329995,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -60
    },
    "id": "familiar-clinic",
    "outputId": "722240de-da69-4e2a-e249-727a1a2bd7e1"
   },
   "outputs": [],
   "source": [
    "norm_mse = np.sum((y_train - prediction) ** 2) / ((N) * np.var(y_train))\n",
    "\n",
    "# You can also use sklearn r2 implementation to calculate this value\n",
    "norm_mse_sk = 1 - r2_score(y_train, prediction)\n",
    "\n",
    "norm_mse, norm_mse_sk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-convergence",
   "metadata": {
    "id": "weekly-convergence"
   },
   "source": [
    " If we divide the mean square error by the variance of the targets t,\n",
    " we get the proportion of the variability of the target that is NOT explained by the model\n",
    "\n",
    " A model with 'norm.mse' equal to 1 is as good as the best constant model\n",
    " (namely, the model that always outputs the average of the target)\n",
    "\n",
    " models with 'norm.mse' above 0.5 are so so, beyond 0.7 they begin to be quite bad\n",
    "\n",
    " models with 'norm.mse' below 0.2 are quite good\n",
    "\n",
    "**Multiple R-squared (R^2)**\n",
    "\n",
    "The Multiple R-squared (R^2) (usually used by statisticians) is obtained by subtracting this quantity from one; that is, the proportion of the target variability that is explained by the model; in this case it reaches ~80%\n",
    "\n",
    "This metric is usually shown between 0 and 1 or in percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-facing",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1678894329995,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -60
    },
    "id": "stopped-facing",
    "outputId": "71fd7a70-f4b5-40e0-865b-9ae0f3ff2092"
   },
   "outputs": [],
   "source": [
    "R_squared = 1 - norm_mse\n",
    "\n",
    "# You can also use sklearn implementation\n",
    "R_squared_sk = r2_score(y_train, prediction)\n",
    "\n",
    "R_squared, R_squared_sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-ethiopia",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 222,
     "status": "ok",
     "timestamp": 1678894330212,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -60
    },
    "id": "representative-ethiopia",
    "outputId": "64427eb1-7831-4754-dcc6-d75b82add4af"
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-grade",
   "metadata": {
    "id": "loose-grade"
   },
   "source": [
    "We have seen here that the model has good results with the training data, but does not mean our model is a good predictor. \n",
    "To have a numerical asessment of its predictive ability we have several options: \n",
    "* Get new data (not possible here!)\n",
    "* Use LOOCV. \n",
    "* Predict on an specific validation partition and calculate the relevant measures.\n",
    "* Use Cross-Validation. \n",
    "\n",
    "We haven't saved a specific partition for obtaining validation metrics and we have too many samples (1123 in train) for doing LOOCV fast. For this reason we will do Cross-Validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-convertible",
   "metadata": {
    "id": "danish-convertible"
   },
   "source": [
    "## SECTION 4: Cross Validation\n",
    "\n",
    "We need to know how good is our model. We cannot use the train dataset for this because it might give artificially good results. We cannot use the test partition because we will need to compare theses results with the ones obtained on other models. We want to use the test set only at the end, so it can simulate runing the model on new data. This way we can obtain a generalization error as close as possible to the error we would get using the model on completely new data. \n",
    "\n",
    "As we cannot use neither the train or the test sets for evaluating the model we will use **Cross-Validation** for computing our metrics, we will use our cross-validation metrics to compare models and take any design decissions.\n",
    "\n",
    "\n",
    "\n",
    "![](cross_validation.PNG) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-instrumentation",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 584
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1678894330214,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -60
    },
    "id": "numeric-instrumentation",
    "outputId": "f084d113-5d51-4932-b17f-a6313fe52d73"
   },
   "outputs": [],
   "source": [
    "cross_val_metrics = pd.DataFrame(columns=[\"MSE\", \"norm_MSE\", \"R2\"])\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "i = 1\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    print(\n",
    "        \"Split {}: \\n\\tTest Folds: [{}] \\n\\tTrain Folds {}\".format(\n",
    "            i, i, [j for j in range(1, 6) if j != i]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    x_train_fold = X_train.values[train_index]\n",
    "    y_train_fold = y_train.values[train_index]\n",
    "    x_test_fold = X_train.values[test_index, :]\n",
    "    y_test_fold = y_train.values[test_index]\n",
    "\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(x_train_fold, y_train_fold)\n",
    "    y_pred_fold = lr.predict(x_test_fold)\n",
    "    fold_mse = mean_squared_error(y_test_fold, y_pred_fold)\n",
    "    fold_nmse = 1 - r2_score(y_test_fold, y_pred_fold)\n",
    "    fold_r2 = r2_score(y_test_fold, y_pred_fold)\n",
    "    print(\"\\tMSE: {} NMSE: {} R2: {}\".format(fold_mse, fold_nmse, fold_r2))\n",
    "\n",
    "    cross_val_metrics.loc[\"Fold {}\".format(i), :] = [fold_mse, fold_nmse, fold_r2]\n",
    "    i += 1\n",
    "\n",
    "\n",
    "cross_val_metrics.loc[\"Mean\", :] = cross_val_metrics.mean()\n",
    "cross_val_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-correction",
   "metadata": {
    "id": "pursuant-correction"
   },
   "source": [
    "We will use the mean of the folds as our metrics. We can also do this using sklearn cros_val_score  method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-separate",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1678894330215,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -60
    },
    "id": "attached-separate",
    "outputId": "e80388d4-025a-4cf4-a02e-8848af6d7aee"
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "folds_r2 = cross_val_score(lr, X_train, y_train, cv=5, scoring=\"r2\")\n",
    "lr_r2 = np.mean(folds_r2)\n",
    "folds_r2, lr_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-cookbook",
   "metadata": {
    "id": "meaningful-cookbook"
   },
   "source": [
    "## SECTION 5: Regularized Linear Regression: Ridge and Lasso\n",
    "\n",
    "Now that we have a way to evaluate our model performance, let's see if it improves by adding regularization. \n",
    "\n",
    "### Ridge Regression\n",
    "\n",
    "This time we are going to minimize the following function:\n",
    "\n",
    "$\\min_w (|| y - Xw ||^2 + \\lambda * ||w||^2_2)$\n",
    "\n",
    "The last term penalizes weights being to large (in magnitude). \n",
    "The lambda hyperparameter will control how much we are penalizing them. \n",
    "\n",
    "Lambda is not computed over the training process like the weights -- it is a *hyper-parameter*. \n",
    "So now the question is: How can we find the best lambda for our dataset? \n",
    "\n",
    "We have said on the previous section that we can use cross-validation metrics for comparing different models predictive performance. We can do the same to compare the same model with different hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-calgary",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "executionInfo": {
     "elapsed": 691,
     "status": "ok",
     "timestamp": 1678894330900,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -60
    },
    "id": "known-calgary",
    "outputId": "e0bbb2a8-eab1-4254-dd2c-d173812c52cd"
   },
   "outputs": [],
   "source": [
    "ridge_cross_val_metrics = pd.DataFrame(columns=[\"mean MSE\", \"mean norm_MSE\", \"mean R2\"])\n",
    "lambdas = [1e-10, 1e-5, 1e-4, 1e-3, 1e-2, 0.1, 0.5, 1, 5, 10, 50, 100]\n",
    "# We calculate the cross-validation metrics for each lambda\n",
    "for lambda_val in lambdas:\n",
    "    kf = KFold(n_splits=5)\n",
    "    i = 1\n",
    "    cv_mse = []\n",
    "    cv_nmse = []\n",
    "    cv_r2 = []\n",
    "    # We compute the metrics for each fold and then perform the mean.\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        x_train_fold = X_train.values[train_index]\n",
    "        y_train_fold = y_train.values[train_index]\n",
    "        x_test_fold = X_train.values[test_index, :]\n",
    "        y_test_fold = y_train.values[test_index]\n",
    "\n",
    "        lr = Ridge(alpha=lambda_val)\n",
    "        lr.fit(x_train_fold, y_train_fold)\n",
    "        y_pred_fold = lr.predict(x_test_fold)\n",
    "        fold_mse = mean_squared_error(y_test_fold, y_pred_fold)\n",
    "        fold_nmse = 1 - r2_score(y_test_fold, y_pred_fold)\n",
    "        fold_r2 = r2_score(y_test_fold, y_pred_fold)\n",
    "        cv_mse.append(fold_mse)\n",
    "        cv_nmse.append(fold_nmse)\n",
    "        cv_r2.append(fold_r2)\n",
    "    ridge_cross_val_metrics.loc[\"Lambda={}\".format(lambda_val), :] = [\n",
    "        np.mean(cv_mse),\n",
    "        np.mean(cv_nmse),\n",
    "        np.mean(cv_r2),\n",
    "    ]\n",
    "\n",
    "ridge_cross_val_metrics.sort_values(by=\"mean R2\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-suicide",
   "metadata": {
    "id": "advance-suicide"
   },
   "source": [
    "Another way to do this is ussing sklearn implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-brown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 698,
     "status": "ok",
     "timestamp": 1678894331596,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -60
    },
    "id": "welcome-brown",
    "outputId": "8332eee3-534f-45f0-9c0f-44aee3386a61"
   },
   "outputs": [],
   "source": [
    "ridge_cv = RidgeCV(alphas=lambdas, cv=5)\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best lambda:\", ridge_cv.alpha_, \"R2 score:\", ridge_cv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-influence",
   "metadata": {
    "id": "prime-influence"
   },
   "source": [
    "Ridge Regression does not seem to improve the results of linear regression. Let's check if Lasso turns out  to work better. \n",
    "\n",
    "### Lasso Regression\n",
    "\n",
    "This time we penalize weights using their L1 norm. \n",
    "\n",
    "\n",
    "$\\min_w (|| y - Xw ||^2 + \\lambda * |w|)$\n",
    "\n",
    "We will use scikit-learn's CV method to compute the best lambda directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-archives",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1147,
     "status": "ok",
     "timestamp": 1678894332742,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -60
    },
    "id": "dedicated-archives",
    "outputId": "74061aa5-a3bc-483e-b17f-0ac96bf6c912"
   },
   "outputs": [],
   "source": [
    "lasso_cv = LassoCV(alphas=lambdas, cv=5)\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "\n",
    "lasso_r2 = np.mean(cross_val_score(lasso_cv, X_train, y_train))\n",
    "\n",
    "print(\"Best lambda:\", lasso_cv.alpha_, \"R2 score:\", lasso_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-perspective",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "executionInfo": {
     "elapsed": 414,
     "status": "ok",
     "timestamp": 1678894333155,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -60
    },
    "id": "seven-perspective",
    "outputId": "6b64bd6d-2edd-4cb4-f1ff-9165c0340d9d"
   },
   "outputs": [],
   "source": [
    "r2_results = pd.DataFrame(\n",
    "    {\"lr\": lr_r2, \"ridge_cv\": ridge_cv.best_score_, \"lasso_cv\": lasso_r2},\n",
    "    index=[\"CV R2\"],\n",
    ")\n",
    "\n",
    "r2_results.loc[\"Train R2\", :] = [\n",
    "    r2_score(y_train, lr.predict(X_train)),\n",
    "    r2_score(y_train, ridge_cv.predict(X_train)),\n",
    "    r2_score(y_train, lasso_cv.predict(X_train)),\n",
    "]\n",
    "r2_results.loc[\"lambda\", \"lr\"] = 0\n",
    "r2_results.loc[\"lambda\", \"ridge_cv\"] = ridge_cv.alpha_\n",
    "r2_results.loc[\"lambda\", \"lasso_cv\"] = lasso_cv.alpha_\n",
    "r2_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivated-details",
   "metadata": {
    "id": "motivated-details"
   },
   "source": [
    "## SECTION 6: Understanding our models is important"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-evening",
   "metadata": {
    "id": "hollow-evening"
   },
   "source": [
    "We can't see a great improvement with Lasso either. \n",
    "\n",
    "So, why is regularization not working when it is supposed to give better models? \n",
    "\n",
    "Let's check the weights of our three models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-temple",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 259
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1678894333155,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -60
    },
    "id": "owned-temple",
    "outputId": "7e8e79e2-7d34-4ebe-f01a-942ad79d6245",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weights = pd.DataFrame(\n",
    "    {\"lr\": lr.coef_, \"ridge_cv\": ridge_cv.coef_, \"lasso_cv\": lasso_cv.coef_},\n",
    "    index=X_train.columns,\n",
    ")\n",
    "weights.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rocky-percentage",
   "metadata": {
    "id": "rocky-percentage"
   },
   "source": [
    "Visually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frank-scottish",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "executionInfo": {
     "elapsed": 685,
     "status": "ok",
     "timestamp": 1678894333835,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -60
    },
    "id": "frank-scottish",
    "outputId": "ab35fe06-e721-4dee-f1d1-40ae8a73a171"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 1))\n",
    "sns.heatmap(\n",
    "    weights.T.loc[[\"lr\"], :].abs(),\n",
    "    annot=True,\n",
    "    linewidths=0.5,\n",
    "    ax=ax,\n",
    "    cbar=False,\n",
    "    xticklabels=False,\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(20, 1))\n",
    "sns.heatmap(\n",
    "    weights.T.loc[[\"ridge_cv\"], :].abs(),\n",
    "    annot=True,\n",
    "    linewidths=0.5,\n",
    "    cbar=False,\n",
    "    xticklabels=False,\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(20, 1))\n",
    "sns.heatmap(\n",
    "    weights.T.loc[[\"lasso_cv\"], :].abs(),\n",
    "    annot=True,\n",
    "    linewidths=0.5,\n",
    "    cbar=False,\n",
    "    xticklabels=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-korean",
   "metadata": {
    "id": "designing-korean"
   },
   "source": [
    "We can see some strange patterns in our weights, in the three models we can see a wide range of values. Both lasso and ridge have almost the same weights. \n",
    "\n",
    "If we compare the weights with the mean values of our variables we can see that the linear regression is trying to balance the variable ranges. That's probably why our regularized models are not working so well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-status",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "executionInfo": {
     "elapsed": 430,
     "status": "ok",
     "timestamp": 1678894334262,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -60
    },
    "id": "comic-status",
    "outputId": "2d9498f6-6b2a-4092-e37d-ce07ccd58a63",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 1))\n",
    "sns.heatmap(\n",
    "    X_train.mean().to_frame().T.rename(index={0: \"Mean\"}),\n",
    "    annot=True,\n",
    "    linewidths=0.5,\n",
    "    cbar=False,\n",
    "    xticklabels=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-quest",
   "metadata": {
    "id": "pursuant-quest"
   },
   "source": [
    "We are going to fix this by scaling our data. This way, all variables will have the same range and we will take better advantage of our models.\n",
    "\n",
    "**Note that the pre-processing is slightly different for train data than for test data. Eseentially, this is to avoid any type of \"leakage\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-laser",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1678894334263,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -60
    },
    "id": "unusual-laser",
    "outputId": "a0a66b32-7d90-4641-91b0-11f572913027"
   },
   "outputs": [],
   "source": [
    "def scaling_preprocessing(X, y, scaler=None):\n",
    "    print(\"Original shape:{}\".format(X.shape))\n",
    "    categorical_columns = X.dtypes[X.dtypes == \"category\"].index.values\n",
    "\n",
    "    # We scale the numerical columns\n",
    "    numerical_columns = [c for c in X.columns if c not in categorical_columns]\n",
    "    if scaler is None:\n",
    "        # We only want the scaler to fit the train data\n",
    "        scaler = MinMaxScaler()\n",
    "        X[numerical_columns] = scaler.fit_transform(X[numerical_columns])\n",
    "    else:\n",
    "        X[numerical_columns] = scaler.transform(X[numerical_columns])\n",
    "\n",
    "    # We kill categorical columns\n",
    "    X = X.drop(columns=categorical_columns)\n",
    "    print(\"Droped: {}\".format(categorical_columns))\n",
    "    # We remove missing values\n",
    "    X = X.dropna()\n",
    "    y = y[X.index]\n",
    "    print(\"New shape:{}\".format(X.shape))\n",
    "    return X, y, scaler\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42\n",
    ")\n",
    "X_train, y_train, scaler = scaling_preprocessing(X_train, y_train)\n",
    "X_test, y_test, _ = scaling_preprocessing(X_test, y_test, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-produce",
   "metadata": {
    "id": "joint-produce"
   },
   "source": [
    "Now our variables have more reasonable ranges. Lets see how this affects our model weights and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-cookbook",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "executionInfo": {
     "elapsed": 338,
     "status": "ok",
     "timestamp": 1678894334596,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -60
    },
    "id": "comfortable-cookbook",
    "outputId": "13b1ee94-ae5e-417b-f3b2-ef4bf809aca1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 1))\n",
    "sns.heatmap(\n",
    "    X_train.mean().to_frame().T.rename(index={0: \"Mean\"}),\n",
    "    annot=True,\n",
    "    linewidths=0.5,\n",
    "    cbar=False,\n",
    "    xticklabels=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-grain",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 242
    },
    "executionInfo": {
     "elapsed": 4756,
     "status": "ok",
     "timestamp": 1678894339348,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -60
    },
    "id": "steady-grain",
    "outputId": "e77aa697-3c03-47b5-8719-d7708dd98d44"
   },
   "outputs": [],
   "source": [
    "lr_scaled = LinearRegression()\n",
    "lr_scaled.fit(X_train, y_train)\n",
    "r2_lr_scaled = np.mean(cross_val_score(lr_scaled, X_train, y_train, cv=5, scoring=\"r2\"))\n",
    "\n",
    "ridge_cv_scaled = RidgeCV(alphas=lambdas, cv=5)\n",
    "ridge_cv_scaled.fit(X_train, y_train)\n",
    "r2_ridge_scaled = np.mean(\n",
    "    cross_val_score(ridge_cv_scaled, X_train, y_train, cv=5, scoring=\"r2\")\n",
    ")\n",
    "\n",
    "\n",
    "lasso_cv_scaled = LassoCV(alphas=lambdas, cv=5)\n",
    "lasso_cv_scaled.fit(X_train, y_train)\n",
    "r2_lasso_scaled = np.mean(\n",
    "    cross_val_score(ridge_cv_scaled, X_train, y_train, cv=5, scoring=\"r2\")\n",
    ")\n",
    "\n",
    "weights = pd.DataFrame(\n",
    "    {\n",
    "        \"lr scaled\": lr_scaled.coef_,\n",
    "        \"ridge_cv scaled\": ridge_cv_scaled.coef_,\n",
    "        \"lasso_cv scaled\": lasso_cv_scaled.coef_,\n",
    "    },\n",
    "    index=X_train.columns,\n",
    ")\n",
    "weights.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sealed-cathedral",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1678894339348,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -60
    },
    "id": "sealed-cathedral",
    "outputId": "a8c6039a-8eed-493f-fcbb-c1ca1af3b942"
   },
   "outputs": [],
   "source": [
    "r2_results = pd.DataFrame(\n",
    "    {\n",
    "        \"lr\": r2_lr_scaled,\n",
    "        \"ridge_cv\": r2_ridge_scaled,\n",
    "        \"lasso_cv\": r2_lasso_scaled,\n",
    "    },\n",
    "    index=[\"CV R2\"],\n",
    ")\n",
    "\n",
    "r2_results.loc[\"Train R2\", :] = [\n",
    "    r2_score(y_train, lr_scaled.predict(X_train)),\n",
    "    r2_score(y_train, ridge_cv_scaled.predict(X_train)),\n",
    "    r2_score(y_train, lasso_cv_scaled.predict(X_train)),\n",
    "]\n",
    "r2_results.loc[\"lambda\", \"lr\"] = 0\n",
    "r2_results.loc[\"lambda\", \"ridge_cv\"] = ridge_cv_scaled.alpha_\n",
    "r2_results.loc[\"lambda\", \"lasso_cv\"] = lasso_cv_scaled.alpha_\n",
    "r2_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-compact",
   "metadata": {
    "id": "conscious-compact"
   },
   "source": [
    "The results haven't improved much. Perhaps including categorical variables helps. \n",
    "\n",
    "We have two categorical variables. Country and status. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-museum",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1678894339349,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -60
    },
    "id": "acting-museum",
    "outputId": "64c33401-d3dc-4e1f-ed0a-dd3c3bad2220"
   },
   "outputs": [],
   "source": [
    "categorical_columns = X.dtypes[X.dtypes == \"O\"].index.values\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insured-arthritis",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 222,
     "status": "ok",
     "timestamp": 1678894339566,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -60
    },
    "id": "insured-arthritis",
    "outputId": "91d43b3b-a532-41ce-990f-fb77bbc7b7fe"
   },
   "outputs": [],
   "source": [
    "def categorical_preprocessing(X, y, scaler=None):\n",
    "    print(\"Original shape:{}\".format(X.shape))\n",
    "    categorical_columns = X.dtypes[X.dtypes == \"category\"].index.values\n",
    "    numerical_columns = [c for c in X.columns if c not in categorical_columns]\n",
    "\n",
    "    # Scale numerical variables\n",
    "    if scaler is None:\n",
    "        # We only want the scaler to fit the train data\n",
    "        scaler = MinMaxScaler()\n",
    "        X[numerical_columns] = scaler.fit_transform(X[numerical_columns])\n",
    "    else:\n",
    "        X[numerical_columns] = scaler.transform(X[numerical_columns])\n",
    "\n",
    "    # Apply one hot encoding to categorical variables\n",
    "    for column in categorical_columns:\n",
    "        X_one_hot = pd.get_dummies(X[column], prefix=column)\n",
    "        X = X.merge(X_one_hot, left_index=True, right_index=True)\n",
    "        X = X.drop(columns=[column])\n",
    "\n",
    "    # Drop missings\n",
    "    X = X.dropna()\n",
    "    y = y[X.index]\n",
    "    print(\"New shape:{}\".format(X.shape))\n",
    "    return X, y, scaler\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42\n",
    ")\n",
    "X_train, y_train, scaler = categorical_preprocessing(X_train, y_train)\n",
    "X_test, y_test, _ = categorical_preprocessing(X_test, y_test, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-setting",
   "metadata": {
    "id": "successful-setting"
   },
   "source": [
    "If we use this new pre-processing we obtain super good results on the training set but super bad results on cross-validation. \n",
    "\n",
    "This means our model wont be able to generalize if we try to use it on new data. \n",
    "\n",
    "How can it be that having more information our model is performing worse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-tract",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 304,
     "status": "ok",
     "timestamp": 1678894339869,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -60
    },
    "id": "junior-tract",
    "outputId": "2d8fa742-f6bc-412c-9894-0dbb2c5b82fa"
   },
   "outputs": [],
   "source": [
    "lr_one_hot = LinearRegression()\n",
    "lr_one_hot.fit(X_train, y_train)\n",
    "\n",
    "r2_lr_one_hot_train = lr_one_hot.score(X_train, y_train)\n",
    "r2_lr_one_hot_cv = np.mean(\n",
    "    cross_val_score(lr_one_hot, X_train, y_train, cv=5, scoring=\"r2\")\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Train R2 score: {}\\nCross-Validation R2 score: {}\".format(\n",
    "        r2_lr_one_hot_train, r2_lr_one_hot_cv\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-greensboro",
   "metadata": {
    "id": "framed-greensboro"
   },
   "source": [
    "We have two problems with our model: \n",
    "1. It is **overfitting** the training data. So, when it receives new data that has not been used on training it is not able to predict. \n",
    "2. This might be caused by the **curse of dimensionallity**. We have 214 variables for 1123 samples, which is not a great ratio. \n",
    "\n",
    "We can also see on the weights of our model that there is something wrong, as there are differences between the weight ranges. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-demand",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 144
    },
    "executionInfo": {
     "elapsed": 1304,
     "status": "ok",
     "timestamp": 1678894341171,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -60
    },
    "id": "thick-demand",
    "outputId": "d14c79a1-f91b-4dd1-b6cd-525631ec08d6"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 1))\n",
    "sns.heatmap(pd.DataFrame({\"lr overfitted\": lr_one_hot.coef_}).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-promise",
   "metadata": {
    "id": "induced-promise"
   },
   "source": [
    "Finally, if we check what our data looks like, we will see that we have a very sparse matrix. So, on top of all our problems we are trying to use SVD on a sparse matrix, which is not ideal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-peripheral",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 841
    },
    "executionInfo": {
     "elapsed": 5178,
     "status": "ok",
     "timestamp": 1678894346346,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -60
    },
    "id": "personalized-peripheral",
    "outputId": "d9ccb8c2-b4f1-461b-add8-f86ec616dfe3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "sns.heatmap(X_test.to_numpy(dtype=float));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-emperor",
   "metadata": {
    "id": "copyrighted-emperor"
   },
   "source": [
    "For fixing overfitting we can either remove this variable or apply regularization. Let's see how our regularized models behave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-mention",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 24834,
     "status": "ok",
     "timestamp": 1678894371169,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -60
    },
    "id": "dietary-mention",
    "outputId": "75b6a900-74c1-46bc-aed5-dac0c01478f5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ridge_cv_one_hot = RidgeCV(alphas=lambdas, cv=5)\n",
    "ridge_cv_one_hot.fit(X_train, y_train)\n",
    "\n",
    "r2_ridge_one_hot_train = ridge_cv_one_hot.score(X_train, y_train)\n",
    "r2_ridge_one_hot_cv = np.mean(\n",
    "    cross_val_score(ridge_cv_one_hot, X_train, y_train, cv=5, scoring=\"r2\")\n",
    ")\n",
    "\n",
    "lasso_cv_one_hot = LassoCV(alphas=lambdas, cv=5)\n",
    "lasso_cv_one_hot.fit(X_train, y_train)\n",
    "\n",
    "r2_lasso_one_hot_train = ridge_cv_one_hot.score(X_train, y_train)\n",
    "r2_lasso_one_hot_cv = np.mean(\n",
    "    cross_val_score(lasso_cv_one_hot, X_train, y_train, cv=5, scoring=\"r2\")\n",
    ")\n",
    "\n",
    "weights = pd.DataFrame(\n",
    "    {\n",
    "        \"lr_one_hot\": lr_one_hot.coef_,\n",
    "        \"ridge_cv_one_hot\": ridge_cv_one_hot.coef_,\n",
    "        \"lasso_cv_one_hot\": lasso_cv_one_hot.coef_,\n",
    "    },\n",
    "    index=X_train.columns,\n",
    ")\n",
    "\n",
    "for column in weights.columns:\n",
    "    fig = plt.figure(figsize=(20, 1))\n",
    "    ax = sns.heatmap(weights[[column]].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-police",
   "metadata": {
    "id": "sorted-police"
   },
   "source": [
    "We obtain smaller weights on our regularized models. Will this affect our models performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-marshall",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1678894371169,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -60
    },
    "id": "maritime-marshall",
    "outputId": "c3555e9b-ff82-41f3-c88c-29ad5eda9bb6"
   },
   "outputs": [],
   "source": [
    "r2_results.loc[:, \"lr_one_hot\"] = [r2_lr_one_hot_cv, r2_lr_one_hot_train, 0]\n",
    "r2_results.loc[:, \"ridge_cv_one_hot\"] = [\n",
    "    r2_ridge_one_hot_cv,\n",
    "    r2_ridge_one_hot_train,\n",
    "    ridge_cv_one_hot.alpha_,\n",
    "]\n",
    "r2_results.loc[:, \"lasso_cv_one_hot\"] = [\n",
    "    r2_lasso_one_hot_cv,\n",
    "    r2_lasso_one_hot_train,\n",
    "    lasso_cv_one_hot.alpha_,\n",
    "]\n",
    "r2_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-desperate",
   "metadata": {
    "id": "forced-desperate"
   },
   "source": [
    "We can see in our cv metrics that regularization improves performance **a lot**. This way the model can take advantage of the extra data we are feeding it and effectively control overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-mother",
   "metadata": {
    "id": "serial-mother"
   },
   "source": [
    "## SECTION 7: Model selection\n",
    "\n",
    "Now we use the cv R2 to choose the best model and give the test R2 as our resulting R2 measure. According to our validation metrics, the best model is ridge_cv with a lambda of 0.001 used with the standarized dataset with categorical variables. \n",
    "\n",
    "Now that we have taken all the decisions concerning our model and data pre-processing. We use the test set to see how the model generalizes. This step is supposed to simulate seen how the model performs with *completely new* data, namely, data it has never seen before.\n",
    "\n",
    "**Typically, before testing the \"winning method\" we would re-fit it on the whole training data; the RidgeCV method does this by default for us so we do not need to do this here.**\n",
    "\n",
    "It is important to use the test set only to give the final performance, otherwise we risk giving an over optimistic result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-afternoon",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1678894371170,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -60
    },
    "id": "changing-afternoon",
    "outputId": "aea55abb-c54c-4303-a27d-ff0331843686"
   },
   "outputs": [],
   "source": [
    "y_tes_predicted = ridge_cv_one_hot.predict(X_test)\n",
    "r2_ridge = ridge_cv_one_hot.score(X_test, y_test)\n",
    "\n",
    "print(\n",
    "    \"Mean sqared error with test data: {}\".format(\n",
    "        mean_squared_error(y_test, y_tes_predicted)\n",
    "    )\n",
    ")\n",
    "print(\"R2 score with test data: {}\".format(r2_ridge))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "281px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
