{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "g8SkT_9EiyqR",
   "metadata": {
    "id": "g8SkT_9EiyqR"
   },
   "source": [
    "# AA1 lab 05 - Discrminant analysis; warm-up with `iris` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rrRZ1eBQiyqk",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1679073432889,
     "user": {
      "displayName": "Marta Arias Vicente",
      "userId": "13874328413650848050"
     },
     "user_tz": -60
    },
    "id": "rrRZ1eBQiyqk"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option(\"display.precision\", 3)\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9686eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load iris data and put it in a dataframe\n",
    "\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "df[\"species\"] = pd.Series([iris.target_names[y] for y in iris.target], dtype=\"category\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e99835e",
   "metadata": {},
   "source": [
    "## plot data using PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6dca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(iris.data)\n",
    "coords = pca.transform(iris.data)\n",
    "sns.scatterplot(x=coords[:, 0], y=coords[:, 1], c=iris.target);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4626331",
   "metadata": {},
   "source": [
    "## drop some rows, to make it a little harder.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14055b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "df = df.drop(\n",
    "    df[df[\"species\"] == \"setosa\"].sample(frac=0.9).index\n",
    ")  # remove 90% from setosa class\n",
    "df = df.drop(\n",
    "    df[df[\"species\"] == \"versicolor\"].sample(frac=0.9).index\n",
    ")  # remove 70% from versicolor class\n",
    "df = df.drop(\n",
    "    df[df[\"species\"] == \"virginica\"].sample(frac=0.9).index\n",
    ")  # remove 30% from virginica class\n",
    "\n",
    "df.describe()\n",
    "\n",
    "print(Counter(df[\"species\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f159908",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=df, hue=\"species\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1f0657",
   "metadata": {},
   "source": [
    "### QDA/LDA/RDA on `iris`\n",
    "\n",
    "#### QDA\n",
    "As a reminder, QDA implements the Bayesian classifier assuming $P(x|y=class)$ for each class is normally distributed. \n",
    "Hence, for $K$ classes it has to estimate their means $\\mu_k$ and covariance matrices $\\Sigma_k$ for $1\\leq k\\leq K$.\n",
    "\n",
    "To predict, it uses the Bayesian rule $\\hat{y}  = \\argmax_k g_k(\\mathbf{x})$ where\n",
    "\n",
    "$$\\begin{aligned}\n",
    "g_k(\\mathbf{x}) &= \\log \\left[P(y=c_k) P(\\mathbf{x}| y = c_k)\\right] \\\\\n",
    "        &= \\log \\pi_k - \\log (|2\\pi\\Sigma_k|^{\\frac{1}{2}}) - \\frac{1}{2}(\\mathbf{x} - \\mu_k)^T \\Sigma_k^{-1}(\\mathbf{x} - \\mu_k) \\\\\n",
    "        &= \\log \\pi_k - \\frac{1}{2}\\left(\\log|\\Sigma_k|  + (\\mathbf{x} - \\mu_k)^T \\Sigma_k^{-1}(\\mathbf{x} - \\mu_k)\\right)  + const\n",
    "\\end{aligned}$$\n",
    "\n",
    "#### LDA\n",
    "LDA is a special case of QDA where we assume all covariance matrices constant accross classes.\n",
    "This simplifies the corresponding discriminant functions to:\n",
    "\n",
    "$$g_k(\\mathbf{x}) = \\log \\pi_k + \\mu_k^T \\Sigma^{-1}\\mathbf{x} - \\frac{1}{2}\\mu_k^T\\Sigma^{-1}\\mu_k$$\n",
    "\n",
    "We can re-write $g_k(\\mathbf{x}) = \\theta_k^T \\mathbf{x} + \\theta_{k,0}$ using\n",
    "- $\\theta_k = \\mu_k^T \\Sigma^{-1}$\n",
    "- $\\theta_{k,0} = \\log \\pi_k + \\frac{1}{2}\\mu_k^T\\Sigma^{-1}\\mu_k$\n",
    "\n",
    "These quantities correspond to the `coef_` and `_intercept` attributes of the object.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2dcdac",
   "metadata": {},
   "source": [
    "### QDA on `iris` data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07f2c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import (\n",
    "    QuadraticDiscriminantAnalysis,\n",
    "    LinearDiscriminantAnalysis,\n",
    ")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "qda = QuadraticDiscriminantAnalysis().fit(X, y)\n",
    "y_pred = qda.predict(X)\n",
    "cm_qda = confusion_matrix(y, y_pred)\n",
    "print(cm_qda)\n",
    "\n",
    "lda = LinearDiscriminantAnalysis().fit(X, y)\n",
    "y_pred = lda.predict(X)\n",
    "cm_lda = confusion_matrix(y, y_pred)\n",
    "print(cm_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad62ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# a little more appealing..\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "sns.heatmap(cm_qda / np.sum(cm_qda), annot=True, fmt=\".2%\", cmap=\"Blues\", ax=axs[0])\n",
    "axs[0].set_title(\"QDA\")\n",
    "sns.heatmap(cm_lda / np.sum(cm_lda), annot=True, fmt=\".2%\", cmap=\"Reds\", ax=axs[1])\n",
    "axs[1].set_title(\"LDA\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b817fb8",
   "metadata": {},
   "source": [
    "### QDA/LDA/RDA by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dad26ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate priors, mean and covariances for each class\n",
    "\n",
    "n_c = Counter(df[\"species\"])  # nr of examples in each class\n",
    "classes = list(n_c.keys())\n",
    "n = sum(n_c.values())\n",
    "d = len(iris.feature_names)\n",
    "\n",
    "pi = {c: n_c[c] / n for c in classes}\n",
    "\n",
    "mu = dict()  # to store means for each class\n",
    "cov = dict()  # to store cov. matrices for each class\n",
    "for c in classes:\n",
    "    # get all examples from class c\n",
    "    X_c = df.loc[df[\"species\"] == c, iris.feature_names].to_numpy()\n",
    "    mu[c] = np.mean(X_c, axis=0)\n",
    "    cov[c] = np.cov(X_c, bias=False, rowvar=False)  # sample covariance matrix\n",
    "\n",
    "for c in classes:\n",
    "    print()\n",
    "    print(f\"class {c}:\")\n",
    "    print(f\"\\tprior: {pi[c]:.2f}\")\n",
    "    print(f\"\\tsample mean: {mu[c]}\")\n",
    "    print(f\"\\tsample cov matrix: {cov[c]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8d0d31",
   "metadata": {},
   "source": [
    "### estimate sample cov matrix for LDA \n",
    "\n",
    "$$\\hat{\\Sigma} = \\frac{1}{n-K}\\sum_{k=1}^K (n_k -1)\\hat{\\Sigma}_k$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06a3e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_cov = np.zeros((d, d))\n",
    "for c in classes:\n",
    "    lda_cov += cov[c] * (n_c[c] - 1)\n",
    "lda_cov /= n - len(classes)\n",
    "print(lda_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1394f33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "# make prediction for QDA using $y_pred = argmax_k g_k(x)$\n",
    "\n",
    "x = (mu[\"versicolor\"] + mu[\"virginica\"]) / 2\n",
    "\n",
    "g = dict()\n",
    "for c in classes:\n",
    "    g[c] = np.log(pi[c]) + scipy.stats.multivariate_normal.logpdf(\n",
    "        x, mean=mu[c], cov=cov[c]\n",
    "    )\n",
    "print(f\"predictions for QDA: {Counter(g).most_common()}\")\n",
    "\n",
    "for c in classes:\n",
    "    g[c] = np.log(pi[c]) + scipy.stats.multivariate_normal.logpdf(\n",
    "        x, mean=mu[c], cov=lda_cov\n",
    "    )\n",
    "print(f\"predictions for LDA: {Counter(g).most_common()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1020e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show location of query point x\n",
    "\n",
    "xy = pca.transform(x.reshape(1, -1))\n",
    "ax = sns.scatterplot(x=coords[:, 0], y=coords[:, 1], c=iris.target, legend=\"auto\")\n",
    "sns.scatterplot(x=xy[:, 0], y=xy[:, 1], c=[\"black\"], s=100, ax=ax, marker=\"x\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65222562",
   "metadata": {},
   "source": [
    "__Exercise:__ Implement _Gaussian NB_  classifier and compute predictions on previous $x$"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "281px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
