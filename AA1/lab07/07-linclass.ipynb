{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "monetary-nothing",
   "metadata": {},
   "source": [
    "# AA1 lab 07\n",
    "\n",
    "# Linear classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-individual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to upgrade packages\n",
    "# !pip install pandas --upgrade --user --quiet\n",
    "# !pip install numpy --upgrade --user --quiet\n",
    "# !pip install scipy --upgrade --user --quiet\n",
    "# !pip install statsmodels --upgrade --user --quiet\n",
    "# !pip install scikit-learn --upgrade --user --quiet\n",
    "# !pip install tensorflow --user\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-tyler",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from time import time\n",
    "from datetime import timedelta\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    KFold,\n",
    "    cross_validate,\n",
    "    GridSearchCV,\n",
    ")\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.discriminant_analysis import (\n",
    "    LinearDiscriminantAnalysis,\n",
    "    QuadraticDiscriminantAnalysis,\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB, CategoricalNB\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "pd.set_option(\"display.precision\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggressive-button",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "np.random.seed(123)  # for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-powder",
   "metadata": {},
   "source": [
    "### Support functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-enemy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion(true, pred):\n",
    "    \"\"\"\n",
    "    Function for pretty printing confusion matrices\n",
    "    \"\"\"\n",
    "    true.name = \"target\"\n",
    "    pred.name = \"predicted\"\n",
    "    cm = pd.crosstab(true.reset_index(drop=True), pred.reset_index(drop=True))\n",
    "    cm = cm[cm.index]\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-dragon",
   "metadata": {},
   "source": [
    "## SECTION 1. Linear Classification on wine data\n",
    "\n",
    " We have the results of an analysis on wines grown in a region in Italy but derived from three different cultivars.\n",
    "The analysis determined the quantities of 13 chemical constituents found in each of the three types of wines. \n",
    "The goal is to separate the three types of wines. \n",
    "\n",
    "For increasing the interest on the exercise we will asume that one of the wine types is cheap, the second is normal, and the third is expensive. Our objective here will be to make a classifier that labels the wine its category price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-resort",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "data = load_wine(as_frame=True)\n",
    "\n",
    "df = data.frame\n",
    "df[\"target\"] = df[\"target\"].map({0: \"cheap\", 1: \"normal\", 2: \"expensive\"})\n",
    "df.target = df.target.astype(\n",
    "    CategoricalDtype(categories=[\"cheap\", \"normal\", \"expensive\"], ordered=True)\n",
    ")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-concentration",
   "metadata": {},
   "source": [
    "If we do a fast pairplot we can see that this dataset is going to be an easy one. The scatterplots are showing clear separation on some variables, and it looks like we would be able to predict the class with only a subset on the variables, like for example Alcohol and OD280/OD315."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-belize",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=df, hue=\"target\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-section",
   "metadata": {},
   "source": [
    "### Resampling protocol\n",
    "\n",
    "We will use two data partitions (train and test) with 5-fold cross-validation over the train partition for deciding hyperparameters. \n",
    "For comparing models we will use 5-fold cross-validation metrics.\n",
    "\n",
    "We have chosen this strategy because we have very few samples (178) and we are going to use quite fast models on this lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-mathematics",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[:, df.columns != \"target\"]\n",
    "y = df[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-surfing",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Our variables are showing different ranges, we are going to scale them because we don't want this to interfere with our models' performance. *Notice that the scaling process is different for the training and the test set; this is to make things compatible with the fact that we cannot use information from the test set to do the scaling*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "balanced-opposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(X, y, scaler=None):\n",
    "    # We scale all the columns\n",
    "    if scaler is None:\n",
    "        # We only want the scaler to fit the train data\n",
    "        scaler = MinMaxScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "    else:\n",
    "        X = scaler.transform(X)\n",
    "    return X, y, scaler\n",
    "\n",
    "\n",
    "X_train, y_train, scaler = preprocessing(X_train, y_train)\n",
    "X_test, y_test, _ = preprocessing(X_test, y_test, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-fairy",
   "metadata": {},
   "source": [
    "### LDA\n",
    "\n",
    "LDA tries to model the probability $p(y=C_k|X=x)$ by assuming: \n",
    "* $p(x|C_k)$ is Gaussian (which means that can be described by $\\mu_k$ and $\\Sigma_k$)\n",
    "* All covariance matrix are the same ($\\Sigma_k = \\Sigma$)\n",
    "\n",
    "By using bayes formula ($p(A|B) = \\frac{P(B|A)P(A)}{P(B)}$) and all these asumptions, we obtain the next discriminant function:\n",
    "\n",
    "$a_k(x) = x^T\\Sigma^{-1}\\mu_k - \\frac{1}{2}\\mu_k^T\\Sigma^{-1}\\mu_k + log(\\pi_k)$\n",
    "\n",
    "Where $\\pi_k$ are the prior probabilities. \n",
    "\n",
    "If we call:\n",
    "\n",
    "$w = \\Sigma^{-1}\\mu_k$\n",
    "\n",
    "$w_0=- \\frac{1}{2}\\mu_k^T\\Sigma^{-1}\\mu_k + log(\\pi_k)$\n",
    "\n",
    "We obtain a linear representation of the formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-southeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LinearDiscriminantAnalysis()\n",
    "lda_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Priors:\", lda_model.priors_)\n",
    "print(\"Means:\\n\")\n",
    "means = pd.DataFrame(lda_model.means_)\n",
    "means.columns = wine.columns[1:]\n",
    "means.index = lda_model.classes_\n",
    "means\n",
    "print(\"Coefs:\")\n",
    "coefs = pd.DataFrame(lda_model.coef_)\n",
    "coefs.columns = wine.columns[1:]\n",
    "coefs.index = lda_model.classes_\n",
    "coefs.T\n",
    "\n",
    "print(\"Intercepts:\")\n",
    "intercepts = pd.DataFrame(lda_model.intercept_)\n",
    "intercepts.index = lda_model.classes_\n",
    "intercepts\n",
    "\n",
    "print(\"Explained Variance Ratio\")\n",
    "pd.DataFrame(lda_model.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-slovenia",
   "metadata": {},
   "source": [
    "We can see now an example of how the model is predicting the class of a single sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-diving",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = X_train[0, :]\n",
    "sample_value = y_train[0]\n",
    "\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-palace",
   "metadata": {},
   "source": [
    "The model coeficients correspond to the $w$ in our linear formula and the intercept corresponds to the $w_0$. \n",
    "\n",
    "We can use them to obtain the decision function values of our sample. We will predict the class with the biggest value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pediatric-conclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.matmul(lda_model.coef_, sample) + lda_model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-question",
   "metadata": {},
   "source": [
    "We can also compute this using the decision function implemented on the lda class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-reservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.decision_function(sample.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-billion",
   "metadata": {},
   "source": [
    "In this case the prediction would be cheap, as it is the category codified first on the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-picking",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-unknown",
   "metadata": {},
   "source": [
    "We can also use predict method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empirical-equipment",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Prediction: {}\\nReal value: {}\".format(\n",
    "        lda_model.predict(sample.reshape(1, -1))[0], sample_value\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-soviet",
   "metadata": {},
   "source": [
    "A big advantage for the LDA model is that it can also perform dimensionality reduction. We can now check the projection of the data into the first two components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-transcription",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-student",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed = lda_model.transform(X_train)\n",
    "\n",
    "X_transformed = pd.DataFrame(X_transformed)\n",
    "X_transformed[\"labels\"] = y_train.reset_index(drop=True)\n",
    "X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-maker",
   "metadata": {},
   "source": [
    "It looks like it is separating well the samples. Now let's evaluate our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naval-living",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=0, y=1, data=X_transformed, hue=\"labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "democratic-strategy",
   "metadata": {},
   "source": [
    "We can see on the confusion matrix that on the train data we are obtaining a perfect classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "postal-direction",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion(y_train, pd.Series(lda_model.predict(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-youth",
   "metadata": {},
   "source": [
    "### Choosing our metrics\n",
    "\n",
    "Now we are going to compute our cross validation metrics. \n",
    "\n",
    "\n",
    "\n",
    "Last time we focussed on Accuracy and Recall. This time we are adding other metrics that are comonly used on classification problems. \n",
    "\n",
    "\n",
    "**Accuracy:**\n",
    "\n",
    "$$accuracy = \\frac{\\sum_c tp_c}{n}$$\n",
    "\n",
    "Where tp_c are the true positive predictions for all the classes and n are the total number of samples. This metric is **sensitive to imbalanced data**.\n",
    "\n",
    "**Precision (of a class):**\n",
    "\n",
    "$$precision_c = \\frac{tp}{tp + fp}$$\n",
    "\n",
    "Where $tp$ are the true positives (samples correctly predicted of this class) and fp are the false positives (samples from another class predicted incorrectly as this class). This metric measures how much the model is predicting correctly a class with respect all the predictions of this class. We will use this metric when having false positive predictions is very harmful in our model context. \n",
    "\n",
    "\n",
    "**Recall (of a class):**\n",
    "\n",
    "$$recall_c = =\\frac{tp}{tp + fn}$$\n",
    "\n",
    "Where $tp$ are the true positives (samples correctly predicted of this class) and fn are the false negatives (samples from this class predicted incorrectly as a different class). This metric measures how much the model is predicting correctly a class with respect all the real values of this class. We will use this metric when having false negative predictions is very harmful in our model context.\n",
    "\n",
    "**F1-score (of a class):**\n",
    "\n",
    "$$\\frac{2 * precission_c * recall_c }{precission_c + recall_c}$$\n",
    "\n",
    "The **harmonic mean** of precision and recall. We will use this metric when we want a good balance between precision and recall.  \n",
    "\n",
    "We are going to use the: Accuracy, F1 macro, precission macro and recall macro metrics. \n",
    "\n",
    "These metrics will give us a precise view of how our model is performing. In this particular problem all the categories are equally important, as we don't want either angry customers or angry sellers. For this reason we will use the macro average of our metrics instead of focusing on the metrics of one specific class. The macro average means averaging the class-metrics.\n",
    "\n",
    "We will use the accuracy metric because classes are more or less balanced. If classes were strongly unbalanced, accuracy would not be a good metric choice, as it could be deceiving. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-arena",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(\n",
    "    index=[], columns=[\"Accuracy\", \"F1 Macro\", \"Precision Macro\", \"Recall Macro\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-samoa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_results = pd.DataFrame(\n",
    "    cross_validate(\n",
    "        lda_model,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=5,\n",
    "        scoring=[\"accuracy\", \"f1_macro\", \"precision_macro\", \"recall_macro\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "results_df.loc[\"LDA\", :] = (\n",
    "    cross_val_results[\n",
    "        [\"test_accuracy\", \"test_f1_macro\", \"test_precision_macro\", \"test_recall_macro\"]\n",
    "    ]\n",
    "    .mean()\n",
    "    .values\n",
    ")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-definition",
   "metadata": {},
   "source": [
    "###  QDA\n",
    "\n",
    "QDA is very similar to LDA.  The main difference is that in this model we do not assume that all the classes have the same covariance. This leads to obtaining a quadratic decision surface. This model can also be regularized with its regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-adventure",
   "metadata": {},
   "outputs": [],
   "source": [
    "qda_model = QuadraticDiscriminantAnalysis(reg_param=0.1).fit(X_train, y_train)\n",
    "\n",
    "print(\"Priors:\", qda_model.priors_)\n",
    "print(\"Means:\\n\")\n",
    "means = pd.DataFrame(qda_model.means_)\n",
    "means.columns = wine.columns[1:]\n",
    "means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-antigua",
   "metadata": {},
   "source": [
    "Now the prediction on the train data is _almost_ perfect. There are three normal wines that the model confused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-direction",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion(y_train, pd.Series(qda_model.predict(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-insert",
   "metadata": {},
   "source": [
    "If we compute again or cv metrics we can see that there is not a lot of difference but LDA is wining. That might be because our dataset is too easy, which makes it difficult to compare the models (since all of them do very well). *Remember this when you chose your dataset for the final project*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-gibson",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_results = pd.DataFrame(\n",
    "    cross_validate(\n",
    "        qda_model,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=5,\n",
    "        scoring=[\"accuracy\", \"f1_macro\", \"precision_macro\", \"recall_macro\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "results_df.loc[\"QDA\", :] = (\n",
    "    cross_val_results[\n",
    "        [\"test_accuracy\", \"test_f1_macro\", \"test_precision_macro\", \"test_recall_macro\"]\n",
    "    ]\n",
    "    .mean()\n",
    "    .values\n",
    ")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-bradford",
   "metadata": {},
   "source": [
    "### $k$-NN\n",
    "\n",
    "$k$-nn stores the training set samples and uses them to classify the new ones. Each new sample will go to the class with the most similar values from the train set (in terms of a specific distance).\n",
    "\n",
    "The $k$ value tells how many samples will be used to compare. \n",
    "\n",
    "We are going to use gridsearch cross-validation to decide which one is the best $k$ and the best distance metric for our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-algeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "\n",
    "knn_cv = GridSearchCV(\n",
    "    estimator=knn,\n",
    "    param_grid={\n",
    "        \"n_neighbors\": [1, 3, 5, 7, 10, 15, 20],\n",
    "        \"metric\": [\"euclidean\", \"minkowski\", \"manhattan\"],\n",
    "    },\n",
    "    scoring=[\"accuracy\", \"f1_macro\", \"precision_macro\", \"recall_macro\"],\n",
    "    refit=False,\n",
    ")\n",
    "\n",
    "knn_cv.fit(X_train, y_train)\n",
    "results_cv = pd.DataFrame(knn_cv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-cruise",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cv.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-socket",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"param_n_neighbors\",\n",
    "    \"param_metric\",\n",
    "    \"mean_test_accuracy\",\n",
    "    \"mean_test_f1_macro\",\n",
    "    \"mean_test_precision_macro\",\n",
    "    \"mean_test_recall_macro\",\n",
    "    \"std_test_accuracy\",\n",
    "    \"std_test_f1_macro\",\n",
    "    \"std_test_precision_macro\",\n",
    "    \"std_test_recall_macro\",\n",
    "]\n",
    "results_cv[cols].sort_values(by=\"mean_test_f1_macro\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-function",
   "metadata": {},
   "source": [
    "It looks like the parameters don't affect the model performance by much this time. We will take $k$=7 and distance = 'minkowski' and add $k$-nn to our results table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-rotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=7, metric=\"minkowski\")\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "cross_val_results = pd.DataFrame(\n",
    "    cross_validate(\n",
    "        knn,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=5,\n",
    "        scoring=[\"accuracy\", \"f1_macro\", \"precision_macro\", \"recall_macro\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "results_df.loc[\"KNN\", :] = (\n",
    "    cross_val_results[\n",
    "        [\"test_accuracy\", \"test_f1_macro\", \"test_precision_macro\", \"test_recall_macro\"]\n",
    "    ]\n",
    "    .mean()\n",
    "    .values\n",
    ")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-australia",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes\n",
    "\n",
    "The next model we are going to use is Gaussian Naive Bayes. \n",
    "\n",
    "Naive Bayes assumes that the attributes of the class conditional probabilities are independent and a certain distribution for them based on the kind of data we are working with.\n",
    "\n",
    "For example: if we are working with numerical variables it will assume that the features are conditionally independent between them and that they follow a Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-pavilion",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "gaussian_nb.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "cross_val_results = pd.DataFrame(\n",
    "    cross_validate(\n",
    "        gaussian_nb,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=5,\n",
    "        scoring=[\"accuracy\", \"f1_macro\", \"precision_macro\", \"recall_macro\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "results_df.loc[\"Gaussian Naive Bayes\", :] = (\n",
    "    cross_val_results[\n",
    "        [\"test_accuracy\", \"test_f1_macro\", \"test_precision_macro\", \"test_recall_macro\"]\n",
    "    ]\n",
    "    .mean()\n",
    "    .values\n",
    ")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-finland",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "\n",
    "Finally, we apply logistic regression to see if it improves performance over the previous models.\n",
    "We use the regularized version, and will optimize its regularization parameter 'C' using cross-validation as usual.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-swaziland",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegressionCV(\n",
    "    Cs=20, random_state=1, cv=10, scoring=\"accuracy\", multi_class=\"multinomial\"\n",
    ")\n",
    "\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-alpha",
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure out optimal parameter value for 'C'\n",
    "\n",
    "avg_crossval_scores = logreg.scores_[\"normal\"].mean(axis=0)\n",
    "idx = np.argmax(avg_crossval_scores)\n",
    "best_C = logreg.Cs_[idx]\n",
    "print(best_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-gilbert",
   "metadata": {},
   "source": [
    "Let us use the optimal C found in the cross-val metrics for our results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-ferry",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(C=best_C, multi_class=\"multinomial\")\n",
    "cross_val_results = pd.DataFrame(\n",
    "    cross_validate(\n",
    "        logreg,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=5,\n",
    "        scoring=[\"accuracy\", \"f1_macro\", \"precision_macro\", \"recall_macro\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "results_df.loc[\"Logistic Regression\", :] = (\n",
    "    cross_val_results[\n",
    "        [\"test_accuracy\", \"test_f1_macro\", \"test_precision_macro\", \"test_recall_macro\"]\n",
    "    ]\n",
    "    .mean()\n",
    "    .values\n",
    ")\n",
    "\n",
    "results_df.sort_values(by=\"Accuracy\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-forge",
   "metadata": {},
   "source": [
    "Our results show that Gaussian Naive Bayes is the best model for our wine data, we saw at the beggining that this dataset was extreamly easy, so it makes sense that results are so good and so close. We can also see that the models that have performed better are the most simple ones. \n",
    "\n",
    "### See generalization performance\n",
    "\n",
    "Now we can chose **Logistic regression** as our best model for the wine data. As there are no more decisions to take we can now check the generalization performance of our best model. To ensure that this generalization performance is the closest posible to the performance we would obtain on new data, we are going to use our test partition to evaluate it. \n",
    "\n",
    "This time we can see the confusion matrix, as well as the classification report for the test set. This will give us the most detailed possible information about our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-wheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "confusion(y_test, pd.Series(y_pred))\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-elephant",
   "metadata": {},
   "source": [
    "Our results show that our simple model has been able to predict perfectly our simple test data. Usually, if we obtain perfect results we should be worried, as normally we only obtain them if we make a mistake on the code. But in this particular situation we can belive our results, as we were solving a toy problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-cylinder",
   "metadata": {},
   "source": [
    "## SECTION 2. Heart Dataset \n",
    "Now we are going to classify a more difficult dataset: Our goal now is to predict whether a patient has a heart disease using medical data (https://www.kaggle.com/ronitf/heart-disease-uci). \n",
    "\n",
    "In this context we don't want to miss any patient with the disease, as she might die. We should take this into account when we select our metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-israel",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "heart = pd.read_csv(\"heart.csv\", delimiter=\",\")\n",
    "\n",
    "numerical_cols = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\"]\n",
    "\n",
    "categorical_cols = [\"sex\", \"cp\", \"exang\", \"fbs\", \"restecg\", \"slope\", \"ca\", \"thal\"]\n",
    "\n",
    "target = \"target\"\n",
    "\n",
    "heart[categorical_cols] = heart[categorical_cols].astype(\"category\")\n",
    "\n",
    "heart.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-supervisor",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart.columns[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-separate",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 4, figsize=(16, 14))\n",
    "\n",
    "for i, c in enumerate(heart.columns[:-1]):\n",
    "    ax = axes.reshape(-1)[i]\n",
    "    if c in categorical_cols:\n",
    "        a = sns.countplot(x=c, data=heart, ax=ax, hue=\"target\")\n",
    "    else:\n",
    "        sns.histplot(x=c, data=heart, hue=\"target\", ax=ax)\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-parliament",
   "metadata": {},
   "source": [
    "### Resampling protocol\n",
    "\n",
    "We have a bit more samples that before, so this time we are going to use a train val test strategy. The previous strategy would be a good choice, as we do not have a lot of samples and the models we are using are fast, but we are going to use the train val test strategy instead so you have an example of both. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-recording",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = heart.loc[:, heart.columns != \"target\"]\n",
    "y = heart[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=43\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.33, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-capability",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "We have different ranges and categorical variables this time. \n",
    "We should handle these two problems in our pre-processing.\n",
    "Notice the different treatment of trainin set and validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-chinese",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(X, y, scaler=None):\n",
    "    # We scale the numerical columns\n",
    "    if scaler is None:\n",
    "        # We only want the scaler to fit the train data\n",
    "        scaler = MinMaxScaler()\n",
    "        X.loc[:, numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
    "    else:\n",
    "        X.loc[:, numerical_cols] = scaler.transform(X[numerical_cols])\n",
    "    # We apply one-hot-encoding to the categorical columns\n",
    "    X = pd.get_dummies(X, drop_first=True)\n",
    "    return X, y, scaler\n",
    "\n",
    "\n",
    "X_train, y_train, scaler = preprocessing(X_train, y_train)\n",
    "X_val, y_val, _ = preprocessing(X_val, y_val, scaler)\n",
    "X_test, y_test, _ = preprocessing(X_test, y_test, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-style",
   "metadata": {},
   "source": [
    "### Choosing our metrics\n",
    "\n",
    "We are going to use the LDA validation results to explain which metrics we are using and why. \n",
    "\n",
    "This time we want to avoid missing any patients with a heart condition, so classifying a 1 as a 0 will be very harmful to our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-petersburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LinearDiscriminantAnalysis()\n",
    "lda_model.fit(X_train, y_train)\n",
    "y_pred_lda = lda_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-ireland",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion(pd.Series(y_val), pd.Series(y_pred_lda))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-china",
   "metadata": {},
   "source": [
    "If we look into our confusion matrix, we have 2 samples that had a heart disease that where classified as healthy. \n",
    "That is a 5% of dead patients (which is unacceptable). We want to select a metric that penalizes this (i.e. will be lower when we predict patients with the disease wrongly). \n",
    "\n",
    "So our best choice would be the **recall of the class 1**. If you remember the recall formula: \n",
    "$$recall_c =\\frac{tp}{tp + fn}$$\n",
    "\n",
    "A recall of 1 would mean 0 samples predicted as false negative, i.e. cero patients with heart disease predicted as healthy.  \n",
    "\n",
    "If we chose only this metric, we would risk a model that predicts only class 1, that might not be ideal, as we would need to check all the patients, which could saturate the hospital. To avoid this we will also use the **f1-score of the class 1** as secondary metric. \n",
    "\n",
    "It is important to be careful while chosing our metrics and addapt the best we can to the context of our problem, so we can really chose the best model for our context (which might not be the one with the best accuracy).\n",
    "\n",
    "We will print a set of metrics, not just these ones, but these ones are going to be the ones we will use to take our decisions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-trunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_val, y_pred_lda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-testimony",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_real, y_pred):\n",
    "    # By default it will compute the binary recall of class 1, we can specify which class do we want by using this parameter\n",
    "    recall_class_1 = recall_score(y_real, y_pred, pos_label=1)\n",
    "    f1_class_1 = f1_score(y_real, y_pred, pos_label=1)\n",
    "    accuracy = accuracy_score(y_real, y_pred)\n",
    "    f1_macro = f1_score(y_real, y_pred, average=\"macro\")\n",
    "    precison_macro = precision_score(y_real, y_pred, average=\"macro\")\n",
    "    recall_macro = recall_score(y_real, y_pred, average=\"macro\")\n",
    "    return [\n",
    "        recall_class_1,\n",
    "        f1_class_1,\n",
    "        accuracy,\n",
    "        f1_macro,\n",
    "        precison_macro,\n",
    "        recall_macro,\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-sperm",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_heart = pd.DataFrame(\n",
    "    index=[],\n",
    "    columns=[\n",
    "        \"**Recall class 1**\",\n",
    "        \"**F1 class 1**\",\n",
    "        \"Accuracy\",\n",
    "        \"F1 Macro\",\n",
    "        \"Precision Macro\",\n",
    "        \"Recall Macro\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "results_heart.loc[\"LDA\", :] = compute_metrics(y_val, y_pred_lda)\n",
    "results_heart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-knock",
   "metadata": {},
   "source": [
    "### QDA\n",
    "Now we are going to train the rest of our models with this new dataset. And store our metrics on our results dataframe. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-coast",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pd.MultiIndex.from_arrays([[], []], names=(\"model\", \"reg\"))\n",
    "results_qda = pd.DataFrame(\n",
    "    index=index,\n",
    "    columns=[\n",
    "        \"**Recall class 1**\",\n",
    "        \"**F1 class 1**\",\n",
    "        \"Accuracy\",\n",
    "        \"F1 Macro\",\n",
    "        \"Precision Macro\",\n",
    "        \"Recall Macro\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "regularization_parameters = [0, 0.0001, 0.001, 0.01, 0.1, 0.5, 1]\n",
    "\n",
    "for reg in regularization_parameters:\n",
    "    qda_model = QuadraticDiscriminantAnalysis(reg_param=reg)\n",
    "    qda_model = qda_model.fit(X_train, y_train)\n",
    "    y_pred = qda_model.predict(X_val)\n",
    "    results_qda.loc[(\"QDA\", reg), :] = compute_metrics(y_val, y_pred)\n",
    "\n",
    "results_qda.sort_values(by=\"**Recall class 1**\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-joseph",
   "metadata": {},
   "source": [
    "According to our validation results, the best parameter is 10. We will use this model to compare with the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-surge",
   "metadata": {},
   "outputs": [],
   "source": [
    "qda_model = QuadraticDiscriminantAnalysis(reg_param=0.1)\n",
    "qda_model = qda_model.fit(X_train, y_train)\n",
    "y_pred_qda = qda_model.predict(X_val)\n",
    "\n",
    "results_heart.loc[\"QDA-0.1\", :] = compute_metrics(y_val, y_pred_qda)\n",
    "results_heart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-klein",
   "metadata": {},
   "source": [
    "### KNN \n",
    "\n",
    "Lets follow the same process with knn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-childhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pd.MultiIndex.from_arrays([[], []], names=(\"model\", \"k\"))\n",
    "results_knn = pd.DataFrame(\n",
    "    index=index,\n",
    "    columns=[\n",
    "        \"**Recall class 1**\",\n",
    "        \"**F1 class 1**\",\n",
    "        \"Accuracy\",\n",
    "        \"F1 Macro\",\n",
    "        \"Precission Macro\",\n",
    "        \"Recall Macro\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "k_values = range(1, 50, 2)\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn = knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_val)\n",
    "    results_knn.loc[(\"KNN\", k), :] = compute_metrics(y_val, y_pred)\n",
    "\n",
    "results_knn.sort_values(by=\"**Recall class 1**\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-dodge",
   "metadata": {},
   "source": [
    "According to our metrics we will chose k = 49."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-expression",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=49)\n",
    "knn = knn.fit(X_train, y_train)\n",
    "y_pred_knn = knn.predict(X_val)\n",
    "results_heart.loc[\"KNN-49\", :] = compute_metrics(y_val, y_pred_knn)\n",
    "results_heart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-poison",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n",
    "Our final model, now we are going to use Naive Bayes in two different ways. First we are going to use gaussian naive bayes, asuming our variables are numerical. This is not fully true, as some of them are categorical variables transformed with a one-hot encoding. \n",
    "\n",
    "In the second way we will try to take advantage of these categorical variables by using to different naive bayes model, one categorical for the categorical variables and one gaussian for the numerical. As it is a probabilistic classifier we can use probabilities from both classifiers to predict our samples.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-ensemble",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_nb = GaussianNB()\n",
    "gaussian_nb = gaussian_nb.fit(X_train, y_train)\n",
    "y_pred_gnb = gaussian_nb.predict(X_val)\n",
    "\n",
    "results_heart.loc[\"Gaussian-NB\", :] = compute_metrics(y_val, y_pred_gnb)\n",
    "results_heart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-continent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_only_numerical(X, scaler=None):\n",
    "    # We scale the numerical columns\n",
    "    if scaler is None:\n",
    "        # We only want the scaler to fit the train data\n",
    "        scaler = MinMaxScaler()\n",
    "        X.loc[:, numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
    "    else:\n",
    "        X.loc[:, numerical_cols] = scaler.transform(X[numerical_cols])\n",
    "    return X, scaler\n",
    "\n",
    "\n",
    "# We need to load again our data so we can pre-process it differently\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=43\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.33, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "X_train_numerical, scaler = preprocessing_only_numerical(X_train[numerical_cols])\n",
    "X_train_categorical = X_train[categorical_cols]\n",
    "\n",
    "X_val_numerical, _ = preprocessing_only_numerical(X_val[numerical_cols], scaler)\n",
    "X_val_categorical = X_val[categorical_cols]\n",
    "\n",
    "X_test_numerical, _ = preprocessing_only_numerical(X_test[numerical_cols], scaler)\n",
    "X_test_categorical = X_test[categorical_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-token",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_nb = GaussianNB()\n",
    "gaussian_nb = gaussian_nb.fit(X_train_numerical, y_train)\n",
    "y_pred_gnb_num = gaussian_nb.predict(X_val_numerical)\n",
    "\n",
    "results_heart.loc[\"Gaussian-NB-only-numerical\", :] = compute_metrics(\n",
    "    y_val, y_pred_gnb_num\n",
    ")\n",
    "\n",
    "\n",
    "cat_nb = CategoricalNB()\n",
    "cat_nb = cat_nb.fit(X_train_categorical, y_train)\n",
    "y_pred_gnb_cat = cat_nb.predict(X_val_categorical)\n",
    "\n",
    "results_heart.loc[\"Gaussian-NB-only-categorical\", :] = compute_metrics(\n",
    "    y_val, y_pred_gnb_cat\n",
    ")\n",
    "\n",
    "results_heart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-resource",
   "metadata": {},
   "source": [
    "Now we are going to use our probabilistic classifiers trick, we can use the probabilities prediction together by multiplying them, sometimes this kind of aggregation can improve the results of the two independent models. \n",
    "\n",
    "We can see on our results that this is not the case. For some reason our only numerical naive bayes is not working super well and it looks like it is affecting the aggregated results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-armenia",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_prediction_proba = cat_nb.predict_proba(\n",
    "    X_val_categorical\n",
    ") * gaussian_nb.predict_proba(X_val_numerical)\n",
    "\n",
    "combined_prediction = np.argmax(combined_prediction_proba, axis=1)\n",
    "\n",
    "results_heart.loc[\"Combined-NB\", :] = compute_metrics(y_val, combined_prediction)\n",
    "results_heart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-calibration",
   "metadata": {},
   "source": [
    "Now there is a last probability trick that we can use. As we want to be completely sure that our model is predicting 0 only when the patient is not diseased, instead of selecting the class with the maximum probability value, we can select class 0 only when the combined probability of class 0 is greater than 0.6.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-silence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_cero_when_you_are_sure(x):\n",
    "    if x[0] > 0.6:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "combined_pred_tuned = pd.DataFrame(combined_prediction_proba).apply(\n",
    "    only_cero_when_you_are_sure, axis=1\n",
    ")\n",
    "\n",
    "results_heart.loc[\"Combined-NB-tuned\", :] = compute_metrics(y_val, combined_pred_tuned)\n",
    "\n",
    "results_heart.sort_values(by=\"**Recall class 1**\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-german",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "\n",
    "The last model we are going to compare is (regularized) Logistic regression. We are going to optimize its regularization parameter as in the other models, by picking the one that maximizes recall on class 1 on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-offer",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pd.MultiIndex.from_arrays([[], []], names=(\"model\", \"C\"))\n",
    "results_logreg = pd.DataFrame(\n",
    "    index=index,\n",
    "    columns=[\n",
    "        \"**Recall class 1**\",\n",
    "        \"**F1 class 1**\",\n",
    "        \"Accuracy\",\n",
    "        \"F1 Macro\",\n",
    "        \"Precission Macro\",\n",
    "        \"Recall Macro\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "lambdas = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "\n",
    "for l in lambdas:\n",
    "    C = 1 / l\n",
    "    logreg = LogisticRegression(C=C)\n",
    "    logreg = logreg.fit(X_train, y_train)\n",
    "    y_pred = logreg.predict(X_val)\n",
    "    results_logreg.loc[(\"LogReg\", C), :] = compute_metrics(y_val, y_pred)\n",
    "\n",
    "results_logreg.sort_values(by=\"**Recall class 1**\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-stocks",
   "metadata": {},
   "source": [
    "It seems the best logistic regression model according to recall on class 1 is with regularization parameter C=0.1. Let us use this model and compare it to the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-minister",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(C=0.1).fit(X_train, y_train)\n",
    "y_pred_logreg = logreg.predict(X_val)\n",
    "results_heart.loc[\"LogReg-0.1\", :] = compute_metrics(y_val, y_pred_logreg)\n",
    "\n",
    "results_heart.sort_values(by=\"**Recall class 1**\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-contractor",
   "metadata": {},
   "source": [
    "This way we are obtaining the best model in terms of recall of the class 1. Notice that this model is not the best in terms of the F1 of class 1 nor the F1 Macro.  If we check our confusion matrix we can see that this model has predicted almost all patients correctly at the cost of having worse performance on the other class. \n",
    "\n",
    "In this specific context we wanted to absolutly prioritize avoiding misclassifying class 1. So our best choice would be the **Combined Naive Bayes Tuned**, if our criteria was different or the importance of missclasifying was more balanced between the classes the best model would be **LDA**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-shock",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Combined Naive Bayes Tuned\")\n",
    "confusion(y_val, combined_pred_tuned)\n",
    "\n",
    "print(\"LDA\")\n",
    "confusion(y_val, pd.Series(y_pred_lda))\n",
    "\n",
    "print(\"QDA\")\n",
    "confusion(y_val, pd.Series(y_pred_qda))\n",
    "\n",
    "print(\"Logistic Regression\")\n",
    "confusion(y_val, pd.Series(y_pred_logreg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-decade",
   "metadata": {},
   "source": [
    "### See generalization performance\n",
    "\n",
    "Now that we have decided our model. As there are no more decisions to make we can now check its generalization performance. To ensure that this generalization performance is the closest posible to the performance we would obtain on new data, we are going to use our test partition to evaluate it. \n",
    "\n",
    "This time we can see the confusion matrix, as well as the classification report for the test set. This will give us the most detailed possible information about our results.\n",
    "\n",
    "These results are quite similar to the validation ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-medium",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_prediction_proba_test = cat_nb.predict_proba(\n",
    "    X_test_categorical\n",
    ") * gaussian_nb.predict_proba(X_test_numerical)\n",
    "combined_pred_tuned_test = pd.DataFrame(combined_prediction_proba_test).apply(\n",
    "    only_cero_when_you_are_sure, axis=1\n",
    ")\n",
    "\n",
    "confusion(y_test, pd.Series(combined_pred_tuned_test))\n",
    "\n",
    "print(classification_report(y_test, combined_pred_tuned_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-pharmacy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-booking",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
