{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25ed12e8",
   "metadata": {},
   "source": [
    "# AA1 lab 06\n",
    "\n",
    "# Dealing with class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6a3f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.datasets import load_breast_cancer, make_classification\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "pd.set_option(\"display.precision\", 3)\n",
    "np.random.seed(123)  # for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bab381",
   "metadata": {},
   "source": [
    "It is very common that datasets for classification present an imbalance in the proportion of classes. This is very common for example\n",
    "with health data where there may be an undesirable, uncommon outcome which is the one we need to detect.\n",
    "\n",
    "In these cases it is often the case that the different types of errors that a predictor may make are not equal in the sense of their consequence in cost.\n",
    "As an example, consider a fraud detection system (binary classification task) that classifies transactions (credit card purchases for example) as \n",
    "being fraudulent (positive class) or not (negative class). In this case, it is very likely that the system will see few cases of fraud and many many cases of \n",
    "non-fraudulent transactions.\n",
    "\n",
    "Suppose that only 1% of transactions are fraudulent. It is trivial to get a classifier with 99% accuracy: just predict __always negative__. But such a classifier\n",
    "is __useless__ since typically we would be very interested in identifying the _positive_ (fraudulent) cases correctly! \n",
    "\n",
    "It makes sense to look at the confusion matrix where we can see the counts for the different types of errors:\n",
    "\n",
    "|              | Predicted Negative | Predicted Positive |\n",
    "|--------------|--------------------|--------------------|\n",
    "| Actual Negative | True Negative (TN) | False Positive (FP) |\n",
    "| Actual Positive | False Negative (FN) | True Positive (TP) |\n",
    "\n",
    "In our case of fraud detection, we would have:\n",
    "\n",
    "|              | Predicted Negative | Predicted Positive |\n",
    "|--------------|--------------------|--------------------|\n",
    "| Actual Negative | 99 | 0 |\n",
    "| Actual Positive | 1 | 0 |\n",
    "\n",
    "$$accuracy = \\frac{99}{99+1+0+0} = 99\\%$$\n",
    "\n",
    "\n",
    "So, the important message here is the following\n",
    "\n",
    "> __DO NOT use accuracy for a classification problem where there is class imbalance__.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24852888",
   "metadata": {},
   "source": [
    "## Metrics for classification performance\n",
    "\n",
    "\n",
    "### - Accuracy:\n",
    "\n",
    "$$accuracy = \\frac{\\sum_c tp_c}{n}$$\n",
    "\n",
    "Where $tp_c$ are the true positive predictions for all the classes and n are the total number of samples. This metric is **sensitive to imbalanced data**.\n",
    "\n",
    "### - Precision (of a class):\n",
    "\n",
    "$$precision_c = \\frac{tp}{tp + fp}$$\n",
    "\n",
    "Where $tp$ are the true positives (samples correctly predicted of this class) and $fp$ are the false positives (samples from another class predicted incorrectly as this class). This metric measures how much the model is predicting correctly a class with respect all the predictions of this class. We will use this metric when having false positive predictions is very harmful in our model context. \n",
    "\n",
    "\n",
    "### - Recall (of a class):\n",
    "\n",
    "$$recall_c = \\frac{tp}{tp + fn}$$\n",
    "\n",
    "Where $tp$ are the true positives (samples correctly predicted of this class) and fn are the false negatives (samples from this class predicted incorrectly as a different class). This metric measures how much the model is predicting correctly a class with respect all the real values of this class. We will use this metric when having false negative predictions is very harmful in our model context.\n",
    "\n",
    "### - F1-score (of a class):\n",
    "\n",
    "$$\\frac{2 * precission_c * recall_c }{precission_c + recall_c}$$\n",
    "\n",
    "The **harmonic mean** of precision and recall. We will use this metric when we want a good balance between precision and recall.  \n",
    "\n",
    "\n",
    "### - Macro averages:\n",
    "\n",
    "For __binary__ classification problems, it is customary to report metrics for the positive class only.\n",
    "In the case of multi-class classification, if all classes are equally important, then we can report __macro__ averages of the relevant metrics (i.e., average accross all classes). If the correct prediction of a particular class is more important, then we may focus the metrics for that specific class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb12923a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "All of these metrics can be computed from the confusion matrix. As an example, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7489b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [0, 1, 2, 2, 2]\n",
    "y_pred = [0, 0, 2, 2, 1]\n",
    "target_names = [\"class 0\", \"class 1\", \"class 2\"]\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"confusion matrix:\")\n",
    "print(\n",
    "    pd.DataFrame(\n",
    "        data=cm,\n",
    "        columns=[f\"predicted {c}\" for c in target_names],\n",
    "        index=[f\"true {c}\" for c in target_names],\n",
    "    )\n",
    ")\n",
    "print()\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b0363c",
   "metadata": {},
   "source": [
    "# Probabilistic classifiers: ROC curve and AUC\n",
    "\n",
    "Receiver Operating Characteristic (ROC) curve and Area Under the Curve (AUC) are commonly used evaluation metrics in binary classification tasks in machine learning.\n",
    "\n",
    "**ROC Curve:**\n",
    "The ROC curve is a graphical representation that illustrates the performance of a binary classification model __across different threshold settings__. \n",
    "It plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold values.\n",
    "\n",
    "- **True Positive Rate (TPR)**, also known as sensitivity or recall, measures the proportion of actual positive cases that are correctly identified by the model. It is calculated as TP / (TP + FN).\n",
    "- **False Positive Rate (FPR)** measures the proportion of actual negative cases that are incorrectly classified as positive by the model. It is calculated as FP / (FP + TN).\n",
    "\n",
    "The ROC curve demonstrates the trade-off between sensitivity and specificity across different threshold values. A model with a better performance will have an ROC curve that is closer to the top-left corner of the plot, indicating higher TPR and lower FPR across various threshold settings.\n",
    "\n",
    "**Area Under the Curve (AUC):**\n",
    "The AUC is a scalar value that quantifies the overall performance of a binary classification model based on its ROC curve. It represents the area under the ROC curve.\n",
    "\n",
    "- AUC ranges from 0 to 1, where:\n",
    "  - AUC = 1 indicates a perfect classifier that achieves a TPR of 1 and an FPR of 0 across all threshold values, resulting in a curve that reaches the top-left corner of the plot.\n",
    "  - AUC = 0.5 indicates a classifier with no discriminatory power, equivalent to random guessing.\n",
    "  - AUC < 0.5 indicates a classifier that performs worse than random guessing.\n",
    "\n",
    "A higher AUC value suggests better discrimination ability of the model, indicating that it can distinguish between positive and negative cases more effectively across various threshold settings.\n",
    "\n",
    "In summary, ROC curve and AUC provide insights into the performance of binary classification models by visualizing the trade-off between sensitivity and specificity and quantifying the model's ability to discriminate between positive and negative cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65d3585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000, n_features=20, n_classes=2, weights=[0.9, 0.1], random_state=42\n",
    ")\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train a logistic regression model\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities on the test set\n",
    "probs = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=\"ROC curve (AUC = %0.2f)\" % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0ba80e",
   "metadata": {},
   "source": [
    "# Learning in the context of class imbalance\n",
    "\n",
    "So, what do we do if our data is not balanced?\n",
    "\n",
    "Well, as a starter we should definitely avoid using __accuracy__ as our performance metric.\n",
    "In cross-validation, for example, we should focus on those metrics that are relevant to our problem.\n",
    "\n",
    "Apart from that, there is the issue of model training, where we can proceed in several ways:\n",
    "\n",
    "- __resampling__: either down-sample majority class or up-sample minority class to make sure all classess are represented and can be learned from\n",
    "- __cost-sensitive learning__: using algorithms (or adapt existing ones) to avoid specific types of undesirable errors in our application through the introduction of cost matrices\n",
    "\n",
    "\n",
    "Let us look at an example with the [breast cancer Wisconsin data](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2c3d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = load_breast_cancer()\n",
    "df = pd.DataFrame(data=bc.data, columns=bc.feature_names)\n",
    "df[\"target\"] = pd.Series([bc.target_names[y] for y in bc.target], dtype=\"category\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dca40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Wisconsin Breast Cancer dataset\n",
    "X, y = bc.data, bc.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Count class occurrences\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(\"Class distribution in the training set:\", dict(zip(unique, counts)))\n",
    "\n",
    "# Calculate class weights to handle imbalance\n",
    "class_counts = dict(zip(unique, counts))\n",
    "total_samples = sum(class_counts.values())\n",
    "class_weights = {cls: total_samples / count for cls, count in class_counts.items()}\n",
    "\n",
    "# Cross-validate a Random Forest classifier with class weights with n trees\n",
    "res = {}\n",
    "for n in np.geomspace(1, 256, num=9, dtype=int):\n",
    "    clf = RandomForestClassifier(\n",
    "        class_weight=class_weights, random_state=42, n_estimators=n\n",
    "    )\n",
    "    res[n] = np.mean(cross_val_score(clf, X_train, y_train, cv=5, scoring=\"f1_macro\"))\n",
    "\n",
    "# show results of cross-val\n",
    "print(Counter(res).most_common())\n",
    "\n",
    "# select best n\n",
    "best_n = Counter(res).most_common()[0][0]\n",
    "print(f\"winning hyper-param is n_estimators={best_n}\")\n",
    "\n",
    "# re-fit on training data\n",
    "clf = RandomForestClassifier(\n",
    "    class_weight=class_weights, random_state=42, n_estimators=best_n\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# confusion matrix on test\n",
    "print(\"\\nConfusion matrix on test set:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report on test set:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455d3dfd",
   "metadata": {},
   "source": [
    "## Sampling approaches to deal with imbalance\n",
    "\n",
    "1. **Random Undersampling**: Randomly remove samples from the majority class to balance the class distribution. This approach can be effective when there is a large amount of data.\n",
    "\n",
    "2. **Random Oversampling**: Randomly duplicate samples from the minority class to balance the class distribution. This can be effective but may lead to overfitting.\n",
    "\n",
    "3. **SMOTE (Synthetic Minority Over-sampling Technique)**: Generate synthetic samples for the minority class by interpolating between existing minority class samples. This helps in balancing the class distribution without simply duplicating existing samples.\n",
    "\n",
    "4. **ADASYN (Adaptive Synthetic Sampling)**: Similar to SMOTE, but it focuses on generating samples near the decision boundary, which can help to tackle the problem of overlapping classes.\n",
    "\n",
    "5. **Borderline-SMOTE**: A variation of SMOTE that focuses on generating synthetic samples near the border between the minority and majority classes.\n",
    "\n",
    "6. **NearMiss**: Undersampling technique that selects samples from the majority class that are closest to the minority class samples. This helps to retain information from the majority class while reducing its dominance.\n",
    "\n",
    "7. **Tomek Links**: Undersampling technique that removes samples from the majority class that are Tomek links with samples from the minority class. Tomek links are pairs of samples from different classes that are closest to each other.\n",
    "\n",
    "8. **Cluster-Based Over Sampling**: Identify clusters in the minority class and generate synthetic samples within these clusters to balance the class distribution.\n",
    "\n",
    "9. **Weighted Sampling**: Assign higher weights to samples from the minority class during training to make them more influential in the learning process. This can be implemented through class weights in the loss function.\n",
    "\n",
    "10. **Ensemble Methods**: Train multiple classifiers on different subsets of the imbalanced dataset and combine their predictions. This can be done using techniques like bagging, boosting, or stacking.\n",
    "\n",
    "11. **Cost-Sensitive Learning**: Modify the learning algorithm to take into account the misclassification costs associated with different classes. This approach assigns higher penalties to misclassifications of the minority class.\n",
    "\n",
    "These sampling approaches can be used alone or in combination to address class imbalance in classification tasks, depending on the specific characteristics of the dataset and the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e18923d",
   "metadata": {},
   "source": [
    "Next, we can find an example that uses SMOTE to up-sample the minority class and then learn with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a20189f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Load the Wisconsin Breast Cancer dataset\n",
    "X, y = bc.data, bc.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# scale data as we are going to use logreg\n",
    "transformer = RobustScaler().fit(X_train)\n",
    "X_train_scaled = transformer.transform(X_train)\n",
    "X_test_scaled = transformer.transform(X_test)\n",
    "\n",
    "# Count class occurrences\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(\"Class distribution in the training set before SMOTE:\", dict(zip(unique, counts)))\n",
    "\n",
    "# Apply SMOTE to balance the classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Count class occurrences after SMOTE\n",
    "unique_resampled, counts_resampled = np.unique(y_train_resampled, return_counts=True)\n",
    "print(\n",
    "    \"Class distribution in the training set after SMOTE:\",\n",
    "    dict(zip(unique_resampled, counts_resampled)),\n",
    ")\n",
    "\n",
    "# Train a Random Forest classifier after SMOTE\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "\n",
    "# confusion matrix on test\n",
    "print(\"\\nConfusion matrix on test set:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c5732a",
   "metadata": {},
   "source": [
    "## suggested exercise\n",
    "\n",
    "- try other up or downsampling technique on same data with some other model, perhaps with hyper-params to cross-validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72461fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
