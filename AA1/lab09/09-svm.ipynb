{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AA1 lab 09\n",
    "\n",
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to upgrade packages\n",
    "#!pip3 install pandas --upgrade --user --quiet\n",
    "#!pip3 install numpy --upgrade --user --quiet\n",
    "#!pip3 install statsmodels --upgrade --user --quiet\n",
    "#!pip3 install scipy --upgrade --user --quiet\n",
    "#!pip3 install scikit-learn --upgrade --user --quiet\n",
    "#!pip3 install graphviz --upgrade --user --quiet\n",
    "#!pip3 install dython  --upgrade --user --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from time import time\n",
    "from datetime import timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from dython.nominal import associations\n",
    "from dython.nominal import correlation_ratio\n",
    "from dython.nominal import cramers_v\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    r2_score,\n",
    "    mean_squared_error,\n",
    "    median_absolute_error,\n",
    "    mean_absolute_error,\n",
    ")\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "from sklearn.svm import LinearSVR, SVR, SVC\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliar functions\n",
    "\n",
    "On this notebook we are going to use the next auxiliar functions: \n",
    "* `load_and_split_life_expectancy_data`: This will help us split our data taking its temporal component into acount. \n",
    "* `compute_metrics`: will help us compute the specific metrics that we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_life_expectancy_data():\n",
    "    life_expectancy_data = pd.read_csv(\"Life_Expectancy_Data.csv\")\n",
    "    life_expectancy_data.columns = [\n",
    "        c.lower().strip().replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"-\", \"_\")\n",
    "        for c in life_expectancy_data.columns\n",
    "    ]\n",
    "    X = life_expectancy_data.loc[:, life_expectancy_data.columns != \"life_expectancy\"]\n",
    "    y = life_expectancy_data[\"life_expectancy\"]\n",
    "\n",
    "    # Transform categorical columns type to avoid problems when transforming to dummies\n",
    "    categorical_columns = [\"country\", \"status\"]\n",
    "    for column in categorical_columns:\n",
    "        X.loc[:, column] = X.loc[:, column].astype(\"category\")\n",
    "\n",
    "    test_index = X[\"year\"] >= 2013  # 2013 2014 2015\n",
    "    val_index = (X[\"year\"] >= 2010) & (X[\"year\"] < 2013)  # 2010 2011 2012\n",
    "    train_index = ~(test_index | val_index)  # < 2010\n",
    "\n",
    "    X_train = X[train_index]\n",
    "    y_train = y[train_index]\n",
    "\n",
    "    X_val = X[val_index]\n",
    "    y_val = y[val_index]\n",
    "\n",
    "    X_test = X[test_index]\n",
    "    y_test = y[test_index]\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "def comptue_metrics(y_pred, y_real):\n",
    "    r2 = r2_score(y_pred, y_real)\n",
    "    mse = mean_squared_error(y_pred, y_real)\n",
    "    median_abs_e = median_absolute_error(y_pred, y_real)\n",
    "    mean_abs_e = mean_absolute_error(y_pred, y_real)\n",
    "    return [r2, mse, median_abs_e, mean_abs_e]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 1: Life expentancy data\n",
    "\n",
    "In this lab we are going to use the Life expectancy dataset again. Last time we treated the year variable as a categorical variable, which did not take into account possible temporal relationships between the samples (like seasonality). This way, we risked obtaining over-optimistic results. This time we will treat it as a temporal variable and our decisions will be affected by it. \n",
    "\n",
    "### Metrics\n",
    "\n",
    "We are going to use the next regression metrics:\n",
    "\n",
    "**MSE** \n",
    "\n",
    "$$MSE(t,y) = \\frac{1}{N} \\sum_{i=1}^N (t - y(x;w))^2$$\n",
    "\n",
    "Where t is the real value of the target and y(x;w) is our prediction. MSE is comonly used while training the models. You need to be carefull if you want to use it as a metric because it depends strongly of the range of the target. Best value is 0. \n",
    "\n",
    "**R2**\n",
    "$$norm\\_MSE(t,y) = \\frac {MSE(t,y)}{s^2(t)} \\Rightarrow R2 = (1 - norm\\_MSE(t,y))$$ \n",
    "\n",
    "Where t is the real value of the target and y(x;w) is our prediction and $s^2$ is the unbiesed variance ($s^2(t) = \\frac{n}{n-1}\\sigma^2(t)$). This metric represents the target variability explained by the model. It will always be less than one and it is the most used metric on regression problems. Can be affected by having a lot of variables. Best value is 1.\n",
    "\n",
    "**median_absolute_error**  \n",
    "\n",
    "\n",
    "$$MedAE(t,y) = median(|t_1 - y(x;w)_1|,...., |t_n - y(x;w)_n| )$$\n",
    "\n",
    "Where t is the real value of the target and y(x;w) is our prediction. Similar to MSE is scale dependent but it is robuts to outliers.\n",
    "\n",
    "**mean_absolute_error**\n",
    "\n",
    "\n",
    "$$MAE(t,y) = \\frac{1}{N} \\sum_{i=1}^N | t - y(x;w)| ^2$$\n",
    "\n",
    "Where t is the real value of the target and y(x;w) is our prediction. Similar to MSE is scale dependent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "life_expectancy_data = pd.read_csv(\"Life_Expectancy_Data.csv\")\n",
    "life_expectancy_data.columns = [\n",
    "    c.lower().strip().replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"-\", \"_\")\n",
    "    for c in life_expectancy_data.columns\n",
    "]\n",
    "life_expectancy_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we check the number of samples of each year we can see that there are almost the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "life_expectancy_data[\"year\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "life_expectancy_data[\"country\"] = life_expectancy_data[\"country\"].astype(\"category\")\n",
    "life_expectancy_data[\"status\"] = life_expectancy_data[\"status\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we where dealing with this problem in the real life we would probably want to predict the results of the next/next years based on the data collected until now. \n",
    "\n",
    "To simulate this we will use the last 3 years for test, the last 3 years after them for validation and the rest of the data as training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = life_expectancy_data.loc[:,life_expectancy_data.columns != 'life_expectancy']\n",
    "# y = life_expectancy_data['life_expectancy']\n",
    "\n",
    "test_index = life_expectancy_data[\"year\"] >= 2013  # 2013 2014 2015\n",
    "val_index = (life_expectancy_data[\"year\"] >= 2010) & (\n",
    "    life_expectancy_data[\"year\"] < 2013\n",
    ")  # 2010 2011 2012\n",
    "train_index = ~(test_index | val_index)  # < 2010"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how this decision affects our target. Does the target distribution change over time? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "life_expectancy_data.loc[train_index, \"subset\"] = \"train\"\n",
    "life_expectancy_data.loc[val_index, \"subset\"] = \"val\"\n",
    "life_expectancy_data.loc[test_index, \"subset\"] = \"test\"\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "sns.histplot(x=\"life_expectancy\", data=life_expectancy_data, ax=axs[0], kde=True)\n",
    "sns.histplot(\n",
    "    x=\"life_expectancy\",\n",
    "    data=life_expectancy_data,\n",
    "    hue=\"subset\",\n",
    "    fill=True,\n",
    "    ax=axs[1],\n",
    "    kde=True,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing this plot we can see that the distribution doesn't seem to change a lot sepparating by years. This could imply that we have chosen a test an validation sets that are representative with respect to our data and that the behaviour of our target does not change significantly between years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations\n",
    "\n",
    "Now, before starting training models we are going to see the correlations between our variables. \n",
    "\n",
    "If we use pandas correlation method it will compute the pearson correlation betweeen the numerical variables and will completely ignore the categorical ones. \n",
    "\n",
    "\n",
    "With our data we are going to focus on the correlations with the target to see if there are direct relationships between the predictive variables and it. We might also check for strong correlations between predictive variables as that could mean that any of the variables could be redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = life_expectancy_data[\n",
    "    life_expectancy_data.select_dtypes([np.number]).columns\n",
    "].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(corr, center=0, square=True, cbar=True, cmap=\"coolwarm\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what can we do if we want to calculate the correlation between categorical variables or categorical and numerical? \n",
    "\n",
    "There are different methods:\n",
    "* Pearson's R for continuous-continuous cases\n",
    "* Correlation Ratio for categorical-continuous cases\n",
    "* Cramer's V or Theil's U for categorical-categorical cases\n",
    "\n",
    "Library `dython` will detect automatically the type of the variables (as long as they have the proper data type for pandas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = associations(\n",
    "    life_expectancy_data, nan_strategy=\"drop_samples\", figsize=(18, 18), cmap=\"coolwarm\"\n",
    ")\n",
    "\n",
    "correlation = output[\"corr\"]\n",
    "ax = output[\"ax\"]\n",
    "\n",
    "correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like categorical variables are important for predicting the target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = list(life_expectancy_data.select_dtypes([np.number]).columns)\n",
    "cat_cols = list(np.setdiff1d(life_expectancy_data.columns, num_cols))\n",
    "print(f\"nums: {num_cols}\")\n",
    "print(f\"cat: {cat_cols}\")\n",
    "\n",
    "categorical_with_target = [\"country\", \"status\", \"life_expectancy\"]\n",
    "correlation.loc[categorical_with_target, categorical_with_target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "life_expectancy_data[\"country\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the country variable shows a strong correlation with life expectancy we are interested on adding it to our model but if we do a one-hot-encoding we might end with a course of dimensionality as last time we tried. 133 countries imply 133 new sparse variables, which could turn out in a very slow mess.\n",
    "\n",
    "First we will try to predict without these variables and then we will see a new approach to handle mixed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First preprocessing: Ignore categorical variables\n",
    "\n",
    "In our first approach we will use the minimum preprocessing of last time but this time with scaling. This scaling is important because the models (KNNR and SVR) we are using in this lab are strongly affected by range difference by the variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = load_and_split_life_expectancy_data()\n",
    "\n",
    "\n",
    "def minimum_preprocessing(X, y):\n",
    "    print(\"Original shape:{}\".format(X.shape))\n",
    "    # We kill categorical columns\n",
    "    cat_cols = [\"status\", \"country\"]\n",
    "    X = X.drop(columns=cat_cols)\n",
    "    print(\"Droped: {}\".format(cat_cols))\n",
    "    # We remove missing values\n",
    "    X = X.dropna()\n",
    "    y = y[X.index]\n",
    "    # Normalize\n",
    "    X = minmax_scale(X)\n",
    "    print(\"New shape:{}\".format(X.shape))\n",
    "    return X, y\n",
    "\n",
    "\n",
    "X_train, y_train = minimum_preprocessing(X_train, y_train)\n",
    "X_val, y_val = minimum_preprocessing(X_val, y_val)\n",
    "X_test, y_test = minimum_preprocessing(X_test, y_test)\n",
    "\n",
    "# We also instantiate our results dataframe\n",
    "results = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"Kernel\",\n",
    "        \"C\",\n",
    "        \"epsilon\",\n",
    "        \"R2\",\n",
    "        \"MSE\",\n",
    "        \"median_absolute_error\",\n",
    "        \"mean_absolute_error\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: KNN\n",
    "\n",
    "We will take as our baseline the KNN regressor model with the default parameters. This model can be a good baseline for comparing with SVR because it is simple, fast, interpretable and distance based(like SVM). \n",
    "\n",
    "We have seen in our correlation study that there are direct correlations between our predictive variables and our target. That implies that the we should be able to predict it to some term using a simple model. So, if we weren't able to predict something using this model it could be a sign that there is something wrong with our preprocessing.\n",
    "\n",
    "Finding problems in pre-processing or in our data is a lot faster if you use a baseline. With complex models you could think that there is some problem with your hyperparameters and spend hours tuning them until you find that the problem was on other step of your pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor()\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_val)\n",
    "\n",
    "results.loc[\"KNN\", :] = [\"-\", \"-\", \"-\"] + comptue_metrics(y_pred, y_val)\n",
    "\n",
    "sns.scatterplot(x=y_val, y=y_pred)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cheking our metrics and the scaterplot of the predicted vs real validation values of the target, we can see that knn is already doing a good job with its default parameters. This is a good sign, as we had already seen on our data analyisis that predicting this target should not be extreamly dificult for a model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Lineal\n",
    "\n",
    "The basic idea of the SVM regressor is to use a limited number of the training samples (suport vectors) to predict. The prediction will be a linear combination of the suport vectors (similar to linear regression). \n",
    "\n",
    "The powerful part of this model is that using the kernel trick you can modify the original space to fit non-linear functions.\n",
    "\n",
    "This model has the next hyperparameters: \n",
    "* $\\epsilon$ : All errors lower to epsilon will be taken as zero in the loss function. \n",
    "* $C$ : Regularization parameter. It will avoid the model to use too many suport vectors. \n",
    "* Kernel: The function that will modify the original space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = LinearSVR()\n",
    "\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_val)\n",
    "results.loc[\"LinearSVR-default\", :] = [\"linear\", 1, 0] + comptue_metrics(y_pred, y_val)\n",
    "\n",
    "sns.scatterplot(x=y_val, y=y_pred)\n",
    "\n",
    "results.sort_values(by=\"R2\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we train our dummy SVR, we obtain slighly better results than with the KNN. That is a good sign. Let's tune the hyperparameters to see if we can improve more our results.\n",
    "\n",
    "For comparing the models we will use our validation partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"Kernel\",\n",
    "        \"C\",\n",
    "        \"epsilon\",\n",
    "        \"R2\",\n",
    "        \"MSE\",\n",
    "        \"median_absolute_error\",\n",
    "        \"mean_absolute_error\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "Cs = [10, 20, 30, 40, 50, 60]\n",
    "epsilons = [0.001, 0.0001, 0.00001, 0.000001, 0]\n",
    "for c in Cs:\n",
    "    for epsilon in epsilons:\n",
    "        svm = SVR(kernel=\"linear\", C=c, epsilon=epsilon)\n",
    "        svm.fit(X_train, y_train)\n",
    "        y_pred = svm.predict(X_val)\n",
    "        cv_results.loc[\"LinearSVR-{}-{}\".format(c, epsilon), :] = [\n",
    "            \"linear\",\n",
    "            c,\n",
    "            epsilon,\n",
    "        ] + comptue_metrics(y_pred, y_val)\n",
    "\n",
    "best = cv_results.sort_values(by=\"R2\", ascending=False).iloc[0, :]\n",
    "results.loc[\"LinearSVR-best\", :] = best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear svr is a __super fast__ model. For this reason we can try a lot of hyperparameters to see which ones are the better fit to our data.\n",
    "\n",
    "Our validation results show that the hyperparameters are not having a strong effect on our results. \n",
    "The best configuration is `C=60 epsilon=0 R2=0.815502`\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results.sort_values(by=\"R2\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare the best LinearSVR result with the other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.sort_values(by=\"R2\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that hyperparameters are not affecting strongly the results. Let's try with more complex kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-linear SVR\n",
    "\n",
    "We obtain a non-linear SVR by applying a non-linear kernel to our data. In sklearn we have available RBF, Sigmoid and Polynomial kernels. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Radial Basis Function\n",
    "\n",
    "This kernel is a bit more complex. Lets see how handles the data with the default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = SVR(kernel=\"rbf\")\n",
    "svr.fit(X_train, y_train)\n",
    "y_pred = svr.predict(X_val)\n",
    "\n",
    "results.loc[\"RBF-SVR-default\", :] = [\"RBF\", 1, 0] + comptue_metrics(y_pred, y_val)\n",
    "\n",
    "sns.scatterplot(x=y_val, y=y_pred)\n",
    "results.sort_values(by=\"R2\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has a worse result than the Linear kernel. \n",
    "Let's see how tuning the hyperparameters affects the model performance. This time we will try less values because it is a slower model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_rbf = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"Kernel\",\n",
    "        \"C\",\n",
    "        \"epsilon\",\n",
    "        \"R2\",\n",
    "        \"MSE\",\n",
    "        \"median_absolute_error\",\n",
    "        \"mean_absolute_error\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "Cs = [10, 20, 30, 40, 50, 60]\n",
    "epsilons = [0.001, 0.0001, 0.00001, 0.000001, 0]\n",
    "for c in Cs:\n",
    "    for epsilon in epsilons:\n",
    "        svm = SVR(kernel=\"rbf\", C=c, epsilon=epsilon)\n",
    "        svm.fit(X_train, y_train)\n",
    "        y_pred = svm.predict(X_val)\n",
    "        cv_results_rbf.loc[\"RBFSVR-{}-{}\".format(c, epsilon), :] = [\n",
    "            \"RBF\",\n",
    "            c,\n",
    "            epsilon,\n",
    "        ] + comptue_metrics(y_pred, y_val)\n",
    "\n",
    "best = cv_results_rbf.sort_values(by=\"R2\", ascending=False).iloc[0, :]\n",
    "results.loc[\"RBFSVR-best\", :] = best\n",
    "\n",
    "\n",
    "cv_results_rbf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time the hyperparameters have a stronger effect. This means that with bad hyperparameters we could obtain a very bad model while with the correct ones we could surpass the performance of simpler models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values(by=\"R2\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid\n",
    "\n",
    "Another non-linear kernel. This one uses the hyperbolic tangent as non-linearity. \n",
    "\n",
    "It is not looking good by its default results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = SVR(kernel=\"sigmoid\")\n",
    "\n",
    "svr.fit(X_train, y_train)\n",
    "y_pred = svr.predict(X_val)\n",
    "results.loc[\"SigmoidSVR-default\", :] = [\"Sigmoid\", 1, 0] + comptue_metrics(\n",
    "    y_pred, y_val\n",
    ")\n",
    "\n",
    "sns.scatterplot(x=y_val, y=y_pred)\n",
    "results.sort_values(by=\"R2\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And still it does not improve a lot with our hyperparameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_sigmoid = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"Kernel\",\n",
    "        \"C\",\n",
    "        \"epsilon\",\n",
    "        \"R2\",\n",
    "        \"MSE\",\n",
    "        \"median_absolute_error\",\n",
    "        \"mean_absolute_error\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "Cs = [10, 20, 30, 40, 50, 60]\n",
    "epsilons = [0.001, 0.0001, 0.00001, 0.000001, 0]\n",
    "for c in Cs:\n",
    "    for epsilon in epsilons:\n",
    "        svm = SVR(kernel=\"sigmoid\", C=c, epsilon=epsilon)\n",
    "        svm.fit(X_train, y_train)\n",
    "        y_pred = svm.predict(X_val)\n",
    "        cv_results_sigmoid.loc[\"SigmoidSVR-{}-{}\".format(c, epsilon), :] = [\n",
    "            \"Sigmoid\",\n",
    "            c,\n",
    "            epsilon,\n",
    "        ] + comptue_metrics(y_pred, y_val)\n",
    "\n",
    "best = cv_results_sigmoid.sort_values(by=\"R2\", ascending=False).iloc[0, :]\n",
    "\n",
    "results.loc[\"SigmoidSVR-best\", :] = best\n",
    "results.sort_values(by=\"R2\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial kernel \n",
    "\n",
    "For the polynomic kernel we will need another extra hyperparaemeter: The degree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = SVR(kernel=\"poly\")\n",
    "\n",
    "svr.fit(X_train, y_train)\n",
    "y_pred = svr.predict(X_val)\n",
    "results.loc[\"poly-SVR-default\", :] = [\"poly-2\", 1, 0] + comptue_metrics(y_pred, y_val)\n",
    "\n",
    "sns.scatterplot(x=y_val, y=y_pred)\n",
    "results.sort_values(by=\"R2\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we tune the hyperparameters. We are doing a gridsearch trying 3*3*4 combinations of parameters. This can be slow, but the most parameters we try the best we could fit our data. We will need to find the balance between how much we want to wait to obtain good results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_poly = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"Kernel\",\n",
    "        \"C\",\n",
    "        \"epsilon\",\n",
    "        \"R2\",\n",
    "        \"MSE\",\n",
    "        \"median_absolute_error\",\n",
    "        \"mean_absolute_error\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "degrees = [2, 3, 5]\n",
    "Cs = [30, 40, 50]\n",
    "epsilons = [0.001, 0.0001, 0.00001, 0]\n",
    "for degree in degrees:\n",
    "    for c in Cs:\n",
    "        for epsilon in epsilons:\n",
    "            svm = SVR(kernel=\"poly\", degree=degree, C=c, epsilon=epsilon)\n",
    "            svm.fit(X_train, y_train)\n",
    "            y_pred = svm.predict(X_val)\n",
    "            cv_results_poly.loc[\"polySVR-{}-{}-{}\".format(degree, c, epsilon), :] = [\n",
    "                \"poly-{}\".format(degree),\n",
    "                c,\n",
    "                epsilon,\n",
    "            ] + comptue_metrics(y_pred, y_val)\n",
    "\n",
    "best = cv_results_poly.sort_values(by=\"R2\", ascending=False).iloc[0, :]\n",
    "results.loc[\"polySVR-best\", :] = best\n",
    "\n",
    "results.sort_values(by=\"R2\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model until now is the RBF, followed by the polynomical kernel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom kernel\n",
    "\n",
    "One of the more powerful features of SVMs is that you can use a custom kernel. That means that you can addapt completely to your data, whatever shape it has. You could do kernels for mixed data, categorical, text, radio signals, etc. \n",
    "\n",
    "To try our custom kernel we will use the same pre-processing than before but including the categorical variables. The other models would not accept categorical data. For this reason we are using them only here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = load_and_split_life_expectancy_data()\n",
    "\n",
    "\n",
    "def minimum_preprocessing_with_cat(X, y):\n",
    "    print(\"Original shape:{}\".format(X.shape))\n",
    "    categorical_columns = [\"status\", \"country\"]\n",
    "    numerical_columns = [c for c in X.columns if c not in categorical_columns]\n",
    "    # We remove missing values\n",
    "    X = X.dropna()\n",
    "    y = y[X.index]\n",
    "    # Normalize\n",
    "    X[numerical_columns] = minmax_scale(X[numerical_columns])\n",
    "    print(\"New shape:{}\".format(X.shape))\n",
    "    return X, y\n",
    "\n",
    "\n",
    "X_train, y_train = minimum_preprocessing_with_cat(X_train, y_train)\n",
    "X_val, y_val = minimum_preprocessing_with_cat(X_val, y_val)\n",
    "X_test, y_test = minimum_preprocessing_with_cat(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use a custom kernel you need to make a kernel function and comute its gram matrix. Here we have a simple example of the gaussian kernel modified so it can use the categorical variables. \n",
    "\n",
    "We use a simple weight approach to add the categorical columns to our kernel similarity. \n",
    "\n",
    "This way we have two new hyperparameters. The weight we will use when the two samples have the same country and the weight we will use when they have the same status. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(x1, x2, country_weight=0.8, developed_weight=0.6):\n",
    "    # Ensure that x1 and x2 are column vectors\n",
    "    index_numerical = [\n",
    "        False,\n",
    "        True,\n",
    "        False,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "    ]\n",
    "    country_index = 0\n",
    "    status_index = 2\n",
    "\n",
    "    x1 = x1[index_numerical].flatten()\n",
    "    x2 = x2[index_numerical].flatten()\n",
    "\n",
    "    country_similarity = (\n",
    "        country_weight if x1[country_index] == x2[country_index] else 1 - country_weight\n",
    "    )\n",
    "    developed_similarity = (\n",
    "        developed_weight\n",
    "        if x1[status_index] == x2[status_index]\n",
    "        else 1 - developed_weight\n",
    "    )\n",
    "\n",
    "    gamma = 1.0 / len(x1)\n",
    "    sim = (\n",
    "        np.exp(-gamma * np.sum(np.power((x1 - x2), 2)))\n",
    "        * country_similarity\n",
    "        * developed_weight\n",
    "    )\n",
    "\n",
    "    return sim\n",
    "\n",
    "\n",
    "def compute_gram_matrix(X1, X2, country_weight, developed_weight):\n",
    "    gram_matrix = np.zeros((X1.shape[0], X2.shape[0]))\n",
    "    for i, x1 in enumerate(X1):\n",
    "        for j, x2 in enumerate(X2):\n",
    "            gram_matrix[i, j] = gaussian_kernel(\n",
    "                x1, x2, country_weight, developed_weight\n",
    "            )\n",
    "    return gram_matrix\n",
    "\n",
    "\n",
    "cv_results_custom = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"Kernel\",\n",
    "        \"C\",\n",
    "        \"epsilon\",\n",
    "        \"country\",\n",
    "        \"developed\",\n",
    "        \"R2\",\n",
    "        \"MSE\",\n",
    "        \"median_absolute_error\",\n",
    "        \"mean_absolute_error\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "country_weights = [0.6, 0.7, 0.8]\n",
    "developed_weights = [0.7, 0.8, 0.9, 0.99]\n",
    "\n",
    "for c in country_weights:\n",
    "    for d in developed_weights:\n",
    "        clf = SVR(C=50, epsilon=0.00001, kernel=\"precomputed\")\n",
    "        clf.fit(compute_gram_matrix(X_train.values, X_train.values, c, d), y_train)\n",
    "        y_pred = clf.predict(compute_gram_matrix(X_val.values, X_train.values, c, d))\n",
    "        cv_results_custom.loc[\"custom-{}-{}\".format(c, d), :] = [\n",
    "            \"custom-{}-{}\".format(c, d),\n",
    "            50,\n",
    "            0.00001,\n",
    "            c,\n",
    "            d,\n",
    "        ] + comptue_metrics(y_pred, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fix the other hyperparameters to focuss on these new ones. Ideally we would tune all hyperparameters, but that would be too slow. \n",
    "\n",
    "The best parameters we find are 0.6 for the country weight and 0.99 for the developed one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_custom.sort_values(by=\"R2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still don't get better results than the ones we got with the RBF, but we might get better results tunning the hyperparameters further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best = cv_results_custom.sort_values(by=\"R2\", ascending=False).iloc[0, :]\n",
    "results.loc[\"CustomSVR-best\", :] = best\n",
    "results.sort_values(by=\"R2\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our best model. Now we see how it generalizes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = load_and_split_life_expectancy_data()\n",
    "\n",
    "X_train, y_train = minimum_preprocessing(X_train, y_train)\n",
    "X_val, y_val = minimum_preprocessing(X_val, y_val)\n",
    "X_test, y_test = minimum_preprocessing(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our R2 score is significantly lower than the one we obtained with the validation partition. This might be caused by overfitting on the training data with a too complex model or by the representativity of the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = SVR(kernel=\"rbf\", C=50, epsilon=0.000005)\n",
    "svr.fit(X_train, y_train)\n",
    "y_pred = svr.predict(X_val)\n",
    "\n",
    "y_test_pred = svr.predict(X_test)\n",
    "\n",
    "comptue_metrics(y_test_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 2: Labeled Faces in the Wild\n",
    "\n",
    "We are going to work again with the LFW dataset. This time using different strategies and new metrics. \n",
    "\n",
    "This dataset contains images in black and white of public personalities. The task is to clasify the images with the proper name.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics we will use to validate the results: \n",
    "\n",
    "Last time we focussed on Accuracy and Recall. This time we are adding the other metrics that are comonly used on classification problems. \n",
    "\n",
    "\n",
    "**Accuracy:**\n",
    "\n",
    "$$accuracy = \\frac{\\sum_c tp_c}{n}$$\n",
    "\n",
    "Where tp_c are the true positive predictions for all the classes and n are the total number of samples. This metric is sensitive to imbalanced data.  \n",
    "\n",
    "**Precision (of a class):**\n",
    "\n",
    "$$precission_c = \\frac{tp}{tp + fp}$$\n",
    "\n",
    "Where tp aer the true positives (samples correctly predicted of this class) and fp are the false positives (samples from another class predicted incorrectly as this class). This metric measures how much the model is predicting correctly a class with respect all the predictions of this class. We will use this metric when having false positive predictions is very harmful in our model context. \n",
    "\n",
    "\n",
    "**Recall (of a class):**\n",
    "\n",
    "$$recall_c = =\\frac{tp}{tp + fn}$$\n",
    "\n",
    "Where tp are the true positives (samples correctly predicted of this class) and fn are the false negatives (samples from this class predicted incorrectly as a different class). This metric measures how much the model is predicting correctly a class with respect all the real values of this class. We will use this metric when having false negative predictions is very harmful in our model context.\n",
    "\n",
    "**F1-score (of a class):**\n",
    "\n",
    "$$\\frac{2 * precission_c * recall_c }{precission_c + recall_c}$$\n",
    "\n",
    "The harmonic mean of precission and recall. We will use this metric when we want a good balance between precission & recall.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "n_samples, h, w = lfw_people.images.shape\n",
    "X = lfw_people.data\n",
    "n_features = X.shape[1]\n",
    "\n",
    "y = lfw_people.target\n",
    "target_names = lfw_people.target_names\n",
    "n_classes = target_names.shape[0]\n",
    "\n",
    "\n",
    "print(\"Total dataset size:\")\n",
    "print(\"n_samples: {}\".format(n_samples))\n",
    "print(\"n_features: {}\".format(n_features))\n",
    "print(\"n_classes: {}\".format(n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "def preprocessing(X_train, X_test):\n",
    "    X_train = StandardScaler().fit_transform(X_train)\n",
    "    X_test = StandardScaler().fit_transform(X_test)\n",
    "\n",
    "    pca = PCA().fit(X_train)\n",
    "\n",
    "    n_components = (pca.explained_variance_ratio_.cumsum() < 0.99).sum()\n",
    "\n",
    "    pca = PCA(n_components=n_components).fit(X_train)\n",
    "\n",
    "    X_train_pca = pca.transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "\n",
    "    return X_train_pca, X_test_pca\n",
    "\n",
    "\n",
    "X_train, X_test = preprocessing(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(\n",
    "    columns=[\"Accuracy\", \"Recall (mean)\", \"F1-score (mean)\", \"Time(s)\"]\n",
    ")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_time = time()\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "training_time = time() - init_time\n",
    "print(timedelta(seconds=training_time))\n",
    "\n",
    "scores = cross_val_score(svm, X_train, y_train, cv=5)\n",
    "scores_recall = cross_val_score(svm, X_train, y_train, cv=5, scoring=\"recall_macro\")\n",
    "scores_f_score = cross_val_score(svm, X_train, y_train, cv=5, scoring=\"f1_macro\")\n",
    "results_df.loc[\"SVM-default\", :] = [\n",
    "    np.mean(scores),\n",
    "    np.mean(scores_recall),\n",
    "    np.mean(scores_f_score),\n",
    "    training_time,\n",
    "]\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But SVM have an advantage that we can use with unbalanced data. They can weight the C hyperparameter based on the number of samples of each class. Penalyzing this way the majoritary classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_time = time()\n",
    "svm = SVC(class_weight=\"balanced\")\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "training_time = time() - init_time\n",
    "print(timedelta(seconds=training_time))\n",
    "\n",
    "scores = cross_val_score(svm, X_train, y_train, cv=5)\n",
    "scores_recall = cross_val_score(svm, X_train, y_train, cv=5, scoring=\"recall_macro\")\n",
    "scores_f_score = cross_val_score(svm, X_train, y_train, cv=5, scoring=\"f1_macro\")\n",
    "results_df.loc[\"SVM-balanced\", :] = [\n",
    "    np.mean(scores),\n",
    "    np.mean(scores_recall),\n",
    "    np.mean(scores_f_score),\n",
    "    training_time,\n",
    "]\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see the effect of tuning the hyperparameters on this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_time = time()\n",
    "\n",
    "svm = SVC(class_weight=\"balanced\")\n",
    "\n",
    "kernels = [\"linear\", \"rbf\", \"poly\"]\n",
    "Cs = [0.1, 0.001, 0.5, 1, 2, 3, 4, 5, 6, 0.0001]\n",
    "\n",
    "trc = GridSearchCV(\n",
    "    estimator=svm,\n",
    "    param_grid={\"C\": Cs, \"kernel\": kernels},\n",
    "    scoring=[\"accuracy\", \"recall_macro\", \"f1_macro\"],\n",
    "    cv=5,\n",
    "    return_train_score=True,\n",
    "    refit=\"f1_macro\",\n",
    ")\n",
    "\n",
    "model_5CV = trc.fit(X_train, y_train)\n",
    "print(timedelta(seconds=(time() - init_time)))\n",
    "\n",
    "model_5CV.best_score_\n",
    "model_5CV.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that the best kernel for our data is linear and the best C is 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model_5CV.cv_results_).loc[\n",
    "    :,\n",
    "    [\n",
    "        \"mean_fit_time\",\n",
    "        \"std_fit_time\",\n",
    "        \"param_C\",\n",
    "        \"param_kernel\",\n",
    "        \"mean_test_accuracy\",\n",
    "        \"std_test_accuracy\",\n",
    "        \"mean_test_recall_macro\",\n",
    "        \"std_test_recall_macro\",\n",
    "        \"mean_test_f1_macro\",\n",
    "        \"std_test_f1_macro\",\n",
    "    ],\n",
    "].sort_values(by=\"mean_test_f1_macro\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5CV.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our best model. We can re-train it and see if it generalizes properly using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_time = time()\n",
    "svm = SVC(kernel=\"linear\", class_weight=\"balanced\", C=0.1)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "training_time = time() - init_time\n",
    "print(timedelta(seconds=training_time))\n",
    "\n",
    "scores = cross_val_score(svm, X_train, y_train, cv=5)\n",
    "scores_recall = cross_val_score(svm, X_train, y_train, cv=5, scoring=\"recall_macro\")\n",
    "scores_f_score = cross_val_score(svm, X_train, y_train, cv=5, scoring=\"f1_macro\")\n",
    "results_df.loc[\"SVM-best\", :] = [\n",
    "    np.mean(scores),\n",
    "    np.mean(scores_recall),\n",
    "    np.mean(scores_f_score),\n",
    "    training_time,\n",
    "]\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain good results on the test set again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "218.969px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
