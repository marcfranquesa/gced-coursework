{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AA1 lab 08  \n",
    "# Random Forest and Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:16:51.958141Z",
     "start_time": "2020-07-15T10:16:51.950124Z"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment to upgrade packages\n",
    "# !pip install pandas --upgrade --user --quiet\n",
    "# !pip install numpy --upgrade --user --quiet\n",
    "# !pip install scipy --upgrade --user --quiet\n",
    "# !pip install statsmodels --upgrade --user --quiet\n",
    "# !pip install scikit-learn --upgrade --user --quiet\n",
    "# !pip install seaborn --upgrade --user --quiet\n",
    "# !pip install imblearn --upgrade  --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:16:52.869323Z",
     "start_time": "2020-07-15T10:16:51.959469Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.precision\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:16:53.209352Z",
     "start_time": "2020-07-15T10:16:52.871159Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extra imports\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import graphviz\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    f1_score,\n",
    ")\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    VotingClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    StackingClassifier,\n",
    "    ExtraTreesClassifier,\n",
    ")\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from time import time\n",
    "from datetime import timedelta\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:16:53.216829Z",
     "start_time": "2020-07-15T10:16:53.211015Z"
    }
   },
   "outputs": [],
   "source": [
    "def confusion(true, pred):\n",
    "    \"\"\"\n",
    "    Function for pretty printing confusion matrices\n",
    "    \"\"\"\n",
    "    pred = pd.Series(pred)\n",
    "    true = pd.Series(true)\n",
    "\n",
    "    true.name = \"target\"\n",
    "    pred.name = \"predicted\"\n",
    "    cm = pd.crosstab(true.reset_index(drop=True), pred.reset_index(drop=True))\n",
    "    cm = cm[cm.index]\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:16:53.352117Z",
     "start_time": "2020-07-15T10:16:53.218578Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(6046)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 1: Financial Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to work with a dataset of direct marketing campaigns (phone calls) of a Portuguese banking institution. \n",
    "\n",
    "The classification goal is to predict wheather the client will subscribe a term deposit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:16:53.589008Z",
     "start_time": "2020-07-15T10:16:53.353294Z"
    }
   },
   "outputs": [],
   "source": [
    "deposit = read_csv(\"bank-full.csv.gz\", header=0, delimiter=\";\")\n",
    "\n",
    "categorical_columns = [\n",
    "    \"job\",\n",
    "    \"marital\",\n",
    "    \"education\",\n",
    "    \"default\",\n",
    "    \"housing\",\n",
    "    \"loan\",\n",
    "    \"contact\",\n",
    "    \"month\",\n",
    "    \"poutcome\",\n",
    "]\n",
    "deposit[categorical_columns] = deposit[categorical_columns].astype(\"category\")\n",
    "\n",
    "deposit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:16:53.689705Z",
     "start_time": "2020-07-15T10:16:53.590846Z"
    }
   },
   "outputs": [],
   "source": [
    "deposit.describe(include=\"all\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast data preprocessing\n",
    "\n",
    "Let's see how our data looks before we start working on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 4, figsize=(26, 20))\n",
    "\n",
    "for i, c in enumerate(deposit.columns):\n",
    "    if c == \"y\":\n",
    "        continue\n",
    "    ax = axes.reshape(-1)[i]\n",
    "    if deposit[c].dtype.kind == \"O\":\n",
    "        ct = pd.crosstab(index=deposit[c], columns=deposit[\"y\"], normalize=\"index\")\n",
    "        a = ct.plot(kind=\"bar\", stacked=True, ax=ax)\n",
    "    else:\n",
    "        b = sns.histplot(x=c, hue=\"y\", data=deposit, multiple=\"stack\", ax=ax)\n",
    "    t = ax.set_title(c)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset needs a lot of pre-processing ... and it also displays a good mixture of categorical and numeric variables.\n",
    "\n",
    "We have many categorical variables. If some categories are under-represented that might be a problem for our model.  Let's check whether this is the case. \n",
    "\n",
    "The `job` looks ok:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:16:53.704292Z",
     "start_time": "2020-07-15T10:16:53.691852Z"
    }
   },
   "outputs": [],
   "source": [
    "deposit.job.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `education` has 4 values, let's check their frequency... it seems OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:16:53.817511Z",
     "start_time": "2020-07-15T10:16:53.706441Z"
    }
   },
   "outputs": [],
   "source": [
    "deposit.education.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`month` looks very suspicious ... but is OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:16:53.924664Z",
     "start_time": "2020-07-15T10:16:53.820260Z"
    }
   },
   "outputs": [],
   "source": [
    "deposit.month.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the numerical variables: Duration is highly skewed ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:16:54.225435Z",
     "start_time": "2020-07-15T10:16:53.926187Z"
    }
   },
   "outputs": [],
   "source": [
    "deposit.duration.plot.hist(figsize=(8, 6));"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can avoid this that applying a logarithm to this column, as all values are positive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:16:54.430687Z",
     "start_time": "2020-07-15T10:16:54.226802Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"use_inf_as_na\", True)\n",
    "ax = deposit.duration.apply(\"log\").dropna().plot.hist(figsize=(8, 6))\n",
    "plt.xlabel(\"log(duration)\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better! We will use the log transformation on this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:16:54.494303Z",
     "start_time": "2020-07-15T10:16:54.432136Z"
    }
   },
   "outputs": [],
   "source": [
    "deposit[\"duration\"] = deposit.duration.apply(lambda x: np.log(x + 0.001))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `previous` has a bit of strange shape. It looks like it has a lot of 0 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x=\"previous\", hue=\"y\", data=deposit)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with a lot we mean more than an eighty percent. We will use this knowledge to make a new discrete feature from this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(deposit[\"previous\"] == 0).sum() / deposit.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deposit[\"previous_discrete\"] = deposit[\"previous\"].apply(\n",
    "    lambda x: \"0\" if x == 0 else \">0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = pd.crosstab(\n",
    "    index=deposit[\"previous_discrete\"], columns=deposit[\"y\"], normalize=\"index\"\n",
    ")\n",
    "a = ct.plot(kind=\"bar\", stacked=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What should we do with `pdays` and `previous`? It is not clear how to best pre-process them; we shall need some financial expertise ... so we leave them as they are\n",
    "\n",
    "The rest of variables seem OK (but it would take a careful analysis, and a lot of domain knowledge)\n",
    "\n",
    "Let's rename the target ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:16:54.568564Z",
     "start_time": "2020-07-15T10:16:54.495669Z"
    }
   },
   "outputs": [],
   "source": [
    "deposit.rename({\"y\": \"subscribe\"}, axis=\"columns\", inplace=True)\n",
    "deposit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deposit.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit learn decision trees classifier do not handle categorical attributes so we have to transform them to numerical. \n",
    "\n",
    "We are going to transform the categorical columns using onehot-encoding; if there are not too many levels this may work well, but in general when doing this one should pay close attention to variable semantics, trying to reduce the number of levels in an ad-hoc way, etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and resampling protocol\n",
    "Now that we know our data, let's put all this in a preprocessing function.\n",
    "\n",
    "This time we are going to split our data into train, test and validation. We can chose this protocol because we have a lot of samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(X, y):\n",
    "    # Apply log to duration so it looks less skewed\n",
    "    X.loc[:, \"duration\"] = X.loc[:, \"duration\"].apply(lambda x: np.log(x + 0.001))\n",
    "\n",
    "    # Make our new derivated column from previous\n",
    "    X[\"previous_discrete\"] = X[\"previous\"].apply(lambda x: \"0\" if x == 0 else \">0\")\n",
    "\n",
    "    # Transform categorical variables into numbers because we are working with decision trees\n",
    "    for column in X.columns:\n",
    "        if X[column].dtype.kind == \"O\":\n",
    "            X_one_hot = pd.get_dummies(X[column], prefix=column)\n",
    "            X = X.merge(X_one_hot, left_index=True, right_index=True)\n",
    "            X = X.drop(columns=[column])\n",
    "\n",
    "    X = X.dropna()\n",
    "    y = y[X.index]\n",
    "    y = y.replace({\"yes\": 1, \"no\": 0})\n",
    "    X = X.reset_index(drop=True)\n",
    "    y = y.reset_index(drop=True)\n",
    "    return X, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the train/val/test datasets we will choose **stratified partitions**; what this means is that we will keep the same proportion of yes/no examples in each partition. With imbalanced datasets this is important since a random partition may increase the imbalance if we are unlucky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:16:54.976717Z",
     "start_time": "2020-07-15T10:16:54.884249Z"
    }
   },
   "outputs": [],
   "source": [
    "deposit = read_csv(\"bank-full.csv.gz\", header=0, delimiter=\";\")\n",
    "deposit.rename({\"y\": \"subscribe\"}, axis=\"columns\", inplace=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    deposit.loc[:, :\"poutcome\"],\n",
    "    deposit.loc[:, \"subscribe\"],\n",
    "    test_size=0.2,\n",
    "    stratify=deposit.loc[:, \"subscribe\"],\n",
    "    random_state=42,\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.25, stratify=y_train, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "X_train, y_train = preprocessing(X_train, y_train)\n",
    "X_val, y_val = preprocessing(X_val, y_val)\n",
    "X_test, y_test = preprocessing(X_test, y_test)\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape,"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics \n",
    "\n",
    "We have a classification problem with a strong imbalance on the target class. This time we will asume equally important the missclassification errors for both classes. For this reason we are going to use the next metrics to evaluate our model:\n",
    "* F1-score for class 1. \n",
    "* F1-score for class 0. \n",
    "* F1-score macro average. \n",
    "* Accuracy. (Just for checking, accuracy is not the best metric with imbalanced data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1_score_1 = f1_score(y_true, y_pred, average=\"binary\", pos_label=1)\n",
    "    f1_score_0 = f1_score(y_true, y_pred, average=\"binary\", pos_label=0)\n",
    "    f1_score_macro = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    return [accuracy, f1_score_1, f1_score_0, f1_score_macro]\n",
    "\n",
    "\n",
    "results = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"Accuracy\",\n",
    "        \"F1-score (class 1)\",\n",
    "        \"F1-score (class 0)\",\n",
    "        \"F1-score (macro avg)\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models of the Decision Tree family"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree\n",
    "\n",
    "First try a standard decision tree. \n",
    "\n",
    "A Decision Tree predicts the target by learning simple decision rules. During the training it will split the data so it can learn these rules. This model is very fast to train and interpretable but it can overfit easily. \n",
    "\n",
    "Hyperparameters: \n",
    "* `criterion`: gini or entropy \n",
    "* `max_depth`: maximum depth of the tree. Controls complexity. \n",
    "* `min_samples_split`: minimum number of samples required to split an internal node. Controls complexity. \n",
    "* `min_samples_leaf`: inimum number of samples required to be at a leaf node. Controls complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:16:55.274511Z",
     "start_time": "2020-07-15T10:16:54.978202Z"
    }
   },
   "outputs": [],
   "source": [
    "model_tree = DecisionTreeClassifier().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:16:55.323824Z",
     "start_time": "2020-07-15T10:16:55.276922Z"
    }
   },
   "outputs": [],
   "source": [
    "dot_data = export_graphviz(\n",
    "    model_tree,\n",
    "    out_file=None,\n",
    "    feature_names=X_train.columns,\n",
    "    class_names=[\"no\", \"yes\"],\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    special_characters=True,\n",
    "    rotate=True,\n",
    "    proportion=True,\n",
    "    max_depth=3,\n",
    ")\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate our first decision tree model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:16:55.439638Z",
     "start_time": "2020-07-15T10:16:55.325875Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = model_tree.predict(X_val)\n",
    "\n",
    "results.loc[\"DT-default\", :] = compute_metrics(y_val, y_pred)\n",
    "\n",
    "confusion(y_val, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the detailed classification report to decide if we have chosen the best metrics for our problem. \n",
    "\n",
    "From the detailed report we can see that the classification for the class `yes` is not very good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        y_val,\n",
    "        y_pred,\n",
    "        target_names=[\"no\", \"yes\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check if this model is overfitting by computing its train metrics and checking its complexity (is it a huge tree?)\n",
    "\n",
    "As we can see on our results this model is able to predict the training data perfectly. It is possible that it has adapted too much to it, and because of that, we are obtaining those bad results on our validation partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = model_tree.predict(X_train)\n",
    "\n",
    "confusion(y_train, y_pred_train)\n",
    "\n",
    "print(\n",
    "    classification_report(\n",
    "        y_train,\n",
    "        y_pred_train,\n",
    "        target_names=[\"no\", \"yes\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we check our classification tree we can see that the model with the default hyperparameters has generated a huge tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Tree depht: {}\\nNodes: {}\".format(\n",
    "        model_tree.tree_.max_depth, model_tree.tree_.node_count\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to improve these results by optimizing its hyperparameters. We are going to use gridsearchcv from sklearn because its implementation is faster but we could just use our validation split to compare all the models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = [\"gini\", \"entropy\"]\n",
    "\n",
    "max_dephts = [None, 5, 10, 15, 20]\n",
    "min_samples_split = [1, 2, 3, 4, 5]\n",
    "min_samples_leaf = [1, 2, 3, 4, 5]\n",
    "max_features = [\"auto\", \"sqrt\", \"log2\", None]\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "init_time = time()\n",
    "model_tree = DecisionTreeClassifier()\n",
    "\n",
    "f1_class_0_scorer = make_scorer(f1_score, pos_label=0)\n",
    "f1_class_1_scorer = make_scorer(f1_score, pos_label=1)\n",
    "\n",
    "scoring_dict = {\n",
    "    \"f1_mac\": \"f1_macro\",\n",
    "    \"f1_class_0\": f1_class_0_scorer,\n",
    "    \"f1_class_1\": f1_class_1_scorer,\n",
    "    \"acc\": \"accuracy\",\n",
    "}\n",
    "\n",
    "trc = GridSearchCV(\n",
    "    estimator=model_tree,\n",
    "    scoring=scoring_dict,\n",
    "    param_grid={\n",
    "        \"criterion\": criterion,\n",
    "        \"max_depth\": max_dephts,\n",
    "        \"min_samples_split\": min_samples_split,\n",
    "        \"min_samples_leaf\": min_samples_leaf,\n",
    "        \"max_features\": max_features,\n",
    "    },\n",
    "    cv=5,\n",
    "    return_train_score=False,\n",
    "    refit=\"f1_mac\",\n",
    ")\n",
    "\n",
    "model_5CV = trc.fit(X_train, y_train)\n",
    "print(timedelta(seconds=(time() - init_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_cols = [\n",
    "    \"param_criterion\",\n",
    "    \"param_max_depth\",\n",
    "    \"param_max_features\",\n",
    "    \"param_min_samples_leaf\",\n",
    "    \"param_min_samples_split\",\n",
    "    \"mean_test_f1_mac\",\n",
    "    \"mean_test_f1_class_0\",\n",
    "    \"mean_test_f1_class_1\",\n",
    "    \"mean_test_acc\",\n",
    "]\n",
    "\n",
    "pd.DataFrame(model_5CV.cv_results_).sort_values(by=\"mean_test_f1_mac\", ascending=False)[\n",
    "    scoring_cols\n",
    "].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameters we have found are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = model_5CV.best_params_\n",
    "best_params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use our validation metrics to save our best decision tree.\n",
    "\n",
    "Our metrics show that the performance has improven a bit, but I would not trust my money on this prediction. Let's see if a more complex model can improve these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_5CV.predict(X_val)\n",
    "\n",
    "results.loc[\"DT-best\", :] = compute_metrics(y_val, y_pred)\n",
    "\n",
    "confusion(y_val, y_pred)\n",
    "results.sort_values(by=\"F1-score (macro avg)\", ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "\n",
    "Random Forest is an ensemble of Decision Trees; the idea is that by averaging high-variance but decorrelated individual decision trees we will avoid their tendency to overfitting. \n",
    "\n",
    "We also introduce the **Out-of-Bag** (OOB) error. This error is a metric that we can compute on Random Forest model while training. It is very very useful because it allows us to perform model selection (tune hyperparameters) without having to execute costly cross-validations.\n",
    "\n",
    "While training, RF separates the data into subsets with replacement and trains the different trees of the forest with them. The OOB score is obtained by computing the accuracy of the trees on a set of samples that are not being used to train each specific tree. So, it acts as a **validation** score rather than a training one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:16:59.297465Z",
     "start_time": "2020-07-15T10:16:55.784208Z"
    }
   },
   "outputs": [],
   "source": [
    "model_rf1 = RandomForestClassifier(oob_score=True).fit(X_train, y_train)\n",
    "\n",
    "pred = model_rf1.predict(X_train)\n",
    "\n",
    "confusion(y_train, pred)\n",
    "\n",
    "print(\n",
    "    classification_report(\n",
    "        y_train,\n",
    "        pred,\n",
    "        target_names=[\"no\", \"yes\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"OOB accuracy=\", model_rf1.oob_score_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain an estimation of the validation accuaracy for free this way.  \n",
    "\n",
    "let's compute the real validation error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:16:59.527365Z",
     "start_time": "2020-07-15T10:16:59.299493Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = model_rf1.predict(X_val)\n",
    "print(\"Validation Accuracy:{}\".format(model_rf1.score(X_val, y_val)))\n",
    "results.loc[\"RF-default\", :] = compute_metrics(y_val, y_pred)\n",
    "confusion(y_val, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the OOB accuracy has estimated quite well the real validation one. \n",
    "\n",
    "Also, our Random Forest looks better than the DT, but it is far from perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values(by=\"F1-score (macro avg)\", ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So OOB really works in estimating accuracy and the RF is better than a single tree; however, there is a big issue with the imbalance in our target class.\n",
    "\n",
    "One way to deal with this is to include **class weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:17:03.172234Z",
     "start_time": "2020-07-15T10:16:59.789533Z"
    }
   },
   "outputs": [],
   "source": [
    "model_rf2 = RandomForestClassifier(\n",
    "    n_estimators=100, oob_score=True, class_weight=\"balanced\"\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "pred = model_rf2.predict(X_train)\n",
    "\n",
    "confusion(y_train, pred)\n",
    "\n",
    "print(\n",
    "    classification_report(\n",
    "        y_train,\n",
    "        pred,\n",
    "        target_names=[\"no\", \"yes\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"OOB accuracy=\", model_rf2.oob_score_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with a better balance; let's compute the real validation error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:17:03.402581Z",
     "start_time": "2020-07-15T10:17:03.173984Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = model_rf2.predict(X_val)\n",
    "\n",
    "results.loc[\"RF-balance\", :] = compute_metrics(y_val, y_pred)\n",
    "\n",
    "confusion(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values(by=\"F1-score (macro avg)\", ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not helping a lot. Now there are two approaches that we could take: \n",
    "* We could try to handle the imbalance problem. \n",
    "* We could try to regularize our model, it has a clear overfitting problem.\n",
    "\n",
    "We will try first tuning our model hyperparameters so it can handle better the overfitting.\n",
    "\n",
    "Note that the parameter sets have been reduced to make the grid search faster on the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_time = time()\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "ntrees = [200, None]\n",
    "max_depth = [100, None]\n",
    "min_samples_split = [4, 6]\n",
    "min_samples_leaf = [4, 6]\n",
    "balance = [None, \"balanced\", \"balanced_subsample\"]\n",
    "\n",
    "trc = GridSearchCV(\n",
    "    estimator=rf_model,\n",
    "    scoring=scoring_dict,\n",
    "    param_grid={\n",
    "        \"n_estimators\": ntrees,\n",
    "        \"max_depth\": max_depth,\n",
    "        \"min_samples_split\": min_samples_split,\n",
    "        \"min_samples_leaf\": min_samples_leaf,\n",
    "        \"class_weight\": balance,\n",
    "    },\n",
    "    cv=5,\n",
    "    return_train_score=False,\n",
    "    refit=False,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "model_5CV = trc.fit(X_train, y_train)\n",
    "print(timedelta(seconds=(time() - init_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scoring_cols = [\n",
    "    \"param_max_depth\",\n",
    "    \"param_min_samples_leaf\",\n",
    "    \"param_min_samples_split\",\n",
    "    \"mean_test_f1_mac\",\n",
    "    \"mean_test_f1_class_0\",\n",
    "    \"mean_test_f1_class_1\",\n",
    "    \"mean_test_acc\",\n",
    "]\n",
    "pd.DataFrame(model_5CV.cv_results_).sort_values(by=\"mean_test_f1_mac\", ascending=False)[\n",
    "    scoring_cols\n",
    "].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters found: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = (\n",
    "    pd.DataFrame(model_5CV.cv_results_)\n",
    "    .sort_values(by=\"mean_test_f1_mac\", ascending=False)[[\"params\"]]\n",
    "    .iloc[0, 0]\n",
    ")\n",
    "best_params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's refit our new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_tuned = RandomForestClassifier(**best_params)\n",
    "rf_model_tuned.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf_model_tuned.predict(X_val)\n",
    "\n",
    "results.loc[\"RF-best\", :] = compute_metrics(y_val, y_pred)\n",
    "\n",
    "results.sort_values(by=\"F1-score (macro avg)\", ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see on our results table that this approach has been quite successful. Improving up to 10 points on the minority F1 score. If we look at the validation confusion table, we notice the clear improvement on the minority class (still not perfect!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion(y_val, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra trees classifier\n",
    "\n",
    "As we have seen, there is room for improvement. There is a model from the family of decision trees that we have not used: `Extra trees`. Essentially `extra trees` add randomization by choosing split-points at random (rather than trying out the best split as in random forests). This makes training them faster, and also more randomized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_trees = ExtraTreesClassifier()\n",
    "extra_trees.fit(X_train, y_train)\n",
    "\n",
    "y_pred = extra_trees.predict(X_val)\n",
    "\n",
    "results.loc[\"extra_trees\", :] = compute_metrics(y_val, y_pred)\n",
    "\n",
    "results.sort_values(by=\"F1-score (macro avg)\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_time = time()\n",
    "\n",
    "rf_model = ExtraTreesClassifier(class_weight=\"balanced\")\n",
    "\n",
    "ntrees = [150, None]\n",
    "max_depth = [100, None]\n",
    "min_samples_split = [4, 6]\n",
    "min_samples_leaf = [2, 4]\n",
    "balance = [None, \"balanced\", \"balanced_subsample\"]\n",
    "\n",
    "trc = GridSearchCV(\n",
    "    estimator=rf_model,\n",
    "    scoring=scoring_dict,\n",
    "    param_grid={\n",
    "        \"n_estimators\": ntrees,\n",
    "        \"max_depth\": max_depth,\n",
    "        \"min_samples_split\": min_samples_split,\n",
    "        \"min_samples_leaf\": min_samples_leaf,\n",
    "        \"class_weight\": balance,\n",
    "    },\n",
    "    cv=5,\n",
    "    return_train_score=True,\n",
    "    refit=False,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "model_5CV = trc.fit(X_train, y_train)\n",
    "print(timedelta(seconds=(time() - init_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model_5CV.cv_results_).sort_values(by=\"mean_test_f1_mac\", ascending=False)[\n",
    "    scoring_cols\n",
    "].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best extra trees found: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = (\n",
    "    pd.DataFrame(model_5CV.cv_results_)\n",
    "    .sort_values(by=\"mean_test_f1_mac\", ascending=False)[[\"params\"]]\n",
    "    .iloc[0, 0]\n",
    ")\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_trees_best = ExtraTreesClassifier(**best_params)\n",
    "extra_trees_best.fit(X_train, y_train)\n",
    "\n",
    "y_pred = extra_trees_best.predict(X_val)\n",
    "\n",
    "results.loc[\"extra_trees-best\", :] = compute_metrics(y_val, y_pred)\n",
    "\n",
    "results.sort_values(by=\"F1-score (macro avg)\", ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance \n",
    "\n",
    "One advantage that RF and DT have is that they perform feature selection on training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T10:17:36.311344Z",
     "start_time": "2020-07-15T10:17:36.141300Z"
    }
   },
   "outputs": [],
   "source": [
    "var_imp = pd.DataFrame(\n",
    "    {\"importance\": rf_model_tuned.feature_importances_}, index=X_train.columns\n",
    ")\n",
    "\n",
    "var_imp.sort_values(by=\"importance\").plot.barh(figsize=(8, 8), legend=False);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "according to our model, `duration` is the most important variable for predicting the target. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a set of models we are going to combine them into more powerful classifiers. \n",
    "\n",
    "First we will take our simple classifiers + GaussianNB and make a voting classifier with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_nb = GaussianNB()\n",
    "gauss_nb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = gauss_nb.predict(X_val)\n",
    "\n",
    "results.loc[\"GaussianNB-default\", :] = compute_metrics(y_val, y_pred)\n",
    "\n",
    "results.sort_values(by=\"F1-score (macro avg)\", ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a voting classifier we can make the models vote in a \"hard\" fashion (majority vote) or \"soft\" fashion (averaging probabilities). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_hard = VotingClassifier(\n",
    "    [\n",
    "        (\"dt\", model_tree),\n",
    "        (\"rf\", model_rf1),\n",
    "        (\"gnb\", gauss_nb),\n",
    "        (\"extratrees\", extra_trees),\n",
    "    ]\n",
    ")\n",
    "voting_hard.fit(X_train, y_train)\n",
    "\n",
    "y_pred = voting_hard.predict(X_val)\n",
    "\n",
    "results.loc[\"voting_hard\", :] = compute_metrics(y_val, y_pred)\n",
    "\n",
    "results.loc[\n",
    "    [\"DT-default\", \"GaussianNB-default\", \"RF-default\", \"extra_trees\", \"voting_hard\"], :\n",
    "].sort_values(by=\"F1-score (class 1)\", ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that soft voting works better. We can also see that the models are performing better together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_soft = VotingClassifier(\n",
    "    [\n",
    "        (\"dt\", model_tree),\n",
    "        (\"rf\", model_rf1),\n",
    "        (\"gnb\", gauss_nb),\n",
    "        (\"extratrees\", extra_trees),\n",
    "    ],\n",
    "    voting=\"soft\",\n",
    ")\n",
    "voting_soft.fit(X_train, y_train)\n",
    "\n",
    "y_pred = voting_soft.predict(X_val)\n",
    "\n",
    "results.loc[\"voting_soft\", :] = compute_metrics(y_val, y_pred)\n",
    "\n",
    "results.loc[\n",
    "    [\n",
    "        \"DT-default\",\n",
    "        \"GaussianNB-default\",\n",
    "        \"RF-default\",\n",
    "        \"extra_trees\",\n",
    "        \"voting_hard\",\n",
    "        \"voting_soft\",\n",
    "    ],\n",
    "    :,\n",
    "].sort_values(by=\"F1-score (class 1)\", ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make a voting classifier with our best models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_best = VotingClassifier(\n",
    "    [(\"rf\", rf_model_tuned), (\"extratrees\", extra_trees_best)], voting=\"soft\"\n",
    ")\n",
    "voting_best.fit(X_train, y_train)\n",
    "\n",
    "y_pred = voting_best.predict(X_val)\n",
    "\n",
    "results.loc[\"voting_best\", :] = compute_metrics(y_val, y_pred)\n",
    "\n",
    "results.loc[[\"RF-best\", \"extra_trees-best\", \"voting_best\"], :].sort_values(\n",
    "    by=\"F1-score (class 1)\", ascending=False\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another ensamble we can use is an Stacking classifier. It will train another classifier on the top of the result of our classifiers instead of voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacky = StackingClassifier(\n",
    "    estimators=[(\"rf\", rf_model_tuned), (\"extratrees\", extra_trees_best)],\n",
    "    final_estimator=GradientBoostingClassifier(),\n",
    ")\n",
    "\n",
    "stacky.fit(X_train, y_train)\n",
    "\n",
    "y_pred = stacky.predict(X_val)\n",
    "\n",
    "results.loc[\"stackyn-clf\", :] = compute_metrics(y_val, y_pred)\n",
    "results.loc[[\"RF-best\", \"extra_trees-best\", \"stackyn-clf\"], :].sort_values(\n",
    "    by=\"F1-score (class 1)\", ascending=False\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our results\n",
    "\n",
    "We have tried many many models and burnt lots of computing time. Let us check our results now. \n",
    "\n",
    "The best model we have trained according to our metrics is the voting soft classifier with the best random forest and extra trees. These two models follow on the ranking closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.sort_values(by=\"F1-score (class 1)\", ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our best model after all that work is the tuned random forest. Now lets check our metrics over this model.\n",
    "\n",
    "Our test results are very similar to the validation ones. Which means that our model would probably generalize well if we used it on new data.\n",
    "\n",
    "We have scalated from a f1-score for class 1 of 0.473 in our first decision tree to a 0.61. Which is a great improvement given the difficulty of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_model_tuned.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
